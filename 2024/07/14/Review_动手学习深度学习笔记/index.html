<!DOCTYPE HTML>
<html>

<head>
	<link rel="bookmark"  type="image/x-icon"  href="/img/logo.jpg"/>
	<link rel="shortcut icon" href="/img/logo.jpg">
	
			    <title>
    Eggman
    </title>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
    <link rel="stylesheet" href="/css/mic_main.css" />
    <link rel="stylesheet" href="/css/dropdownMenu.css" />
    <meta name="keywords" content="cuiyi eggman" />
    
    	<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
	 
    <noscript>
        <link rel="stylesheet" href="/css/noscript.css" />
    </noscript>
    <style type="text/css">
        body:before {
          content: ' ';
          position: fixed;
          top: 0;
          background: url('/img/bg.jpg') center 0 no-repeat;
          right: 0;
          bottom: 0;
          left: 0;
          background-size: cover; 
        }
    </style>

			    
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script async type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


    <script src="/js/jquery.min.js"></script>
    <script src="/js/jquery.scrollex.min.js"></script>
    <script src="/js/jquery.scrolly.min.js"></script>
    <script src="/js/skel.min.js"></script>
    <script src="/js/util.js"></script>
    <script src="/js/main.js"></script>
	
<meta name="generator" content="Hexo 6.3.0"></head>
    
		
<!-- Layouts -->



<!--  代码渲染  -->
<link rel="stylesheet" href="/css/prism_coy.css" />
<link rel="stylesheet" href="/css/typo.css" />
<!-- 文章页 -->
<body class="is-loading">
    <!-- Wrapper 外包 s-->
    <div id="wrapper" class="fade-in">
        <!-- Intro 头部显示 s -->
        <!-- Intro 头部显示 e -->
        <!-- Header 头部logo start -->
        <header id="header">
    <a href="/" class="logo">Innovation</a>
</header>
        <!-- Nav 导航条 start -->
        <nav id="nav" class="special" >
            <ul class="menu links" >
			<!-- Homepage  主页  --> 
			<li >
	            <a href="/" rel="nofollow">Home</a>
	        </li>
			<!-- categories_name  分类   --> 
	        
	        <!-- archives  归档   --> 
	        
	        <li class="active">
	            <a href="#s1">Archive</a>
	                    <ul class="submenu">
	                        <li>
	                        <a class="archive-link" href="/archives/2024/11/">November 2024</a></li><li><a class="archive-link" href="/archives/2024/09/">September 2024</a></li><li><a class="archive-link" href="/archives/2024/07/">July 2024</a></li><li><a class="archive-link" href="/archives/2024/04/">April 2024</a></li><li><a class="archive-link" href="/archives/2023/09/">September 2023</a>
	                    </ul>
	        </li>
	        
	        
		        <!-- Pages 自定义   -->
		        
		        <li>
		            <a href="/about/" title="About me">
		                About me
		            </a>
		        </li>
		        
		        <li>
		            <a href="/group/" title="Team">
		                Team
		            </a>
		        </li>
		        
		        <li>
		            <a href="/gallery/" title="Gallery">
		                Gallery
		            </a>
		        </li>
		        


            </ul>
            <!-- icons 图标   -->
			<ul class="icons">
                    
                    <li>
                        <a title="github" href="https://github.com/cuiyi126" target="_blank" rel="noopener">
                            <i class="icon fa fa-github"></i>
                        </a>
                    </li>
                    
                    <li>
                        <a title="qq" href="https://qm.qq.com/q/9CbJ0aB9u0&personal_qrcode_source=4" target="_blank" rel="noopener">
                            <i class="icon fa fa-qq"></i>
                        </a>
                    </li>
                    
			</ul>
</nav>

        <div id="main" >
            <div class ="post_page_title_img" style="height: 25rem;background-image: url(../../../../gallery/capybara2.jpg);background-position: center; background-repeat:no-repeat; background-size:cover;-moz-background-size:cover;overflow:hidden;" >
                <a href="#" style="padding: 4rem 4rem 2rem 4rem ;"><h2 >Deep-learning知识简要回顾</h2></a>
            </div>
            <!-- Post -->
            <div class="typo" style="padding: 3rem;">
                <h1 id="参考“李沐-动手学习深度学习”重新系统的回顾必要知识，顺手记录感悟"><a href="#参考“李沐-动手学习深度学习”重新系统的回顾必要知识，顺手记录感悟" class="headerlink" title="参考“李沐-动手学习深度学习”重新系统的回顾必要知识，顺手记录感悟"></a>参考<a target="_blank" rel="noopener" href="https://zh-v2.d2l.ai/chapter_introduction/index.html">“李沐-动手学习深度学习”</a>重新系统的回顾必要知识，顺手记录感悟</h1><h1 id="一、机器学习与神经网络前言-2024-7-14更"><a href="#一、机器学习与神经网络前言-2024-7-14更" class="headerlink" title="一、机器学习与神经网络前言 (2024&#x2F;7&#x2F;14更)"></a>一、机器学习与神经网络前言 (2024&#x2F;7&#x2F;14更)</h1><p><strong>机器学习的核心组件：数据，模型，目标函数，算法</strong></p>
<ul>
<li>*没有高质量的数据，深度学习将黯然失色（所以说数据就像对人的教育）</li>
<li>*深度学习倾向于end-to-end模型，模型就像做事的方法逻辑</li>
<li>*目标函数用于反馈和迭代，不然如何进步？做事要有一定的方向</li>
<li>*优化算法是实际执行的步骤代码，是开始行动的标志；高效的算法代表高效的行动，作用于数据-模型-函数之间。</li>
</ul>
<p><strong>看了一会发现这节看过了，简单再记一些平时做的少的部分。</strong></p>
<ul>
<li>*监督学习：…，标记（多标签），搜索，推荐，序列…</li>
<li>*无监督学习：…，因果，概率图…</li>
<li>*强化学习：…</li>
</ul>
<h1 id="二、预备知识"><a href="#二、预备知识" class="headerlink" title="二、预备知识"></a>二、预备知识</h1><p><strong>这部分涉及数据处理和一些数学（向量，矩阵，微分…）回顾浏览一下并简要记录一些可能不熟悉的部分。当然也可以看看一些教程里的一些没用过的库和表达。</strong></p>
<ol>
<li><p>1.数据操作：这部分介绍了基础的张量操作，新手可以注意节省内存部分、广播部分、对象转换部分</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#以下操作导致id重新分配</span><br><span class="line">before = id(Y)</span><br><span class="line">Y = Y + X</span><br><span class="line">id(Y) == before</span><br><span class="line">#通过切片指向指定id</span><br><span class="line">before = id(X)</span><br><span class="line">X += Y</span><br><span class="line">id(X) == before</span><br></pre></td></tr></table></figure>
</li>
<li><p>2.数据预处理：pandas（我还挺少用来着），对于处理大批量，高质量的源头数据要用到，可以用于处理缺失数据</p>
</li>
<li><p>3.线性代数： 矩阵&#x2F;张量操作；非降维求和（keepdims）；矩阵乘法与Hadamard积；范数…</p>
</li>
<li><p>4.微积分： （仿佛回到了高中），这部分要清楚导数，梯度，链式法则，有一说一太久不用我都不一定写的出来链式法则0.0</p>
</li>
<li><p>5.自动微分：这应该是基于梯度的优化最重要的部分了（但是对使用者并不需要严格掌握，大部分时间几乎不需要知道，但深入底层其实往往会自己autograd函数），大部分深度学习库的底层都是一个个计算图；对于高阶和高维的y和x，求导的结果可以是一个高阶张量；控制流的梯度计算允许梯度在各种结构中被计算</p>
</li>
<li><p>6.概率：真神降临！——概率是更古典的数学美学。概率将不确定的事情进行了确定的描述，概率和人性有一种关联，概率可以通往哲学。随机变量，最基础的概念，但实际上很容易搞错；贝叶斯定理，统计学中最有用的方程之一；边际概率；期望、（协）方差（也非常重要的概念）</p>
</li>
</ol>
<h1 id="三、线性神经网络-2024-7-16更"><a href="#三、线性神经网络-2024-7-16更" class="headerlink" title="三、线性神经网络 (2024&#x2F;7&#x2F;16更)"></a>三、线性神经网络 (2024&#x2F;7&#x2F;16更)</h1><p><strong>在介绍深度神经网络之前，我们需要了解神经网络训练的基础知识。 本章我们将介绍神经网络的整个训练过程， 包括：定义简单的神经网络架构、数据处理、指定损失函数和如何训练模型。 为了更容易学习，我们将从经典算法————线性神经网络开始，介绍神经网络的基础知识。 经典统计学习技术中的线性回归和softmax回归可以视为线性神经网络， 这些知识将为其他部分中更复杂的技术奠定基础。</strong></p>
<ol>
<li>1.线性回归：线性模型y&#x3D;Xw+b;损失函数；解析解（简单的模型可以有解析解了，复杂的深度学习模型很难有解析解）；随机梯度下降；预测&#x2F;推断；矢量化运算；正态分布与平方损失——这部分建议深入思考学习（在高斯噪声的假设下，最小化均方误差等价于对线性模型的极大似然估计）（插一嘴极大似然，哲学的理解就是冥冥之中自有定数，既然发生了，那一定是有因果可循，一定是最有可能如此才会被观察到）；从线性回归到深度网络。</li>
</ol>
<ul>
<li>小结：机器学习模型中的关键要素是训练数据、损失函数、优化算法，还有模型本身。矢量化使数学表达上更简洁，同时运行的更快。最小化目标函数和执行极大似然估计等价。线性回归模型也是一个简单的神经网络。</li>
</ul>
<ol start="2">
<li><p>2.从0开始实现线性回归（回顾入门代码）<br>Examples：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"># 我们定义一个data_iter函数， 该函数接收批量大小、特征矩阵和标</span><br><span class="line"># 签向量作为输入，生成大小为batch_size的小批量。 </span><br><span class="line"># 每个小批量包含一组特征和标签。</span><br><span class="line">def data_iter(batch_size, features, labels):</span><br><span class="line">    num_examples = len(features)</span><br><span class="line">    indices = list(range(num_examples))</span><br><span class="line">    # 这些样本是随机读取的，没有特定的顺序</span><br><span class="line">    random.shuffle(indices)</span><br><span class="line">    for i in range(0, num_examples, batch_size):</span><br><span class="line">        batch_indices = np.array(</span><br><span class="line">            indices[i: min(i + batch_size, num_examples)])</span><br><span class="line">        yield features[batch_indices], labels[batch_indices]</span><br><span class="line"></span><br><span class="line">batch_size = 10</span><br><span class="line"></span><br><span class="line">for X, y in data_iter(batch_size, features, labels):</span><br><span class="line">    print(X, &#x27;\n&#x27;, y)</span><br><span class="line">    break</span><br></pre></td></tr></table></figure>
</li>
<li><p>3.线性回归的深度学习框架实现</p>
</li>
<li><p>4.Softmax回归——分类：全连接层的参数开销（具体来说，对于任何具有d个输入和q个输出的全连接层， 参数开销为O(dq)，这个数字在实践中可能高得令人望而却步。幸运的是，将d个输入转换为q个输出的成本可以减少到O(dq&#x2F;n)， 其中超参数n可以由我们灵活指定，以在实际应用中平衡参数节约和模型有效性）；交叉熵与信息论（这部分同样很重要）</p>
</li>
<li><p>5.图像数据集——MNIST&#x2F;F-MNIST</p>
</li>
<li><p>6.Softmax回归从0实现</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">def softmax(X):</span><br><span class="line">    X_exp = torch.exp(X)</span><br><span class="line">    partition = X_exp.sum(1, keepdim=True)</span><br><span class="line">    return X_exp / partition  # 这里应用了广播机制</span><br></pre></td></tr></table></figure>
</li>
<li><p>7.Softmax回归简洁实现</p>
</li>
</ol>
<h1 id="四、多层感知机"><a href="#四、多层感知机" class="headerlink" title="四、多层感知机"></a>四、多层感知机</h1><p><strong>深度学习的基本模型</strong></p>
<ol>
<li><p>1.MLP：从单层到多层；从线性到非线性；通用近似；激活函数；</p>
</li>
<li><p>2.从0开始MLP （我觉得教程这个也不算从0开始，依然用库简化了很多步骤，真男人要从梯度开始手撕）</p>
</li>
<li><p>3.简洁实现MLP（。。。）</p>
</li>
<li><p>4.模型选择，欠拟合与过拟合</p>
</li>
</ol>
<ul>
<li>4.1.将模型在训练数据上拟合的比在潜在分布中更接近的现象称为过拟合（overfitting）， 用于对抗过拟合的技术称为正则化（regularization）。训练误差与泛化误差</li>
<li>4.2.i.i.d. 与 VC理论</li>
<li>4.3.模型选择，验证集（k-fold），欠拟合or过拟合？</li>
<li>4.4.模型复杂性-损失 关系图</li>
<li>4.5.数据集</li>
</ul>
<ol start="5">
<li><p>5.权重衰减(weight decay): </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># L(w,b)+λ/2*L2</span><br><span class="line">def l2_penalty(w):</span><br><span class="line">    return (w**2).sum() / 2</span><br></pre></td></tr></table></figure>
</li>
<li><p>6.Dropout: 扰动的稳健性</p>
</li>
<li><p>7.前向传播，反向传播与计算图 （这部分的推导可以看看，但不够详细）；在训练期间需要额外存储计算图中元素的梯度，导致额外的内存开销</p>
</li>
<li><p>8.数值稳定性与模型初始化<br><strong>初始化方案的选择在神经网络学习中起着举足轻重的作用， 它对保持数值稳定性至关重要。 此外，这些初始化方案的选择可以与非线性激活函数的选择有趣的结合在一起。 我们选择哪个函数以及如何初始化参数可以决定优化算法收敛的速度有多快。 糟糕选择可能会导致我们在训练时遇到梯度爆炸或梯度消失。</strong></p>
</li>
</ol>
<ul>
<li>*梯度消失和梯度爆炸：不稳定梯度带来的风险不止在于数值表示； 不稳定梯度也威胁到我们优化算法的稳定性。 我们可能面临一些问题。 要么是梯度爆炸（gradient exploding）问题： 参数更新过大，破坏了模型的稳定收敛； 要么是梯度消失（gradient vanishing）问题： 参数更新过小，在每次更新时几乎不会移动，导致模型无法学习。</li>
<li>*Sigmoid梯度消失</li>
<li>*打破对称性：对称的参数更新时也依然对称；随机初始化</li>
<li>*Xavier初始化 1&#x2F;2(nin+nout)σ2&#x3D;1</li>
<li>*…其他巧妙初始化方法也被广泛研究</li>
</ul>
<ol start="9">
<li>9.环境和分布偏移 （非常容易被忽略，非常致命）<br><strong>许多失败的机器学习部署（即实际应用）都可以追究到这种方式——我们从来没有想过数据最初从哪里来？以及我们计划最终如何处理模型的输出？ 通常情况下，开发人员会拥有一些数据且急于开发模型，而不关注这些基本问题。 有时，根据测试集的精度衡量，模型表现得非常出色。 但是当数据分布突然改变时，模型在部署中会出现灾难性的失败。 更隐蔽的是，有时模型的部署本身就是扰乱数据分布的催化剂。</strong></li>
</ol>
<ul>
<li>*分布偏移的类型：协变量（特征）偏移；标签偏移；概念偏移；</li>
<li>*分布偏移实例</li>
<li>*纠正分布偏移：协变量偏移纠正（加权）；标签偏移纠正（线性补偿）；概念偏移纠正（类似迁移学习）</li>
<li>*批量学习；在线学习；老虎机；强化学习——上述不同情况之间的一个关键区别是：在静止环境中可能一直有效的相同策略， 在环境能够改变的情况下可能不会始终有效。</li>
<li>*最后，重要的是，当我们部署机器学习系统时， 不仅仅是在优化一个预测模型， 而通常是在提供一个会被用来（部分或完全）进行自动化决策的工具。 这些技术系统可能会通过其进行的决定而影响到每个人的生活。</li>
</ul>
<ol start="10">
<li>10.<a target="_blank" rel="noopener" href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques">Kaggle实战</a></li>
</ol>
<ul>
<li>小结：真实数据通常混合了不同的数据类型，需要进行预处理。常用的预处理方法：将实值数据重新缩放为零均值和单位方法；用均值替换缺失值。将类别特征转化为指标特征，可以使我们把这个特征当作一个独热向量来对待。我们可以使用k折交叉验证来选择模型并调整超参数。对数对于相对误差很有用。</li>
</ul>
<h1 id="五、深度学习-（2024-9-22更）"><a href="#五、深度学习-（2024-9-22更）" class="headerlink" title="五、深度学习 （2024&#x2F;9&#x2F;22更）"></a>五、深度学习 （2024&#x2F;9&#x2F;22更）</h1><p><strong>在本章中，我们将深入探索深度学习计算的关键组件， 即模型构建、参数访问与初始化、设计自定义层和块、将模型读写到磁盘， 以及利用GPU实现显著的加速</strong></p>
<ol>
<li>1.层和块</li>
</ol>
<ul>
<li>*事实证明，研究讨论“比单个层大”但“比整个模型小”的组件更有价值。 例如，在计算机视觉中广泛流行的ResNet-152架构就有数百层， 这些层是由层组（groups of layers）的重复模式组成。</li>
<li>*自定义块<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">class MLP(nn.Module):</span><br><span class="line">    # 用模型参数声明层。这里，我们声明两个全连接的层</span><br><span class="line">    def __init__(self):</span><br><span class="line">        # 调用MLP的父类Module的构造函数来执行必要的初始化。</span><br><span class="line">        # 这样，在类实例化时也可以指定其他函数参数，例如模型参数params（稍后将介绍）</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.hidden = nn.Linear(20, 256)  # 隐藏层</span><br><span class="line">        self.out = nn.Linear(256, 10)  # 输出层</span><br><span class="line"></span><br><span class="line">    # 定义模型的前向传播，即如何根据输入X返回所需的模型输出</span><br><span class="line">    def forward(self, X):</span><br><span class="line">        # 注意，这里我们使用ReLU的函数版本，其在nn.functional模块中定义。</span><br><span class="line">        return self.out(F.relu(self.hidden(X)))</span><br></pre></td></tr></table></figure></li>
<li>顺序块、嵌套模块<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">class NestMLP(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.net = nn.Sequential(nn.Linear(20, 64), nn.ReLU(),</span><br><span class="line">                                 nn.Linear(64, 32), nn.ReLU())</span><br><span class="line">        self.linear = nn.Linear(32, 16)</span><br><span class="line"></span><br><span class="line">    def forward(self, X):</span><br><span class="line">        return self.linear(self.net(X))</span><br><span class="line"></span><br><span class="line">chimera = nn.Sequential(NestMLP(), nn.Linear(16, 20), FixedHiddenMLP())</span><br><span class="line">chimera(X)</span><br></pre></td></tr></table></figure></li>
</ul>
<ol start="2">
<li>2.参数管理</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">print(net[1].params) # 层参数概要</span><br><span class="line">print(type(net[1].bias)) </span><br><span class="line">print(net[1].bias)</span><br><span class="line">print(net[1].bias.data())</span><br><span class="line">net[1].weight.grad() #访问参数梯度</span><br><span class="line"></span><br><span class="line">print(net[0].collect_params()) #访问所有参数</span><br><span class="line">print(net.collect_params())</span><br></pre></td></tr></table></figure>
<ul>
<li>*嵌套块的参数访问</li>
<li>*参数初始化：<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"># 内置初始化</span><br><span class="line">def init_normal(m):</span><br><span class="line">    if type(m) == nn.Linear:</span><br><span class="line">        nn.init.normal_(m.weight, mean=0, std=0.01)</span><br><span class="line">        nn.init.zeros_(m.bias)</span><br><span class="line">net.apply(init_normal)</span><br><span class="line">net[0].weight.data[0], net[0].bias.data[0]</span><br><span class="line"></span><br><span class="line"># 常数/自定义初始化</span><br><span class="line">def init_constant(m):</span><br><span class="line">    if type(m) == nn.Linear:</span><br><span class="line">        nn.init.constant_(m.weight, 1)</span><br><span class="line">        nn.init.zeros_(m.bias)</span><br><span class="line">net.apply(init_constant)</span><br><span class="line">net[0].weight.data[0], net[0].bias.data[0]</span><br><span class="line"></span><br><span class="line">def my_init(m):</span><br><span class="line">    if type(m) == nn.Linear:</span><br><span class="line">        print(&quot;Init&quot;, *[(name, param.shape)</span><br><span class="line">                        for name, param in m.named_parameters()][0])</span><br><span class="line">        nn.init.uniform_(m.weight, -10, 10)</span><br><span class="line">        m.weight.data *= m.weight.data.abs() &gt;= 5</span><br><span class="line">net.apply(my_init)</span><br><span class="line">net[0].weight[:2]</span><br><span class="line"></span><br><span class="line"># 分层初始化</span><br><span class="line">def init_xavier(m):</span><br><span class="line">    if type(m) == nn.Linear:</span><br><span class="line">        nn.init.xavier_uniform_(m.weight)</span><br><span class="line">def init_42(m):</span><br><span class="line">    if type(m) == nn.Linear:</span><br><span class="line">        nn.init.constant_(m.weight, 42)</span><br><span class="line">net[0].apply(init_xavier)</span><br><span class="line">net[2].apply(init_42)</span><br></pre></td></tr></table></figure></li>
<li>*参数绑定&#x2F;共享：有时我们希望在多个层间共享参数： 我们可以定义一个稠密层，然后使用它的参数来设置另一个层的参数。</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># 我们需要给共享层一个名称，以便可以引用它的参数</span><br><span class="line">shared = nn.Linear(8, 8)</span><br><span class="line">net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(),</span><br><span class="line">                    shared, nn.ReLU(),</span><br><span class="line">                    shared, nn.ReLU(),</span><br><span class="line">                    nn.Linear(8, 1))</span><br><span class="line">net(X)</span><br><span class="line"># 检查参数是否相同</span><br><span class="line">print(net[2].weight.data[0] == net[4].weight.data[0])</span><br><span class="line">net[2].weight.data[0, 0] = 100</span><br><span class="line"># 确保它们实际上是同一个对象，而不只是有相同的值</span><br><span class="line">print(net[2].weight.data[0] == net[4].weight.data[0])</span><br></pre></td></tr></table></figure>
<p>这个例子表明第三个和第五个神经网络层的参数是绑定的。 它们不仅值相等，而且由相同的张量表示。 因此，如果我们改变其中一个参数，另一个参数也会改变。 这里有一个问题：当参数绑定时，梯度会发生什么情况？ 答案是由于模型参数包含梯度，因此在反向传播期间第二个隐藏层 （即第三个神经网络层）和第三个隐藏层（即第五个神经网络层）的梯度会加在一起。</p>
<ol start="3">
<li>3.延后初始化<br><strong>到目前为止，我们忽略了建立网络时需要做的以下这些事情：我们定义了网络架构，但没有指定输入维度。我们添加层时没有指定前一层的输出维度。我们在初始化参数时，甚至没有足够的信息来确定模型应该包含多少参数。这里的诀窍是框架的延后初始化（defers initialization）， 即直到数据第一次通过模型传递时，框架才会动态地推断出每个层的大小。在以后，当使用卷积神经网络时， 由于输入维度（即图像的分辨率）将影响每个后续层的维数， 有了该技术将更加方便。</strong></li>
</ol>
<ul>
<li>*延后初始化使框架能够自动推断参数形状，使修改模型架构变得容易，避免了一些常见的错误。我们可以通过模型传递数据，使框架最终初始化参数</li>
</ul>
<ol start="4">
<li>4.自定义层<br><strong>深度学习成功背后的一个因素是神经网络的灵活性： 我们可以用创造性的方式组合不同的层，从而设计出适用于各种任务的架构。</strong></li>
</ol>
<ul>
<li><p>*不带任何参数的层</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">class CenteredLayer(nn.Block):</span><br><span class="line">    def __init__(self, **kwargs):</span><br><span class="line">        super().__init__(**kwargs)</span><br><span class="line"></span><br><span class="line">    def forward(self, X):</span><br><span class="line">        return X - X.mean()</span><br></pre></td></tr></table></figure>
</li>
<li><p>*带参数的层</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">class MyLinear(nn.Module):</span><br><span class="line">    def __init__(self, in_units, units):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.weight = nn.Parameter(torch.randn(in_units, units))</span><br><span class="line">        self.bias = nn.Parameter(torch.randn(units,))</span><br><span class="line">    def forward(self, X):</span><br><span class="line">        linear = torch.matmul(X, self.weight.data) + self.bias.data</span><br><span class="line">        return F.relu(linear)</span><br></pre></td></tr></table></figure></li>
</ul>
<ol start="5">
<li>5.读写文件<br><strong>到目前为止，我们讨论了如何处理数据， 以及如何构建、训练和测试深度学习模型。 然而，有时我们希望保存训练的模型， 以备将来在各种环境中使用（比如在部署中进行预测）。 此外，当运行一个耗时较长的训练过程时， 最佳的做法是定期保存中间结果， 以确保在服务器电源被不小心断掉时，我们不会损失几天的计算结果。</strong></li>
</ol>
<ul>
<li>*结果你只给了个 torch.save(net.state_dict(), ‘mlp.params’)</li>
</ul>
<ol start="6">
<li>6.GPU</li>
</ol>
<ul>
<li>*简而言之，自2000年以来，GPU性能每十年增长1000倍。</li>
<li>*如果有多个GPU，我们使用torch.device(f’cuda:{i}’) 来表示第<br>块GPU（从0开始）OS： 贫穷限制了我的想象</li>
<li>*有几种方法可以在GPU上存储张量。 例如，我们可以在创建张量时指定存储设备。接 下来，我们在第一个gpu上创建张量变量X。 在GPU上创建的张量只消耗这个GPU的显存。 我们可以使用nvidia-smi命令查看显存使用情况。 一般来说，我们需要确保不创建超过GPU显存限制的数据。</li>
<li>*复制，X(GPU0) +Y(GPU1)将导致错误，需复制Z&#x3D;X(GPU1),使用Z+Y</li>
</ul>
<h1 id="六、-卷积神经网络-2024-11-6更新"><a href="#六、-卷积神经网络-2024-11-6更新" class="headerlink" title="六、 卷积神经网络 (2024&#x2F;11&#x2F;6更新)"></a>六、 卷积神经网络 (2024&#x2F;11&#x2F;6更新)</h1><p><strong>到目前为止，我们处理这类结构丰富的数据的方式还不够有效。 我们仅仅通过将图像数据展平成一维向量而忽略了每个图像的空间结构信息，再将数据送入一个全连接的多层感知机中。</strong></p>
<ol>
<li>从全连接到卷积</li>
</ol>
<ul>
<li>1.1-2 不变性与多层感知机推理<br>平移不变性（translation invariance）：不管检测对象出现在图像中的哪个位置，神经网络的前面几层应该对相同的图像区域具有相似的反应，即为“平移不变性”。</li>
</ul>
<p>局部性（locality）：神经网络的前面几层应该只探索输入图像中的局部区域，而不过度在意图像中相隔较远区域的关系，这就是“局部性”原则。最终，可以聚合这些局部特征，以在整个图像级别进行预测。</p>
<p>两者存在数学解释</p>
<ul>
<li>1.3 卷积</li>
</ul>
<p>一维函数卷积：(f<em>g)(x)&#x3D;∫ f(z)g(x-z)dz<br>二维函数卷积: (f</em>g)(i,j)&#x3D;ΣaΣb f(a,b)g(i-a,j-b)</p>
<ul>
<li>1.4 通道</li>
</ul>
<ol start="2">
<li>图像卷积</li>
</ol>
<ul>
<li><p>2.1 互相关运算——卷积层严格来说应称为互相关运算<br>注意，输出大小略小于输入大小。这是因为卷积核的宽度和高度大于1， 而卷积核只与图像中每个大小完全适合的位置进行互相关运算。</p>
</li>
<li><p>2.2 卷积层<br>卷积层对输入和卷积核权重进行互相关运算，并在添加标量偏置之后产生输出。 所以，卷积层中的两个被训练的参数是卷积核权重和标量偏置。</p>
</li>
<li><p>2.3 边缘检测 </p>
</li>
<li><p>2.4 学习卷积核——更复杂的“边缘检测”</p>
</li>
<li><p>2.5 互相关和卷积</p>
</li>
</ul>
<p>*2.6 特征映射和感受野<br>在卷积神经网络中，对于某一层的任意元素，其感受野（receptive field）<br>是指在前向传播期间可能影响计算的所有元素（来自所有先前层）。</p>
<ol start="3">
<li><p>填充和步幅<br>有时，在应用了连续的卷积之后，我们最终得到的输出远小于输入大小。这是由于卷积核的宽度和高度通常大于1<br>所导致的。比如，一个240<em>240像素的图像，经过10层的5</em>5卷积后，将减少到200*200像素。如此一来，原始图像<br>的边界丢失了许多有用信息。而填充是解决此问题最有效的方法； 有时，我们可能希望大幅降低图像的宽度和高度。<br>例如，如果我们发现原始的输入分辨率十分冗余。步幅则可以在这类情况下提供帮助。</p>
</li>
<li><p>多输入和多输出通道——当我们添加通道时，我们的输入和隐藏的表示都变成了三&#x2F;N维张量。</p>
</li>
</ol>
<ul>
<li>4.1 多输入通道数&#x3D;卷积核通道数</li>
<li>4.2 多输出通道个数&#x3D;卷积核个数</li>
<li>4.3 1x1卷积层</li>
</ul>
<ol start="5">
<li>汇聚(池化)层<br>本节将介绍汇聚（pooling）层，它具有双重目的：降低卷积层对位置的敏感性，同时降低对空间降采样表示的敏感性。</li>
</ol>
<p>*5.1 最大汇聚与平均汇聚层<br>*5.2 填充和步幅<br>*5.3 多通道——单独运算而不在通道上汇总</p>
<ol start="6">
<li>卷积神经网络 LeNet<br>本节将介绍LeNet，它是最早发布的卷积神经网络之一，因其在计算机视觉任务中的高效性能而受到广泛关注。<br>这个模型是由AT&amp;T贝尔实验室的研究员Yann LeCun在1989年提出的（并以其命名），目的是识别图像 (LeCun et al., 1998)中的手写数字<br><img src="/../../../../gallery/LeNet.jpg" alt="LeNet"></li>
</ol>
<h1 id="七、-现代卷积神经网络-（2024-11-11更）"><a href="#七、-现代卷积神经网络-（2024-11-11更）" class="headerlink" title="七、 现代卷积神经网络 （2024&#x2F;11&#x2F;11更）"></a>七、 现代卷积神经网络 （2024&#x2F;11&#x2F;11更）</h1><p><strong>在本章中的每一个模型都曾一度占据主导地位，其中许多模型都是ImageNet竞赛的优胜者。ImageNet竞赛自2010年以来，一直是计算机视觉中监督学习进展的指向标。</strong></p>
<p><strong>这些模型包括：</strong></p>
<p><strong>AlexNet。它是第一个在大规模视觉竞赛中击败传统计算机视觉模型的大型神经网络；</strong></p>
<p><strong>使用重复块的网络（VGG）。它利用许多重复的神经网络块；</strong></p>
<p><strong>网络中的网络（NiN）。它重复使用由卷积层和卷积层（用来代替全连接层）来构建深层网络;</strong></p>
<p><strong>含并行连结的网络（GoogLeNet）。它使用并行连结的网络，通过不同窗口大小的卷积层和最大汇聚层来并行抽取信息；</strong></p>
<p><strong>残差网络（ResNet）。它通过残差块构建跨层的数据通道，是计算机视觉中最流行的体系架构；</strong></p>
<p><strong>稠密连接网络（DenseNet）。它的计算成本很高，但给我们带来了更好的效果。</strong></p>
<ol>
<li>1.深度卷积神经网络 AlexNet<br>在LeNet提出后，卷积神经网络在计算机视觉和机器学习领域中很有名气。但卷积神经网络并没有主导这些领域。</li>
</ol>
<p>这是因为虽然LeNet在小数据集上取得了很好的效果，但是在更大、更真实的数据集上训练卷积神经网络的性能和可行性还有待研究。</p>
<p>在计算机视觉中，直接将神经网络与其他机器学习方法进行比较也许不公平。这是因为，卷积神经网络的输入是由原始像素值或是经过简单预处理（例如居中、缩放）的像素值组成的。但在使用传统机器学习方法时，从业者永远不会将原始像素作为输入。在传统机器学习方法中，计算机视觉流水线是由经过人的手工精心设计的特征流水线组成的。对于这些传统方法，大部分的进展都来自于对特征有了更聪明的想法，并且学习到的算法往往归于事后的解释。</p>
<p>虽然上世纪90年代就有了一些神经网络加速卡，但仅靠它们还不足以开发出有大量参数的深层多通道多层卷积神经网络。此外，当时的数据集仍然相对较小。除了这些障碍，训练神经网络的一些关键技巧仍然缺失，包括启发式参数初始化、随机梯度下降的变体、非挤压激活函数和有效的正则化技术。</p>
<p>因此，与训练端到端（从像素到分类结果）系统不同，经典机器学习的流水线看起来更像下面这样：</p>
<p>获取一个有趣的数据集。在早期，收集这些数据集需要昂贵的传感器（在当时最先进的图像也就100万像素）。</p>
<p>根据光学、几何学、其他知识以及偶然的发现，手工对特征数据集进行预处理。</p>
<p>通过标准的特征提取算法，如SIFT（尺度不变特征变换） (Lowe, 2004)和SURF（加速鲁棒特征） (Bay et al., 2006)或其他手动调整的流水线来输入数据。</p>
<p>将提取的特征送入最喜欢的分类器中（例如线性模型或其它核方法），以训练分类器。</p>
<p>当人们和机器学习研究人员交谈时，会发现机器学习研究人员相信机器学习既重要又美丽：优雅的理论去证明各种模型的性质。机器学习是一个正在蓬勃发展、严谨且非常有用的领域。然而，当人们和计算机视觉研究人员交谈，会听到一个完全不同的故事。计算机视觉研究人员会告诉一个诡异事实————推动领域进步的是数据特征，而不是学习算法。计算机视觉研究人员相信，从对最终模型精度的影响来说，更大或更干净的数据集、或是稍微改进的特征提取，比任何学习算法带来的进步要大得多。</p>
<ul>
<li>1.1 学习表征<br>2012年前，图像特征都是机械地计算出来的。事实上，设计一套新的特征函数、改进结果，并撰写论文是盛极一时的潮流。<br>事实上，Alex Krizhevsky、Ilya Sutskever和Geoff Hinton提出了一种新的卷积神经网络变体AlexNet。在2012年ImageNet挑战赛中取得了轰动一时的成绩。AlexNet以Alex Krizhevsky的名字命名，他是论文 (Krizhevsky et al., 2012)的第一作者。</li>
</ul>
<p>成功的因素：<br>1-缺少的成分：数据 2009年，ImageNet数据集发布，并发起ImageNet挑战赛：要求研究人员从100万个样本中训练模型，以区分1000个不同类别的对象。ImageNet数据集由斯坦福教授李飞飞小组的研究人员开发，利用谷歌图像搜索（Google Image Search）对每一类图像进行预筛选，并利用亚马逊众包（Amazon Mechanical Turk）来标注每张图片的相关类别。这种规模是前所未有的。这项被称为ImageNet的挑战赛推动了计算机视觉和机器学习研究的发展，挑战研究人员确定哪些模型能够在更大的数据规模下表现最好。<br>2-缺少的成分：硬件 GPU vs CPU</p>
<ul>
<li>1.2 AlexNet<br>2012年，AlexNet横空出世。它首次证明了学习到的特征可以超越手工设计的特征。它一举打破了计算机视觉研究的现状。 AlexNet使用了8层卷积神经网络，并以很大的优势赢得了2012年ImageNet图像识别挑战赛。</li>
</ul>
<p>AlexNet和LeNet的架构非常相似，如 图7.1.2所示。 注意，本书在这里提供的是一个稍微精简版本的AlexNet，去除了当年需要两个小型GPU同时运算的设计特点。<br>AlexNet和LeNet的设计理念非常相似，但也存在显著差异。</p>
<p>AlexNet比相对较小的LeNet5要深得多。AlexNet由八层组成：五个卷积层、两个全连接隐藏层和一个全连接输出层。</p>
<p>AlexNet使用ReLU而不是sigmoid作为其激活函数。<br><img src="/../../../../gallery/AlexNet.jpg" alt="AlexNet"></p>
<p>Alex细节： 更大的卷积窗口11x11；逐渐缩减卷积窗口；更多的卷积通道数；<br>在最后一个卷积层后有两个全连接层，分别有4096个输出。 这两个巨大的全连接层拥有将近1GB的模型参数。<br>由于早期GPU显存有限，原版的AlexNet采用了双数据流设计，使得每个GPU只负责存储和计算模型的一半参数。<br>幸运的是，现在GPU显存相对充裕，所以现在很少需要跨GPU分解模型（因此，本书的AlexNet模型在这方面与原始论文稍有不同）。<br>激活函数选Relu；暂退法&#x2F;图像增强&#x2F;…<br>这里的主要变化是使用更小的学习速率训练，网络更深更广、图像分辨率更高，训练卷积神经网络就更昂贵。</p>
<ol start="2">
<li>2.使用块的网络（VGG）<br>虽然AlexNet证明深层神经网络卓有成效，但它没有提供一个通用的模板来指导后续的研究人员设计新的网络。 在下面的几个章节中，我们将介绍一些常用于设计深层神经网络的启发式概念。</li>
</ol>
<p>与芯片设计中工程师从放置晶体管到逻辑元件再到逻辑块的过程类似，神经网络架构的设计也逐渐变得更加抽象。研究人员开始从单个神经元的角度思考问题，发展到整个层，现在又转向块，重复层的模式。</p>
<p>使用块的想法首先出现在牛津大学的视觉几何组（visual geometry group）的VGG网络中。通过使用循环和子程序，可以很容易地在任何现代深度学习框架的代码中实现这些重复的架构。</p>
<ul>
<li>2.1 VGG块<br>经典卷积神经网络的基本组成部分是下面的这个序列：</li>
</ul>
<p>带填充以保持分辨率的卷积层；</p>
<p>非线性激活函数，如ReLU；</p>
<p>汇聚层，如最大汇聚层。</p>
<p>而一个VGG块与之类似，由一系列卷积层组成，后面再加上用于空间下采样的最大汇聚层。在最初的VGG论文中 (Simonyan and Zisserman, 2014)，作者使用了带有<br>卷积核、填充为1（保持高度和宽度）的卷积层，和带有<br>汇聚窗口、步幅为2（每个块后的分辨率减半）的最大汇聚层。在下面的代码中，我们定义了一个名为vgg_block的函数来实现一个VGG块。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">def vgg_block(num_convs, in_channels, out_channels):</span><br><span class="line">    layers = []</span><br><span class="line">    for _ in range(num_convs):</span><br><span class="line">        layers.append(nn.Conv2d(in_channels, out_channels,</span><br><span class="line">                                kernel_size=3, padding=1))</span><br><span class="line">        layers.append(nn.ReLU())</span><br><span class="line">        in_channels = out_channels</span><br><span class="line">    layers.append(nn.MaxPool2d(kernel_size=2,stride=2))</span><br><span class="line">    return nn.Sequential(*layers)</span><br></pre></td></tr></table></figure>

<ul>
<li>2.2 VGG网络<br><img src="/../../../../gallery/VGG.jpg" alt="VGGNet"></li>
</ul>
<p>VGG神经网络连接 图7.2.1的几个VGG块（在vgg_block函数中定义）。其中有超参数变量conv_arch。该变量指定了每个VGG块里卷积层个数和输出通道数。全连接模块则与AlexNet中的相同。</p>
<p>原始VGG网络有5个卷积块，其中前两个块各有一个卷积层，后三个块各包含两个卷积层。 第一个模块有64个输出通道，每个后续模块将输出通道数量翻倍，直到该数字达到512。由于该网络使用8个卷积层和3个全连接层，因此它通常被称为VGG-11。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">conv_arch = ((1, 64), (1, 128), (2, 256), (2, 512), (2, 512))</span><br><span class="line">def vgg(conv_arch):</span><br><span class="line">    conv_blks = []</span><br><span class="line">    in_channels = 1</span><br><span class="line">    # 卷积层部分</span><br><span class="line">    for (num_convs, out_channels) in conv_arch:</span><br><span class="line">        conv_blks.append(vgg_block(num_convs, in_channels, out_channels))</span><br><span class="line">        in_channels = out_channels</span><br><span class="line"></span><br><span class="line">    return nn.Sequential(</span><br><span class="line">        *conv_blks, nn.Flatten(),</span><br><span class="line">        # 全连接层部分</span><br><span class="line">        nn.Linear(out_channels * 7 * 7, 4096), nn.ReLU(), nn.Dropout(0.5),</span><br><span class="line">        nn.Linear(4096, 4096), nn.ReLU(), nn.Dropout(0.5),</span><br><span class="line">        nn.Linear(4096, 10))</span><br><span class="line"></span><br><span class="line">net = vgg(conv_arch)</span><br></pre></td></tr></table></figure>

<ul>
<li>2.3 总结<br>VGG-11使用可复用的卷积块构造网络。不同的VGG模型可通过每个块中卷积层数量和输出通道数量的差异来定义。</li>
</ul>
<p>块的使用导致网络定义的非常简洁。使用块可以有效地设计复杂的网络。</p>
<p>在VGG论文中，Simonyan和Ziserman尝试了各种架构。特别是他们发现深层且窄的卷积（即<br>）比较浅层且宽的卷积更有效。</p>
<ol start="3">
<li>3.网络中的网络 （NiN）<br>LeNet、AlexNet和VGG都有一个共同的设计模式：通过一系列的卷积层与汇聚层来提取空间结构特征；然后通过全连接层对特征的表征进行处理。<br>AlexNet和VGG对LeNet的改进主要在于如何扩大和加深这两个模块。 或者，可以想象在这个过程的早期使用全连接层。然而，如果使用了全连接层，可能会完全放弃表征的空间结构。<br>网络中的网络（NiN）提供了一个非常简单的解决方案：在每个像素的通道上分别使用多层感知机 (Lin et al., 2013)</li>
</ol>
<ul>
<li><p>3.1 NiN块<br>如果我们将权重连接到每个空间位置，我们可以将其视为1x1卷积层，或作为在每个像素位置上独立作用的全连接层。 从另一个角度看，即将空间维度中的每个像素视为单个样本，将通道维度视为不同特征（feature）。<br><img src="/../../../../gallery/NiN.jpg" alt="NINNet"><br>图7.3.1说明了VGG和NiN及它们的块之间主要架构差异。 NiN块以一个普通卷积层开始，后面是两个1x1的卷积层。这两个卷积层充当带有ReLU激活函数的逐像素全连接层。 第一层的卷积窗口形状通常由用户设置，随后的卷积窗口形状固定为1x1。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">def nin_block(in_channels, out_channels, kernel_size, strides, padding):</span><br><span class="line">    return nn.Sequential(</span><br><span class="line">        nn.Conv2d(in_channels, out_channels, kernel_size, strides, padding),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        nn.Conv2d(out_channels, out_channels, kernel_size=1), nn.ReLU(),</span><br><span class="line">        nn.Conv2d(out_channels, out_channels, kernel_size=1), nn.ReLU())</span><br></pre></td></tr></table></figure>
</li>
<li><p>3.2 NiN模型<br>NiN和AlexNet之间的一个显著区别是NiN完全取消了全连接层。 相反，NiN使用一个NiN块，其输出通道数等于标签类别的数量。<br>最后放一个全局平均汇聚层（global average pooling layer），生成一个对数几率 （logits）。NiN设计的一个优点是，它显著减少了模型所需参数的数量。<br>然而，在实践中，这种设计有时会增加训练模型的时间。</p>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(</span><br><span class="line">    nin_block(1, 96, kernel_size=11, strides=4, padding=0),</span><br><span class="line">    nn.MaxPool2d(3, stride=2),</span><br><span class="line">    nin_block(96, 256, kernel_size=5, strides=1, padding=2),</span><br><span class="line">    nn.MaxPool2d(3, stride=2),</span><br><span class="line">    nin_block(256, 384, kernel_size=3, strides=1, padding=1),</span><br><span class="line">    nn.MaxPool2d(3, stride=2),</span><br><span class="line">    nn.Dropout(0.5),</span><br><span class="line">    # 标签类别数是10</span><br><span class="line">    nin_block(384, 10, kernel_size=3, strides=1, padding=1),</span><br><span class="line">    nn.AdaptiveAvgPool2d((1, 1)),</span><br><span class="line">    # 将四维的输出转成二维的输出，其形状为(批量大小,10)</span><br><span class="line">    nn.Flatten())</span><br></pre></td></tr></table></figure>

<ul>
<li>3.3 总结<br>NiN使用由一个卷积层和多个1x1卷积层组成的块。该块可以在卷积神经网络中使用，以允许更多的每像素非线性。</li>
</ul>
<p>NiN去除了容易造成过拟合的全连接层，将它们替换为全局平均汇聚层（即在所有位置上进行求和）。该汇聚层通道数量为所需的输出数量（例如，Fashion-MNIST的输出为10）。</p>
<p>移除全连接层可减少过拟合，同时显著减少NiN的参数。</p>
<p>NiN的设计影响了许多后续卷积神经网络的设计。</p>
<ol start="4">
<li>4.含并行连接的网络（GoogLeNet）<br>在2014年的ImageNet图像识别挑战赛中，一个名叫GoogLeNet (Szegedy et al., 2015)的网络架构大放异彩。 GoogLeNet吸收了NiN中串联网络的思想，并在此基础上做了改进。 这篇论文的一个重点是解决了什么样大小的卷积核最合适的问题。<br>毕竟，以前流行的网络使用小到1x1，大到11x11的卷积核。 本文的一个观点是，有时使用不同大小的卷积核组合是有利的。 本节将介绍一个稍微简化的GoogLeNet版本：我们省略了一些为稳定训练而添加的特殊特性，现在有了更好的训练方法，这些特性不是必要的。</li>
</ol>
<ul>
<li>4.1 Inception块<br><img src="/../../../../gallery/Inception.jpg" alt="Inception"><br>Inception块由四条并行路径组成。 前三条路径使用窗口大小为1x1、3x3和5x5的卷积层，从不同空间大小中提取信息。 中间的两条路径在输入上执行<br>卷积，以减少通道数，从而降低模型的复杂性。 第四条路径使用最大汇聚层，然后使用1x1卷积层来改变通道数。<br>这四条路径都使用合适的填充来使输入与输出的高和宽一致，最后我们将每条线路的输出在通道维度上连结，并构成Inception块的输出。在Inception块中，通常调整的超参数是每层输出通道数。</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">class Inception(nn.Module):</span><br><span class="line">    # c1--c4是每条路径的输出通道数</span><br><span class="line">    def __init__(self, in_channels, c1, c2, c3, c4, **kwargs):</span><br><span class="line">        super(Inception, self).__init__(**kwargs)</span><br><span class="line">        # 线路1，单1x1卷积层</span><br><span class="line">        self.p1_1 = nn.Conv2d(in_channels, c1, kernel_size=1)</span><br><span class="line">        # 线路2，1x1卷积层后接3x3卷积层</span><br><span class="line">        self.p2_1 = nn.Conv2d(in_channels, c2[0], kernel_size=1)</span><br><span class="line">        self.p2_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)</span><br><span class="line">        # 线路3，1x1卷积层后接5x5卷积层</span><br><span class="line">        self.p3_1 = nn.Conv2d(in_channels, c3[0], kernel_size=1)</span><br><span class="line">        self.p3_2 = nn.Conv2d(c3[0], c3[1], kernel_size=5, padding=2)</span><br><span class="line">        # 线路4，3x3最大汇聚层后接1x1卷积层</span><br><span class="line">        self.p4_1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)</span><br><span class="line">        self.p4_2 = nn.Conv2d(in_channels, c4, kernel_size=1)</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        p1 = F.relu(self.p1_1(x))</span><br><span class="line">        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))</span><br><span class="line">        p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))</span><br><span class="line">        p4 = F.relu(self.p4_2(self.p4_1(x)))</span><br><span class="line">        # 在通道维度上连结输出</span><br><span class="line">        return torch.cat((p1, p2, p3, p4), dim=1)</span><br></pre></td></tr></table></figure>
<p>那么为什么GoogLeNet这个网络如此有效呢？ 首先我们考虑一下滤波器（filter）的组合，它们可以用各种滤波器尺寸探索图像，<br>这意味着不同大小的滤波器可以有效地识别不同范围的图像细节。 同时，我们可以为不同的滤波器分配不同数量的参数。</p>
<ul>
<li>4.2 GoogLeNet<br>如图7.4.2所示，GoogLeNet一共使用9个Inception块和全局平均汇聚层的堆叠来生成其估计值。Inception块之间的最大汇聚层可降低维度。 第一个模块类似于AlexNet和LeNet，Inception块的组合从VGG继承，全局平均汇聚层避免了在最后使用全连接层。<br><img src="/../../../../gallery/GoogleNet.jpg" alt="GoogLeNet"></li>
</ul>
<p>需要注意的是，第五模块的后面紧跟输出层，该模块同NiN一样使用全局平均汇聚层，将每个通道的高和宽变成1。 最后我们将输出变成二维数组，再接上一个输出个数为标签类别数的全连接层。</p>
<ul>
<li>4.3 总结<br>Inception块相当于一个有4条路径的子网络。它通过不同窗口形状的卷积层和最大汇聚层来并行抽取信息，并使用<br>卷积层减少每像素级别上的通道维数从而降低模型复杂度。</li>
</ul>
<p>GoogLeNet将多个设计精细的Inception块与其他层（卷积层、全连接层）串联起来。其中Inception块的通道数分配之比是在ImageNet数据集上通过大量的实验得来的。</p>
<p>GoogLeNet和它的后继者们一度是ImageNet上最有效的模型之一：它以较低的计算复杂度提供了类似的测试精度。</p>
<ol start="5">
<li>批量规范化<br>训练深层神经网络是十分困难的，特别是在较短的时间内使他们收敛更加棘手。 本节将介绍批量规范化（batch normalization） (Ioffe and Szegedy, 2015)，这是一种流行且有效的技术，可持续加速深层网络的收敛速度。 再结合在 7.6节中将介绍的残差块，批量规范化使得研究人员能够训练100层以上的网络。</li>
</ol>
<ul>
<li>5.1 训练深层神经网络<br>首先，数据预处理的方式通常会对最终结果产生巨大影响。<br>第二，对于典型的多层感知机或卷积神经网络。批量规范化的发明者非正式地假设，这些变量分布中的这种偏移可能会阻碍网络的收敛。 直观地说，我们可能会猜想，如果一个层的可变值是另一层的100倍，这可能需要对学习率进行补偿调整。<br>第三，更深层的网络很复杂，容易过拟合。 这意味着正则化变得更加重要。<br><img src="/../../../../gallery/BN.jpg" alt="BN"><br>批量规范化应用于单个可选层（也可以应用到所有层），其原理如下：在每次训练迭代中，我们首先规范化输入，即通过减去其均值并除以其标准差，其中两者均基于当前小批量处理。 接下来，我们应用比例系数和比例偏移。 正是由于这个基于批量统计的标准化，才有了批量规范化的名称。</li>
</ul>
<p>请注意，如果我们尝试使用大小为1的小批量应用批量规范化，我们将无法学到任何东西。 这是因为在减去均值之后，每个隐藏单元将为0。 所以，只有使用足够大的小批量，批量规范化这种方法才是有效且稳定的。 请注意，在应用批量规范化时，批量大小的选择可能比没有批量规范化时更重要。</p>
<p>事实证明，这是深度学习中一个反复出现的主题。 由于尚未在理论上明确的原因，优化中的各种噪声源通常会导致更快的训练和较少的过拟合：这种变化似乎是正则化的一种形式。 在一些初步研究中， (Teye et al., 2018)和 (Luo et al., 2018)分别将批量规范化的性质与贝叶斯先验相关联。 这些理论揭示了为什么批量规范化最适应<br>范围中的中等批量大小的难题。</p>
<ul>
<li>5.2 批量规范化层<br>回想一下，批量规范化和其他层之间的一个关键区别是，由于批量规范化在完整的小批量上运行，因此我们不能像以前在引入其他层时那样忽略批量大小。 我们在下面讨论这两种情况：全连接层和卷积层，他们的批量规范化实现略有不同。</li>
</ul>
<p>全连接层：通常，我们将批量规范化层置于全连接层中的仿射变换和激活函数之间。<br>卷积层：对于卷积层，我们可以在卷积层之后和非线性激活函数之前应用批量规范化。 当卷积有多个输出通道时，我们需要对这些通道的“每个”输出执行批量规范化，每个通道都有自己的拉伸（scale）和偏移（shift）参数，这两个参数都是标量。在计算平均值和方差时，我们会收集所有空间位置的值，然后在给定通道内应用相同的均值和方差，以便在每个空间位置对值进行规范化。<br>预测推理中的批量规范化：正如我们前面提到的，批量规范化在训练模式和预测模式下的行为通常不同。 首先，将训练好的模型用于预测时，我们不再需要样本均值中的噪声以及在微批次上估计每个小批次产生的样本方差了。 其次，例如，我们可能需要使用我们的模型对逐个样本进行预测。 一种常用的方法是通过移动平均估算整个训练数据集的样本均值和方差，并在预测时使用它们得到确定的输出。 可见，和暂退法一样，批量规范化层在训练模式和预测模式下的计算结果也是不一样的。</p>
<ul>
<li>5.3 从零实现Batchnorm<br>撇开算法细节，注意我们实现层的基础设计模式。 通常情况下，我们用一个单独的函数定义其数学原理，比如说batch_norm。 然后，我们将此功能集成到一个自定义层中，其代码主要处理数据移动到训练设备（如GPU）、分配和初始化任何必需的变量、跟踪移动平均线（此处为均值和方差）等问题。</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">def batch_norm(X, gamma, beta, moving_mean, moving_var, eps, momentum):</span><br><span class="line">    # 通过is_grad_enabled来判断当前模式是训练模式还是预测模式</span><br><span class="line">    if not torch.is_grad_enabled():</span><br><span class="line">        # 如果是在预测模式下，直接使用传入的移动平均所得的均值和方差</span><br><span class="line">        X_hat = (X - moving_mean) / torch.sqrt(moving_var + eps)</span><br><span class="line">    else:</span><br><span class="line">        assert len(X.shape) in (2, 4)</span><br><span class="line">        if len(X.shape) == 2:</span><br><span class="line">            # 使用全连接层的情况，计算特征维上的均值和方差</span><br><span class="line">            mean = X.mean(dim=0)</span><br><span class="line">            var = ((X - mean) ** 2).mean(dim=0)</span><br><span class="line">        else:</span><br><span class="line">            # 使用二维卷积层的情况，计算通道维上（axis=1）的均值和方差。</span><br><span class="line">            # 这里我们需要保持X的形状以便后面可以做广播运算</span><br><span class="line">            mean = X.mean(dim=(0, 2, 3), keepdim=True)</span><br><span class="line">            var = ((X - mean) ** 2).mean(dim=(0, 2, 3), keepdim=True)</span><br><span class="line">        # 训练模式下，用当前的均值和方差做标准化</span><br><span class="line">        X_hat = (X - mean) / torch.sqrt(var + eps)</span><br><span class="line">        # 更新移动平均的均值和方差</span><br><span class="line">        moving_mean = momentum * moving_mean + (1.0 - momentum) * mean</span><br><span class="line">        moving_var = momentum * moving_var + (1.0 - momentum) * var</span><br><span class="line">    Y = gamma * X_hat + beta  # 缩放和移位</span><br><span class="line">    return Y, moving_mean.data, moving_var.data</span><br><span class="line"></span><br><span class="line">class BatchNorm(nn.Module):</span><br><span class="line">    # num_features：完全连接层的输出数量或卷积层的输出通道数。</span><br><span class="line">    # num_dims：2表示完全连接层，4表示卷积层</span><br><span class="line">    def __init__(self, num_features, num_dims):</span><br><span class="line">        super().__init__()</span><br><span class="line">        if num_dims == 2:</span><br><span class="line">            shape = (1, num_features)</span><br><span class="line">        else:</span><br><span class="line">            shape = (1, num_features, 1, 1)</span><br><span class="line">        # 参与求梯度和迭代的拉伸和偏移参数，分别初始化成1和0</span><br><span class="line">        self.gamma = nn.Parameter(torch.ones(shape))</span><br><span class="line">        self.beta = nn.Parameter(torch.zeros(shape))</span><br><span class="line">        # 非模型参数的变量初始化为0和1</span><br><span class="line">        self.moving_mean = torch.zeros(shape)</span><br><span class="line">        self.moving_var = torch.ones(shape)</span><br><span class="line"></span><br><span class="line">    def forward(self, X):</span><br><span class="line">        # 如果X不在内存上，将moving_mean和moving_var</span><br><span class="line">        # 复制到X所在显存上</span><br><span class="line">        if self.moving_mean.device != X.device:</span><br><span class="line">            self.moving_mean = self.moving_mean.to(X.device)</span><br><span class="line">            self.moving_var = self.moving_var.to(X.device)</span><br><span class="line">        # 保存更新过的moving_mean和moving_var</span><br><span class="line">        Y, self.moving_mean, self.moving_var = batch_norm(</span><br><span class="line">            X, self.gamma, self.beta, self.moving_mean,</span><br><span class="line">            self.moving_var, eps=1e-5, momentum=0.9)</span><br><span class="line">        return Y</span><br></pre></td></tr></table></figure>

<ul>
<li>5.4 使用批量规范化的LeNet</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(</span><br><span class="line">    nn.Conv2d(1, 6, kernel_size=5), BatchNorm(6, num_dims=4), nn.Sigmoid(),</span><br><span class="line">    nn.AvgPool2d(kernel_size=2, stride=2),</span><br><span class="line">    nn.Conv2d(6, 16, kernel_size=5), BatchNorm(16, num_dims=4), nn.Sigmoid(),</span><br><span class="line">    nn.AvgPool2d(kernel_size=2, stride=2), nn.Flatten(),</span><br><span class="line">    nn.Linear(16*4*4, 120), BatchNorm(120, num_dims=2), nn.Sigmoid(),</span><br><span class="line">    nn.Linear(120, 84), BatchNorm(84, num_dims=2), nn.Sigmoid(),</span><br><span class="line">    nn.Linear(84, 10))</span><br></pre></td></tr></table></figure>
<p>由于引入了Batchnorm，训练时可以使用很大的学习率（例如1）</p>
<ul>
<li>5.5 争议<br>直观地说，批量规范化被认为可以使优化更加平滑。 然而，我们必须小心区分直觉和对我们观察到的现象的真实解释。</li>
</ul>
<p>在提出批量规范化的论文中，作者除了介绍了其应用，还解释了其原理：通过减少内部协变量偏移（internal covariate shift）。<br>据推测，作者所说的内部协变量转移类似于上述的投机直觉，即变量值的分布在训练过程中会发生变化。 然而，这种解释有两个问题： 1、这种偏移与严格定义的协变量偏移（covariate shift）非常不同，所以这个名字用词不当； 2、这种解释只提供了一种不明确的直觉，但留下了一个有待后续挖掘的问题：为什么这项技术如此有效？ 本书旨在传达实践者用来发展深层神经网络的直觉。 然而，重要的是将这些指导性直觉与既定的科学事实区分开来。 最终，当你掌握了这些方法，并开始撰写自己的研究论文时，你会希望清楚地区分技术和直觉。</p>
<p>随着批量规范化的普及，内部协变量偏移的解释反复出现在技术文献的辩论，特别是关于“如何展示机器学习研究”的更广泛的讨论中。 Ali Rahimi在接受2017年NeurIPS大会的“接受时间考验奖”（Test of Time Award）时发表了一篇令人难忘的演讲。他将“内部协变量转移”作为焦点，将现代深度学习的实践比作炼金术。 他对该示例进行了详细回顾 (Lipton and Steinhardt, 2018)，概述了机器学习中令人不安的趋势。 此外，一些作者对批量规范化的成功提出了另一种解释：在某些方面，批量规范化的表现出与原始论文 (Santurkar et al., 2018)中声称的行为是相反的。</p>
<p>然而，与机器学习文献中成千上万类似模糊的说法相比，内部协变量偏移没有更值得批评。 很可能，它作为这些辩论的焦点而产生共鸣，要归功于目标受众对它的广泛认可。 批量规范化已经被证明是一种不可或缺的方法。它适用于几乎所有图像分类器，并在学术界获得了数万引用。</p>
<ul>
<li>5.6 总结<br>在模型训练过程中，批量规范化利用小批量的均值和标准差，不断调整神经网络的中间输出，使整个神经网络各层的中间输出值更加稳定。</li>
</ul>
<p>批量规范化在全连接层和卷积层的使用略有不同。</p>
<p>批量规范化层和暂退层一样，在训练模式和预测模式下计算不同。</p>
<p>批量规范化有许多有益的副作用，主要是正则化。另一方面，”减少内部协变量偏移“的原始动机似乎不是一个有效的解释。</p>
<ol start="6">
<li>6.残差网络（ResNet）</li>
</ol>
<p>随着我们设计越来越深的网络，深刻理解“新添加的层如何提升神经网络的性能”变得至关重要。更重要的是设计网络的能力，在这种网络中，添加层会使网络更具表现力。</p>
<ul>
<li>6.1 函数类<br><img src="/../../../../gallery/Embeddingfunc.jpg" alt="Embeddingfunc"></li>
</ul>
<p>因此，只有当较复杂的函数类包含较小的函数类时，我们才能确保提高它们的性能。 对于深度神经网络，如果我们能将新添加的层训练成恒等映射（identity function）<br>f(x)&#x3D;x，新模型和原模型将同样有效。 同时，由于新模型可能得出更优的解来拟合训练数据集，因此添加层似乎更容易降低训练误差。</p>
<p>针对这一问题，何恺明等人提出了残差网络（ResNet） (He et al., 2016)。 它在2015年的ImageNet图像识别挑战赛夺魁，并深刻影响了后来的深度神经网络的设计。<br>残差网络的核心思想是：每个附加层都应该更容易地包含原始函数作为其元素之一。 于是，残差块（residual blocks）便诞生了，这个设计对如何建立深层神经网络产生了深远的影响。<br>凭借它，ResNet赢得了2015年ImageNet大规模视觉识别挑战赛。</p>
<ul>
<li>6.2 残差块<br><img src="/../../../../gallery/Resblock.jpg" alt="Resblock"></li>
</ul>
<p>ResNet沿用了VGG完整的3x3卷积层设计。 残差块里首先有2个有相同输出通道数的3x3卷积层。 每个卷积层后接一个批量规范化层和ReLU激活函数。 然后我们通过跨层数据通路，跳过这2个卷积运算，<br>将输入直接加在最后的ReLU激活函数前。 这样的设计要求2个卷积层的输出与输入形状一样，从而使它们可以相加。 如果想改变通道数，就需要引入一个额外的1x1卷积层来将输入变换成需要的形状后再做相加运算。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">class Residual(nn.Module):  #@save</span><br><span class="line">    def __init__(self, input_channels, num_channels,</span><br><span class="line">                 use_1x1conv=False, strides=1):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(input_channels, num_channels,</span><br><span class="line">                               kernel_size=3, padding=1, stride=strides)</span><br><span class="line">        self.conv2 = nn.Conv2d(num_channels, num_channels,</span><br><span class="line">                               kernel_size=3, padding=1)</span><br><span class="line">        if use_1x1conv:</span><br><span class="line">            self.conv3 = nn.Conv2d(input_channels, num_channels,</span><br><span class="line">                                   kernel_size=1, stride=strides)</span><br><span class="line">        else:</span><br><span class="line">            self.conv3 = None</span><br><span class="line">        self.bn1 = nn.BatchNorm2d(num_channels)</span><br><span class="line">        self.bn2 = nn.BatchNorm2d(num_channels)</span><br><span class="line"></span><br><span class="line">    def forward(self, X):</span><br><span class="line">        Y = F.relu(self.bn1(self.conv1(X)))</span><br><span class="line">        Y = self.bn2(self.conv2(Y))</span><br><span class="line">        if self.conv3:</span><br><span class="line">            X = self.conv3(X)</span><br><span class="line">        Y += X</span><br><span class="line">        return F.relu(Y)</span><br></pre></td></tr></table></figure>

<p><img src="/../../../../gallery/ResNet.jpg" alt="ResNet"></p>
<ul>
<li>6.3 ResNet模型<br>ResNet的前两层跟之前介绍的GoogLeNet中的一样： 在输出通道数为64、步幅为2的7x7卷积层后，接步幅为2的<br>3x3的最大汇聚层。不同之处在于ResNet每个卷积层后增加了批量规范化层。</li>
</ul>
<p><img src="/../../../../gallery/ResNet18.jpg" alt="ResNet-18"></p>
<ol start="7">
<li>7.稠密连接网络（DenseNet）<br>ResNet极大地改变了如何参数化深层网络中函数的观点。 稠密连接网络（DenseNet） (Huang et al., 2017)在某种程度上是ResNet的逻辑扩展。</li>
</ol>
<ul>
<li>7.1 从ResNet到DenseNet<br><img src="/../../../../gallery/DenseNet.jpg" alt="DenseNet"></li>
</ul>
<p><img src="/../../../../gallery/Denseconnect.jpg" alt="Denseconnect"></p>
<p>稠密网络主要由2部分构成：稠密块（dense block）和过渡层（transition layer）。 前者定义如何连接输入和输出，而后者则控制通道数量，使其不会太复杂。</p>
<ul>
<li><p>7.2 稠密块体</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">def conv_block(input_channels, num_channels):</span><br><span class="line">    return nn.Sequential(</span><br><span class="line">        nn.BatchNorm2d(input_channels), nn.ReLU(),</span><br><span class="line">        nn.Conv2d(input_channels, num_channels, kernel_size=3, padding=1))</span><br><span class="line"></span><br><span class="line">class DenseBlock(nn.Module):</span><br><span class="line">    def __init__(self, num_convs, input_channels, num_channels):</span><br><span class="line">        super(DenseBlock, self).__init__()</span><br><span class="line">        layer = []</span><br><span class="line">        for i in range(num_convs):</span><br><span class="line">            layer.append(conv_block(</span><br><span class="line">                num_channels * i + input_channels, num_channels))</span><br><span class="line">        self.net = nn.Sequential(*layer)</span><br><span class="line"></span><br><span class="line">    def forward(self, X):</span><br><span class="line">        for blk in self.net:</span><br><span class="line">            Y = blk(X)</span><br><span class="line">            # 连接通道维度上每个块的输入和输出</span><br><span class="line">            X = torch.cat((X, Y), dim=1)</span><br><span class="line">        return X</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>一个稠密块由多个卷积块组成，每个卷积块使用相同数量的输出通道。 然而，在前向传播中，我们将每个卷积块的输入和输出在通道维上连结。</p>
</li>
<li><p>7.3 过渡层<br>由于每个稠密块都会带来通道数的增加，使用过多则会过于复杂化模型。 而过渡层可以用来控制模型复杂度。 它通过<br>1X1卷积层来减小通道数，并使用步幅为2的平均汇聚层减半高和宽，从而进一步降低模型复杂度。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">def transition_block(input_channels, num_channels):</span><br><span class="line">    return nn.Sequential(</span><br><span class="line">        nn.BatchNorm2d(input_channels), nn.ReLU(),</span><br><span class="line">        nn.Conv2d(input_channels, num_channels, kernel_size=1),</span><br><span class="line">        nn.AvgPool2d(kernel_size=2, stride=2))</span><br></pre></td></tr></table></figure>
<p>对上一个例子中稠密块的输出使用通道数为10的过渡层。 此时输出的通道数减为10，高和宽均减半。</p>
</li>
<li><p>7.4 DenseNet模型<br>我们来构造DenseNet模型。DenseNet首先使用同ResNet一样的单卷积层和最大汇聚层。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">b1 = nn.Sequential(</span><br><span class="line">    nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),</span><br><span class="line">    nn.BatchNorm2d(64), nn.ReLU(),</span><br><span class="line">    nn.MaxPool2d(kernel_size=3, stride=2, padding=1))</span><br></pre></td></tr></table></figure>
<p>接下来，类似于ResNet使用的4个残差块，DenseNet使用的是4个稠密块。 与ResNet类似，我们可以设置每个稠密块使用多少个卷积层。 这里我们设成4，从而与 7.6节的ResNet-18保持一致。 稠密块里的卷积层通道数（即增长率）设为32，所以每个稠密块将增加128个通道。</p>
</li>
</ul>
<p>在每个模块之间，ResNet通过步幅为2的残差块减小高和宽，DenseNet则使用过渡层来减半高和宽，并减半通道数。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># num_channels为当前的通道数</span><br><span class="line">num_channels, growth_rate = 64, 32</span><br><span class="line">num_convs_in_dense_blocks = [4, 4, 4, 4]</span><br><span class="line">blks = []</span><br><span class="line">for i, num_convs in enumerate(num_convs_in_dense_blocks):</span><br><span class="line">    blks.append(DenseBlock(num_convs, num_channels, growth_rate))</span><br><span class="line">    # 上一个稠密块的输出通道数</span><br><span class="line">    num_channels += num_convs * growth_rate</span><br><span class="line">    # 在稠密块之间添加一个转换层，使通道数量减半</span><br><span class="line">    if i != len(num_convs_in_dense_blocks) - 1:</span><br><span class="line">        blks.append(transition_block(num_channels, num_channels // 2))</span><br><span class="line">        num_channels = num_channels // 2</span><br></pre></td></tr></table></figure>
<p>与ResNet类似，最后接上全局汇聚层和全连接层来输出结果。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(</span><br><span class="line">    b1, *blks,</span><br><span class="line">    nn.BatchNorm2d(num_channels), nn.ReLU(),</span><br><span class="line">    nn.AdaptiveAvgPool2d((1, 1)),</span><br><span class="line">    nn.Flatten(),</span><br><span class="line">    nn.Linear(num_channels, 10))</span><br></pre></td></tr></table></figure>

<ul>
<li>7.5 总结<br>在跨层连接上，不同于ResNet中将输入与输出相加，稠密连接网络（DenseNet）在通道维上连结输入与输出。</li>
</ul>
<p>DenseNet的主要构建模块是稠密块和过渡层。</p>
<p>在构建DenseNet时，我们需要通过添加过渡层来控制网络的维数，从而再次减少通道的数量。</p>
<h1 id="八、-循环神经网络-（2024-11-12更）"><a href="#八、-循环神经网络-（2024-11-12更）" class="headerlink" title="八、 循环神经网络 （2024&#x2F;11&#x2F;12更）"></a>八、 循环神经网络 （2024&#x2F;11&#x2F;12更）</h1><p><strong>目前为止我们默认数据都来自于某种分布， 并且所有样本都是独立同分布的 （independently and identically distributed，i.i.d.）。 然而，大多数的数据并非如此。</strong><br><strong>例如，文章中的单词是按顺序写的，如果顺序被随机地重排，就很难理解文章原始的意思。 同样，视频中的图像帧、对话中的音频信号以及网站上的浏览行为都是有顺序的。 因此，针对此类数据而设计特定模型，可能效果会更好。</strong><br><strong>另一个问题来自这样一个事实： 我们不仅仅可以接收一个序列作为输入，而是还可能期望继续猜测这个序列的后续。</strong><br><strong>简言之，如果说卷积神经网络可以有效地处理空间信息， 那么本章的循环神经网络（recurrent neural network，RNN）则可以更好地处理序列信息。 循环神经网络通过引入状态变量存储过去的信息和当前的输入，从而可以确定当前的输出。</strong></p>
<p><strong>许多使用循环网络的例子都是基于文本数据的，因此我们将在本章中重点介绍语言模型。 在对序列数据进行更详细的回顾之后，我们将介绍文本预处理的实用技术。 然后，我们将讨论语言模型的基本概念，并将此讨论作为循环神经网络设计的灵感。 最后，我们描述了循环神经网络的梯度计算方法，以探讨训练此类网络时可能遇到的问题。</strong></p>
<ol>
<li>1.序列模型</li>
</ol>
<ul>
<li><p>1.1 统计工具</p>
</li>
<li><p>1.1.1 自回归模型&#x2F;隐变量自回归模型</p>
</li>
<li><p>1.1.2 马尔可夫模型<br><img src="/../../../../gallery/Markov.jpg" alt="Markov"></p>
</li>
<li><p>1.1.3 因果关系</p>
</li>
</ul>
<p>事实上，如果基于一个马尔可夫模型， 我们还可以得到一个反向的条件概率分布。 然而，在许多情况下，数据存在一个自然的方向，即在时间上是前进的。 很明显，未来的事件不能影响过去。 因此，如果我们改变x(t)，可能会影响未来发生的事情<br>X(t+1)，但不能反过来。 也就是说，如果我们改变X(t)，基于过去事件得到的分布不会改变。 因此，解释P(xt+1|xt)<br>应该比解释P(xt|xt+1)更容易。 例如，在某些情况下，对于某些可加性噪声e， 显然我们可以找到x(t+1)&#x3D;f(xt)+e， 而反之则不行 (Hoyer et al., 2009)。 而这个向前推进的方向恰好也是我们通常感兴趣的方向。 彼得斯等人 (Peters et al., 2017) 对该主题的更多内容做了详尽的解释，而我们的上述讨论只是其中的冰山一角。</p>
<ul>
<li><p>1.2 训练</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch import nn</span><br><span class="line">from d2l import torch as d2l</span><br><span class="line"></span><br><span class="line">T = 1000  # 总共产生1000个点</span><br><span class="line">time = torch.arange(1, T + 1, dtype=torch.float32)</span><br><span class="line">x = torch.sin(0.01 * time) + torch.normal(0, 0.2, (T,))</span><br><span class="line">d2l.plot(time, [x], &#x27;time&#x27;, &#x27;x&#x27;, xlim=[1, 1000], figsize=(6, 3))</span><br><span class="line"></span><br><span class="line">tau = 4</span><br><span class="line">features = torch.zeros((T - tau, tau))</span><br><span class="line">for i in range(tau):</span><br><span class="line">    features[:, i] = x[i: T - tau + i]</span><br><span class="line">labels = x[tau:].reshape((-1, 1))</span><br><span class="line"></span><br><span class="line">batch_size, n_train = 16, 600</span><br><span class="line"># 只有前n_train个样本用于训练</span><br><span class="line">train_iter = d2l.load_array((features[:n_train], labels[:n_train]),</span><br><span class="line">                            batch_size, is_train=True)</span><br><span class="line"></span><br><span class="line">在这里，我们使用一个相当简单的架构训练模型： 一个拥有两个全连接层的多层感知机，ReLU激活函数和平方损失。</span><br><span class="line"></span><br><span class="line"># 初始化网络权重的函数</span><br><span class="line">def init_weights(m):</span><br><span class="line">    if type(m) == nn.Linear:</span><br><span class="line">        nn.init.xavier_uniform_(m.weight)</span><br><span class="line"></span><br><span class="line"># 一个简单的多层感知机</span><br><span class="line">def get_net():</span><br><span class="line">    net = nn.Sequential(nn.Linear(4, 10),</span><br><span class="line">                        nn.ReLU(),</span><br><span class="line">                        nn.Linear(10, 1))</span><br><span class="line">    net.apply(init_weights)</span><br><span class="line">    return net</span><br><span class="line"></span><br><span class="line"># 平方损失。注意：MSELoss计算平方误差时不带系数1/2</span><br><span class="line">loss = nn.MSELoss(reduction=&#x27;none&#x27;)</span><br><span class="line"></span><br><span class="line">def train(net, train_iter, loss, epochs, lr):</span><br><span class="line">    trainer = torch.optim.Adam(net.parameters(), lr)</span><br><span class="line">    for epoch in range(epochs):</span><br><span class="line">        for X, y in train_iter:</span><br><span class="line">            trainer.zero_grad()</span><br><span class="line">            l = loss(net(X), y)</span><br><span class="line">            l.sum().backward()</span><br><span class="line">            trainer.step()</span><br><span class="line">        print(f&#x27;epoch &#123;epoch + 1&#125;, &#x27;</span><br><span class="line">              f&#x27;loss: &#123;d2l.evaluate_loss(net, train_iter, loss):f&#125;&#x27;)</span><br><span class="line"></span><br><span class="line">net = get_net()</span><br><span class="line">train(net, train_iter, loss, 5, 0.01)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
</li>
<li><p>1.3 预测<br>由于训练损失很小，因此我们期望模型能有很好的工作效果。 让我们看看这在实践中意味着什么。 首先是检查模型预测下一个时间步的能力， 也就是单步预测（one-step-ahead prediction）。</p>
</li>
</ul>
<p>通常，对于直到xt的观测序列，其在时间步t+k处的预测输出x(t+k)称为k步预测（k-step-ahead-prediction）。 由于我们的观察已经到了x604<br>，它的k步预测是x(604+k)。换句话说，我们必须使用我们自己的预测（而不是原始数据）来进行多步预测。 让我们看看效果如何。</p>
<ul>
<li>1.4 总结</li>
</ul>
<p>内插法（在现有观测值之间进行估计）和外推法（对超出已知观测范围进行预测）在实践的难度上差别很大。因此，对于所拥有的序列数据，在训练时始终要尊重其时间顺序，即最好不要基于未来的数据进行训练。</p>
<p>序列模型的估计需要专门的统计工具，两种较流行的选择是自回归模型和隐变量自回归模型。</p>
<p>对于时间是向前推进的因果模型，正向估计通常比反向估计更容易。</p>
<p>对于直到时间步t的观测序列，其在时间步t+k的预测输出是“k步预测”。随着我们对预测时间k值的增加，会造成误差的快速累积和预测质量的极速下降。<br>(单步预测使用真实值预测未来一步；多步预测包含使用预测值持续预测到未来k步)</p>
<ol start="2">
<li>2.文本预处理<br>对于序列数据处理问题，我们在 8.1节中 评估了所需的统计工具和预测时面临的挑战。 这样的数据存在许多种形式，文本是最常见例子之一。 例如，一篇文章可以被简单地看作一串单词序列，甚至是一串字符序列。 本节中，我们将解析文本的常见预处理步骤。 这些步骤通常包括：</li>
</ol>
<p>将文本作为字符串加载到内存中。</p>
<p>将字符串拆分为词元（如单词和字符）。</p>
<p>建立一个词表，将拆分的词元映射到数字索引。</p>
<p>将文本转换为数字索引序列，方便模型操作。</p>
<ul>
<li><p>2.1 读取数据集<br>从H.G.Well的时光机器中加载文本。 这是一个相当小的语料库，只有30000多个单词，但足够我们小试牛刀， 而现实中的文档集合可能会包含数十亿个单词。 下面的函数将数据集读取到由多条文本行组成的列表中，其中每条文本行都是一个字符串。 为简单起见，我们在这里忽略了标点符号和字母大写。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">#@save</span><br><span class="line">d2l.DATA_HUB[&#x27;time_machine&#x27;] = (d2l.DATA_URL + &#x27;timemachine.txt&#x27;,</span><br><span class="line">                                &#x27;090b5e7e70c295757f55df93cb0a180b9691891a&#x27;)</span><br><span class="line"></span><br><span class="line">def read_time_machine():  #@save</span><br><span class="line">    &quot;&quot;&quot;将时间机器数据集加载到文本行的列表中&quot;&quot;&quot;</span><br><span class="line">    with open(d2l.download(&#x27;time_machine&#x27;), &#x27;r&#x27;) as f:</span><br><span class="line">        lines = f.readlines()</span><br><span class="line">    return [re.sub(&#x27;[^A-Za-z]+&#x27;, &#x27; &#x27;, line).strip().lower() for line in lines]</span><br><span class="line"></span><br><span class="line">lines = read_time_machine()</span><br><span class="line">print(f&#x27;# 文本总行数: &#123;len(lines)&#125;&#x27;)</span><br><span class="line">print(lines[0])</span><br></pre></td></tr></table></figure>
</li>
<li><p>2.2 词元化<br>下面的tokenize函数将文本行列表（lines）作为输入， 列表中的每个元素是一个文本序列（如一条文本行）。 每个文本序列又被拆分成一个词元列表，词元（token）是文本的基本单位。 最后，返回一个由词元列表组成的列表，其中的每个词元都是一个字符串（string）。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">def tokenize(lines, token=&#x27;word&#x27;):  #@save</span><br><span class="line">    &quot;&quot;&quot;将文本行拆分为单词或字符词元&quot;&quot;&quot;</span><br><span class="line">    if token == &#x27;word&#x27;:</span><br><span class="line">        return [line.split() for line in lines]</span><br><span class="line">    elif token == &#x27;char&#x27;:</span><br><span class="line">        return [list(line) for line in lines]</span><br><span class="line">    else:</span><br><span class="line">        print(&#x27;错误：未知词元类型：&#x27; + token)</span><br><span class="line"></span><br><span class="line">tokens = tokenize(lines)</span><br><span class="line">for i in range(11):</span><br><span class="line">    print(tokens[i])</span><br></pre></td></tr></table></figure>
</li>
<li><p>2.3 词表<br>词元的类型是字符串，而模型需要的输入是数字，因此这种类型不方便模型使用。 现在，让我们构建一个字典，通常也叫做词表（vocabulary）， 用来将字符串类型的词元映射到从<br>0开始的数字索引中。 我们先将训练集中的所有文档合并在一起，对它们的唯一词元进行统计， 得到的统计结果称之为语料（corpus）。 然后根据每个唯一词元的出现频率，为其分配一个数字索引。<br>很少出现的词元通常被移除，这可以降低复杂性。 另外，语料库中不存在或已删除的任何词元都将映射到一个特定的未知词元“<unk>”。 我们可以选择增加一个列表，用于保存那些被保留的词元， 例如：填充词元（“<pad>”）； 序列开始词元（“<bos>”）； 序列结束词元（“<eos>”）。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">class Vocab:  #@save</span><br><span class="line">    &quot;&quot;&quot;文本词表&quot;&quot;&quot;</span><br><span class="line">    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):</span><br><span class="line">        if tokens is None:</span><br><span class="line">            tokens = []</span><br><span class="line">        if reserved_tokens is None:</span><br><span class="line">            reserved_tokens = []</span><br><span class="line">        # 按出现频率排序</span><br><span class="line">        counter = count_corpus(tokens)</span><br><span class="line">        self._token_freqs = sorted(counter.items(), key=lambda x: x[1],</span><br><span class="line">                                   reverse=True)</span><br><span class="line">        # 未知词元的索引为0</span><br><span class="line">        self.idx_to_token = [&#x27;&lt;unk&gt;&#x27;] + reserved_tokens</span><br><span class="line">        self.token_to_idx = &#123;token: idx</span><br><span class="line">                             for idx, token in enumerate(self.idx_to_token)&#125;</span><br><span class="line">        for token, freq in self._token_freqs:</span><br><span class="line">            if freq &lt; min_freq:</span><br><span class="line">                break</span><br><span class="line">            if token not in self.token_to_idx:</span><br><span class="line">                self.idx_to_token.append(token)</span><br><span class="line">                self.token_to_idx[token] = len(self.idx_to_token) - 1</span><br><span class="line"></span><br><span class="line">    def __len__(self):</span><br><span class="line">        return len(self.idx_to_token)</span><br><span class="line"></span><br><span class="line">    def __getitem__(self, tokens):</span><br><span class="line">        if not isinstance(tokens, (list, tuple)):</span><br><span class="line">            return self.token_to_idx.get(tokens, self.unk)</span><br><span class="line">        return [self.__getitem__(token) for token in tokens]</span><br><span class="line"></span><br><span class="line">    def to_tokens(self, indices):</span><br><span class="line">        if not isinstance(indices, (list, tuple)):</span><br><span class="line">            return self.idx_to_token[indices]</span><br><span class="line">        return [self.idx_to_token[index] for index in indices]</span><br><span class="line"></span><br><span class="line">    @property</span><br><span class="line">    def unk(self):  # 未知词元的索引为0</span><br><span class="line">        return 0</span><br><span class="line"></span><br><span class="line">    @property</span><br><span class="line">    def token_freqs(self):</span><br><span class="line">        return self._token_freqs</span><br><span class="line"></span><br><span class="line">def count_corpus(tokens):  #@save</span><br><span class="line">    &quot;&quot;&quot;统计词元的频率&quot;&quot;&quot;</span><br><span class="line">    # 这里的tokens是1D列表或2D列表</span><br><span class="line">    if len(tokens) == 0 or isinstance(tokens[0], list):</span><br><span class="line">        # 将词元列表展平成一个列表</span><br><span class="line">        tokens = [token for line in tokens for token in line]</span><br><span class="line">    return collections.Counter(tokens)</span><br></pre></td></tr></table></figure>
<p>我们首先使用时光机器数据集作为语料库来构建词表，然后打印前几个高频词元及其索引。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vocab = Vocab(tokens)</span><br><span class="line">print(list(vocab.token_to_idx.items())[:10])</span><br><span class="line"># [(&#x27;&lt;unk&gt;&#x27;, 0), (&#x27;the&#x27;, 1), (&#x27;i&#x27;, 2), (&#x27;and&#x27;, 3), (&#x27;of&#x27;, 4), (&#x27;a&#x27;, 5), (&#x27;to&#x27;, 6), (&#x27;was&#x27;, 7), (&#x27;in&#x27;, 8), (&#x27;that&#x27;, 9)]</span><br></pre></td></tr></table></figure>
<p>现在，我们可以将每一条文本行转换成一个数字索引列表。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">for i in [0, 10]:</span><br><span class="line">    print(&#x27;文本:&#x27;, tokens[i])</span><br><span class="line">    print(&#x27;索引:&#x27;, vocab[tokens[i]])</span><br><span class="line"></span><br><span class="line">#文本: [&#x27;the&#x27;, &#x27;time&#x27;, &#x27;machine&#x27;, &#x27;by&#x27;, &#x27;h&#x27;, &#x27;g&#x27;, &#x27;wells&#x27;]</span><br><span class="line">#索引: [1, 19, 50, 40, 2183, 2184, 400]</span><br><span class="line">#文本: [&#x27;twinkled&#x27;, &#x27;and&#x27;, &#x27;his&#x27;, &#x27;usually&#x27;, &#x27;pale&#x27;, &#x27;face&#x27;, &#x27;was&#x27;, &#x27;flushed&#x27;, &#x27;and&#x27;, &#x27;animated&#x27;, &#x27;the&#x27;]</span><br><span class="line">#索引: [2186, 3, 25, 1044, 362, 113, 7, 1421, 3, 1045, 1]</span><br></pre></td></tr></table></figure>
</li>
<li><p>2.4 整合功能<br>在使用上述函数时，我们将所有功能打包到load_corpus_time_machine函数中， 该函数返回corpus（词元索引列表）和vocab（时光机器语料库的词表）。 我们在这里所做的改变是：</p>
</li>
</ul>
<p>为了简化后面章节中的训练，我们使用字符（而不是单词）实现文本词元化；</p>
<p>时光机器数据集中的每个文本行不一定是一个句子或一个段落，还可能是一个单词，因此返回的corpus仅处理为单个列表，而不是使用多词元列表构成的一个列表。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">def load_corpus_time_machine(max_tokens=-1):  #@save</span><br><span class="line">    &quot;&quot;&quot;返回时光机器数据集的词元索引列表和词表&quot;&quot;&quot;</span><br><span class="line">    lines = read_time_machine()</span><br><span class="line">    tokens = tokenize(lines, &#x27;char&#x27;)</span><br><span class="line">    vocab = Vocab(tokens)</span><br><span class="line">    # 因为时光机器数据集中的每个文本行不一定是一个句子或一个段落，</span><br><span class="line">    # 所以将所有文本行展平到一个列表中</span><br><span class="line">    corpus = [vocab[token] for line in tokens for token in line]</span><br><span class="line">    if max_tokens &gt; 0:</span><br><span class="line">        corpus = corpus[:max_tokens]</span><br><span class="line">    return corpus, vocab</span><br><span class="line"></span><br><span class="line">corpus, vocab = load_corpus_time_machine()</span><br><span class="line">len(corpus), len(vocab)</span><br><span class="line"># (170580, 28)</span><br></pre></td></tr></table></figure>

<ul>
<li>2.5 总结<br>文本是序列数据的一种最常见的形式之一。</li>
</ul>
<p>为了对文本进行预处理，我们通常将文本拆分为词元，构建词表将词元字符串映射为数字索引，并将文本数据转换为词元索引以供模型操作。</p>
<ol start="3">
<li>3.语言模型和数据集<br>我们了解了如何将文本数据映射为词元，以及将这些词元可以视为一系列离散的观测，例如单词或字符。假设长度为T的文本序列中的词元依次为x1，x2，…,xT。 于是，xt（1≤t≤T）可以被认为是文本序列在时间步t处的观测或标签。 在给定这样的文本序列时，语言模型（language model）的目标是估计序列的联合概率P(x1,x2,…,xT)</li>
</ol>
<ul>
<li><p>3.1 学习语言模型<br><img src="/../../../../gallery/NLP1.jpg" alt="NLP1"></p>
</li>
<li><p>3.2 马尔可夫模型与n元语法<br><img src="/../../../../gallery/NLP832.jpg" alt="NLP832"></p>
</li>
<li><p>3.3 自然语言统计<br>词频以一种明确的方式迅速衰减。 将前几个单词作为例外消除后，剩余的所有单词大致遵循双对数坐标图上的一条直线。 这意味着单词的频率满足齐普夫定律（Zipf’s law）， 即第i个最常用单词的频率ni为：<br>ni 正比于 1&#x2F;(i**α) -等价于- logni&#x3D;-αlogi+c<br><img src="/../../../../gallery/NLP833.jpg" alt="NLP833"></p>
</li>
<li><p>3.4 读取长序列数据<br>当序列变得太长而不能被模型一次性全部处理时， 我们可能希望拆分这样的序列方便模型读取。 假设我们将使用神经网络来训练语言模型， 模型中的网络一次处理具有预定义长度 （例如n个时间步）的一个小批量序列。 现在的问题是如何随机生成一个小批量数据的特征和标签以供读取。<br>首先，由于文本序列可以是任意长的， 例如整本《时光机器》（The Time Machine）， 于是任意长的序列可以被我们划分为具有相同时间步数的子序列。 当训练我们的神经网络时，这样的小批量子序列将被输入到模型中。 假设网络一次只处理具有n个时间步的子序列。 图8.3.1画出了 从原始文本序列获得子序列的所有不同的方式， 其中n&#x3D;5，并且每个时间步的词元对应于一个字符。 请注意，因为我们可以选择任意偏移量来指示初始位置，所以我们有相当大的自由度。<br><img src="/../../../../gallery/NLP8340.jpg" alt="NLP8340"><br>因此，我们应该从 图8.3.1中选择哪一个呢？ 事实上，他们都一样的好。 然而，如果我们只选择一个偏移量， 那么用于训练网络的、所有可能的子序列的覆盖范围将是有限的。 因此，我们可以从随机偏移量开始划分序列， 以同时获得覆盖性（coverage）和随机性（randomness）。 下面，我们将描述如何实现随机采样（random sampling）和 顺序分区（sequential partitioning）策略。</p>
</li>
</ul>
<p>随机采样：<br>在随机采样中，每个样本都是在原始的长序列上任意捕获的子序列。 在迭代过程中，来自两个相邻的、随机的、小批量中的子序列不一定在原始序列上相邻。 对于语言建模，目标是基于到目前为止我们看到的词元来预测下一个词元， 因此标签是移位了一个词元的原始序列。</p>
<p>顺序分区：<br>在迭代过程中，除了对原始序列可以随机抽样外， 我们还可以保证两个相邻的小批量中的子序列在原始序列上也是相邻的。 这种策略在基于小批量的迭代过程中保留了拆分的子序列的顺序，因此称为顺序分区。</p>
<ul>
<li>3.5 总结<br>语言模型是自然语言处理的关键。</li>
</ul>
<p>n元语法通过截断相关性，为处理长序列提供了一种实用的模型。</p>
<p>长序列存在一个问题：它们很少出现或者从不出现。</p>
<p>齐普夫定律支配着单词的分布，这个分布不仅适用于一元语法，还适用于其他n元语法。</p>
<p>通过拉普拉斯平滑法可以有效地处理结构丰富而频率不足的低频词词组。</p>
<p>读取长序列的主要方式是随机采样和顺序分区。在迭代过程中，后者可以保证来自两个相邻的小批量中的子序列在原始序列上也是相邻的。</p>
<ol start="4">
<li>4.循环神经网络 （2024&#x2F;11&#x2F;13更）<br>P(xt|xt-1,…,x1) ≈ P(xt|ht-1)<br>隐状态整合了先前时间步上的信息，相比于n元语法模型可能更高效，因为词表体积会随着n的取值指数增加。</li>
</ol>
<p>隐藏层和隐状态指的是两个截然不同的概念。 如上所述，隐藏层是在从输入到输出的路径上（以观测角度来理解）的隐藏的层， 而隐状态则是在给定步骤所做的任何事情（以技术角度来定义）的输入， 并且这些状态只能通过先前时间步的数据来计算。</p>
<p>循环神经网络（recurrent neural networks，RNNs） 是具有隐状态的神经网络。 在介绍循环神经网络模型之前， 我们首先回顾 4.1节中介绍的多层感知机模型。</p>
<ul>
<li><p>4.1 无隐状态的神经网络——含隐藏层的多层感知机</p>
</li>
<li><p>4.2 有隐状态的循环神经网络<br>当引入隐藏状态，我们用Xt每一行表示t时刻的序列中的一个样本，Ht表示实践部t的隐藏变量。我们保存前一个时间步隐藏变量Ht-1，并引入新的权重参数Whh。具体的，当前时间步隐变量由当前时间步输入和前一个时间步隐变量共同决定： Ht&#x3D;ɸ(Xt·Wxh+Ht-1·Whh+bh) （8.4.5）</p>
</li>
</ul>
<p>从相邻时间步的隐藏变量Ht和 Ht-1之间的关系可知， 这些变量捕获并保留了序列直到其当前时间步的历史信息， 就如当前时间步下神经网络的状态或记忆， 因此这样的隐藏变量被称为隐状态（hidden state）。 由于在当前时间步中， 隐状态使用的定义与前一个时间步中使用的定义相同， 因此 (8.4.5)的计算是循环的（recurrent）。 于是基于循环计算的隐状态神经网络被命名为 循环神经网络（recurrent neural network）。 在循环神经网络中执行 (8.4.5)计算的层 称为循环层（recurrent layer）。</p>
<p>有许多不同的方法可以构建循环神经网络， 由 (8.4.5)定义的隐状态的循环神经网络是非常常见的一种。 对于时间步<br>，输出层的输出类似于多层感知机中的计算： Ot&#x3D;Ht·Whq+bq<br>参数包括隐层权重Wxh，Whh，偏置bh，输出层权重Whq和偏执bq。值得一提的是，即使在不同的时间步，循环神经网络也总是使用这些模型参数。 因此，循环神经网络的参数开销不会随着时间步的增加而增加。<br><img src="/../../../../gallery/RNN.jpg" alt="RNN"></p>
<p>一个运算中的数学性质(分块矩阵性质)：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">X, W_xh = torch.normal(0, 1, (3, 1)), torch.normal(0, 1, (1, 4))</span><br><span class="line">H, W_hh = torch.normal(0, 1, (3, 4)), torch.normal(0, 1, (4, 4))</span><br><span class="line">torch.matmul(X, W_xh) + torch.matmul(H, W_hh)</span><br><span class="line"># 等价于下面</span><br><span class="line">torch.matmul(torch.cat((X, H), 1), torch.cat((W_xh, W_hh), 0))</span><br></pre></td></tr></table></figure>

<ul>
<li><p>4.3 基于循环神经网络的字符级语言模型<br>我们的目标是根据过去的和当前的词元预测下一个词元， 因此我们将原始序列移位一个词元作为标签。 Bengio等人首先提出使用神经网络进行语言建模 (Bengio et al., 2003)。 接下来，我们看一下如何使用循环神经网络来构建语言模型。 设小批量大小为1，批量中的文本序列为“machine”。 为了简化后续部分的训练，我们考虑使用 字符级语言模型（character-level language model）， 将文本词元化为字符而不是单词。 图8.4.2演示了 如何通过基于字符级语言建模的循环神经网络， 使用当前的和先前的字符预测下一个字符。<br><img src="/../../../../gallery/RNN2.jpg" alt="RNN2"><br>在训练过程中，我们对每个时间步的输出层的输出进行softmax操作， 然后利用交叉熵损失计算模型输出和标签之间的误差。 由于隐藏层中隐状态的循环计算， 图8.4.2中的第3个时间步的输出O3由文本序列“m”“a”和“c”确定。由于训练数据中这个文本序列的下一个字符是“h”， 因此第3个时间步的损失将取决于下一个字符的概率分布， 而下一个字符是基于特征序列“m”“a”“c”和这个时间步的标签“h”生成的。<br>在实践中，我们使用的批量大小为n＞1，每个词元都由一个d维向量表示。 因此，在时间步t输入Xt将是一个n×d矩阵， 这与我们在 4.2节中的讨论相同。</p>
</li>
<li><p>4.4 困惑度（Perplexity）<br>最后，让我们讨论如何度量语言模型的质量， 这将在后续部分中用于评估基于循环神经网络的模型。 一个好的语言模型能够用高度准确的词元来预测我们接下来会看到什么。 考虑一下由不同的语言模型给出的对“It is raining …”（“…下雨了”）的续写：</p>
</li>
</ul>
<p>“It is raining outside”（外面下雨了）；</p>
<p>“It is raining banana tree”（香蕉树下雨了）；</p>
<p>“It is raining piouw;kcj pwepoiut”（piouw;kcj pwepoiut下雨了）。</p>
<p>就质量而言，例1显然是最合乎情理、在逻辑上最连贯的。 虽然这个模型可能没有很准确地反映出后续词的语义， 比如，“It is raining in San Francisco”（旧金山下雨了） 和“It is raining in winter”（冬天下雨了） 可能才是更完美的合理扩展， 但该模型已经能够捕捉到跟在后面的是哪类单词。 例2则要糟糕得多，因为其产生了一个无意义的续写。 尽管如此，至少该模型已经学会了如何拼写单词， 以及单词之间的某种程度的相关性。 最后，例3表明了训练不足的模型是无法正确地拟合数据的。<br><img src="/../../../../gallery/Perplexity.jpg" alt="Perplexity"></p>
<ul>
<li>4.5 总结<br>对隐状态使用循环计算的神经网络称为循环神经网络（RNN）。</li>
</ul>
<p>循环神经网络的隐状态可以捕获直到当前时间步序列的历史信息。</p>
<p>循环神经网络模型的参数数量不会随着时间步的增加而增加。</p>
<p>我们可以使用循环神经网络创建字符级语言模型。</p>
<p>我们可以使用困惑度来评价语言模型的质量。</p>
<ol start="5">
<li>5.循环神经网络的从零开始实现</li>
</ol>
<ul>
<li>5.1 独热编码<br>回想一下，在train_iter中，每个词元都表示为一个数字索引， 将这些索引直接输入神经网络可能会使学习变得困难。 我们通常将每个词元表示为更具表现力的特征向量。 最简单的表示称为独热编码（one-hot encoding）。</li>
</ul>
<p>我们每次采样的小批量数据形状是二维张量： （批量大小，时间步数）。 one_hot函数将这样一个小批量数据转换成三维张量， 张量的最后一个维度等于词表大小（len(vocab)）。 我们经常转换输入的维度，以便获得形状为 （时间步数，批量大小，词表大小）的输出。 这将使我们能够更方便地通过最外层的维度， 一步一步地更新小批量数据的隐状态。</p>
<ul>
<li><p>5.2 初始化模型参数<br>接下来，我们初始化循环神经网络模型的模型参数。 隐藏单元数num_hiddens是一个可调的超参数。 当训练语言模型时，输入和输出来自相同的词表。 因此，它们具有相同的维度，即词表的大小。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">def get_params(vocab_size, num_hiddens, device):</span><br><span class="line">    num_inputs = num_outputs = vocab_size</span><br><span class="line"></span><br><span class="line">    def normal(shape):</span><br><span class="line">        return torch.randn(size=shape, device=device) * 0.01</span><br><span class="line"></span><br><span class="line">    # 隐藏层参数</span><br><span class="line">    W_xh = normal((num_inputs, num_hiddens))</span><br><span class="line">    W_hh = normal((num_hiddens, num_hiddens))</span><br><span class="line">    b_h = torch.zeros(num_hiddens, device=device)</span><br><span class="line">    # 输出层参数</span><br><span class="line">    W_hq = normal((num_hiddens, num_outputs))</span><br><span class="line">    b_q = torch.zeros(num_outputs, device=device)</span><br><span class="line">    # 附加梯度</span><br><span class="line">    params = [W_xh, W_hh, b_h, W_hq, b_q]</span><br><span class="line">    for param in params:</span><br><span class="line">        param.requires_grad_(True)</span><br><span class="line">    return params</span><br></pre></td></tr></table></figure></li>
<li><p>5.3 循环神经网络模型<br>为了定义循环神经网络模型， 我们首先需要一个init_rnn_state函数在初始化时返回隐状态。 这个函数的返回是一个张量，张量全用0填充， 形状为（批量大小，隐藏单元数）。 在后面的章节中我们将会遇到隐状态包含多个变量的情况， 而使用元组可以更容易地处理些。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">def init_rnn_state(batch_size, num_hiddens, device):</span><br><span class="line">    return (torch.zeros((batch_size, num_hiddens), device=device), )</span><br></pre></td></tr></table></figure>
<p>下面的rnn函数定义了如何在一个时间步内计算隐状态和输出。 循环神经网络模型通过inputs最外层的维度实现循环， 以便逐时间步更新小批量数据的隐状态H。 此外，这里使用tanh函数作为激活函数。 如 4.1节所述， 当元素在实数上满足均匀分布时，tanh函数的平均值为0</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">def rnn(inputs, state, params):</span><br><span class="line">    # inputs的形状：(时间步数量，批量大小，词表大小)</span><br><span class="line">    W_xh, W_hh, b_h, W_hq, b_q = params</span><br><span class="line">    H, = state</span><br><span class="line">    outputs = []</span><br><span class="line">    # X的形状：(批量大小，词表大小)</span><br><span class="line">    for X in inputs:</span><br><span class="line">        H = torch.tanh(torch.mm(X, W_xh) + torch.mm(H, W_hh) + b_h)</span><br><span class="line">        Y = torch.mm(H, W_hq) + b_q</span><br><span class="line">        outputs.append(Y)</span><br><span class="line">    return torch.cat(outputs, dim=0), (H,)</span><br></pre></td></tr></table></figure>
<p>定义了所有需要的函数之后，接下来我们创建一个类来包装这些函数， 并存储从零开始实现的循环神经网络模型的参数。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">class RNNModelScratch: #@save</span><br><span class="line">    &quot;&quot;&quot;从零开始实现的循环神经网络模型&quot;&quot;&quot;</span><br><span class="line">    def __init__(self, vocab_size, num_hiddens, device,</span><br><span class="line">                 get_params, init_state, forward_fn):</span><br><span class="line">        self.vocab_size, self.num_hiddens = vocab_size, num_hiddens</span><br><span class="line">        self.params = get_params(vocab_size, num_hiddens, device)</span><br><span class="line">        self.init_state, self.forward_fn = init_state, forward_fn</span><br><span class="line"></span><br><span class="line">    def __call__(self, X, state):</span><br><span class="line">        X = F.one_hot(X.T, self.vocab_size).type(torch.float32)</span><br><span class="line">        return self.forward_fn(X, state, self.params)</span><br><span class="line"></span><br><span class="line">    def begin_state(self, batch_size, device):</span><br><span class="line">        return self.init_state(batch_size, self.num_hiddens, device)</span><br></pre></td></tr></table></figure>
</li>
<li><p>5.4 预测<br>让我们首先定义预测函数来生成prefix之后的新字符， 其中的prefix是一个用户提供的包含多个字符的字符串。 在循环遍历prefix中的开始字符时， 我们不断地将隐状态传递到下一个时间步，但是不生成任何输出。 这被称为预热（warm-up）期， 因为在此期间模型会自我更新（例如，更新隐状态）， 但不会进行预测。 预热期结束后，隐状态的值通常比刚开始的初始值更适合预测， 从而预测字符并输出它们。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">def predict_ch8(prefix, num_preds, net, vocab, device):  #@save</span><br><span class="line">    &quot;&quot;&quot;在prefix后面生成新字符&quot;&quot;&quot;</span><br><span class="line">    state = net.begin_state(batch_size=1, device=device)</span><br><span class="line">    outputs = [vocab[prefix[0]]]</span><br><span class="line">    get_input = lambda: torch.tensor([outputs[-1]], device=device).reshape((1, 1))</span><br><span class="line">    for y in prefix[1:]:  # 预热期</span><br><span class="line">        _, state = net(get_input(), state)</span><br><span class="line">        outputs.append(vocab[y])</span><br><span class="line">    for _ in range(num_preds):  # 预测num_preds步</span><br><span class="line">        y, state = net(get_input(), state)</span><br><span class="line">        outputs.append(int(y.argmax(dim=1).reshape(1)))</span><br><span class="line">    return &#x27;&#x27;.join([vocab.idx_to_token[i] for i in outputs])</span><br></pre></td></tr></table></figure>
<p>现在我们可以测试predict_ch8函数。 我们将前缀指定为time traveller， 并基于这个前缀生成10个后续字符。 鉴于我们还没有训练网络，它会生成荒谬的预测结果。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">predict_ch8(&#x27;time traveller &#x27;, 10, net, vocab, d2l.try_gpu())</span><br><span class="line">#&#x27;time traveller aaaaaaaaaa&#x27;</span><br></pre></td></tr></table></figure>
</li>
<li><p>5.5 梯度裁剪<br>对于长度为T的序列，我们在迭代中计算这T个时间步上的梯度， 将会在反向传播过程中产生长度为O(T)的矩阵乘法链。 如 4.8节所述， 当T<br>较大时，它可能导致数值不稳定， 例如可能导致梯度爆炸或梯度消失。 因此，循环神经网络模型往往需要额外的方式来支持稳定训练。<br><img src="/../../../../gallery/gradientclip.jpg" alt="gradientclip"></p>
</li>
<li><p>5.6 训练<br>在训练模型之前，让我们定义一个函数在一个迭代周期内训练模型。 它与我们训练前面的模型的方式有三个不同之处。</p>
</li>
</ul>
<p>序列数据的不同采样方法（随机采样和顺序分区）将导致隐状态初始化的差异。</p>
<p>我们在更新模型参数之前裁剪梯度。 这样的操作的目的是，即使训练过程中某个点上发生了梯度爆炸，也能保证模型不会发散。</p>
<p>我们用困惑度来评价模型。如 8.4.4节所述， 这样的度量确保了不同长度的序列具有可比性。</p>
<p>具体来说，当使用顺序分区时， 我们只在每个迭代周期的开始位置初始化隐状态。 由于下一个小批量数据中的第i个子序列样本 与当前第i个子序列样本相邻， 因此当前小批量数据最后一个样本的隐状态， 将用于初始化下一个小批量数据第一个样本的隐状态。 这样，存储在隐状态中的序列的历史信息 可以在一个迭代周期内流经相邻的子序列。 然而，在任何一点隐状态的计算， 都依赖于同一迭代周期中前面所有的小批量数据， 这使得梯度计算变得复杂。 为了降低计算量，在处理任何一个小批量数据之前， 我们先分离梯度，使得隐状态的梯度计算总是限制在一个小批量数据的时间步内。</p>
<p>当使用随机抽样时，因为每个样本都是在一个随机位置抽样的， 因此需要为每个迭代周期重新初始化隐状态</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">#@save</span><br><span class="line">def train_epoch_ch8(net, train_iter, loss, updater, device, use_random_iter):</span><br><span class="line">    &quot;&quot;&quot;训练网络一个迭代周期（定义见第8章）&quot;&quot;&quot;</span><br><span class="line">    state, timer = None, d2l.Timer()</span><br><span class="line">    metric = d2l.Accumulator(2)  # 训练损失之和,词元数量</span><br><span class="line">    for X, Y in train_iter:</span><br><span class="line">        if state is None or use_random_iter:</span><br><span class="line">            # 在第一次迭代或使用随机抽样时初始化state</span><br><span class="line">            state = net.begin_state(batch_size=X.shape[0], device=device)</span><br><span class="line">        else:</span><br><span class="line">            if isinstance(net, nn.Module) and not isinstance(state, tuple):</span><br><span class="line">                # state对于nn.GRU是个张量</span><br><span class="line">                state.detach_()</span><br><span class="line">            else:</span><br><span class="line">                # state对于nn.LSTM或对于我们从零开始实现的模型是个张量</span><br><span class="line">                for s in state:</span><br><span class="line">                    s.detach_()</span><br><span class="line">        y = Y.T.reshape(-1)</span><br><span class="line">        X, y = X.to(device), y.to(device)</span><br><span class="line">        y_hat, state = net(X, state)</span><br><span class="line">        l = loss(y_hat, y.long()).mean()</span><br><span class="line">        if isinstance(updater, torch.optim.Optimizer):</span><br><span class="line">            updater.zero_grad()</span><br><span class="line">            l.backward()</span><br><span class="line">            grad_clipping(net, 1)</span><br><span class="line">            updater.step()</span><br><span class="line">        else:</span><br><span class="line">            l.backward()</span><br><span class="line">            grad_clipping(net, 1)</span><br><span class="line">            # 因为已经调用了mean函数</span><br><span class="line">            updater(batch_size=1)</span><br><span class="line">        metric.add(l * y.numel(), y.numel())</span><br><span class="line">    return math.exp(metric[0] / metric[1]), metric[1] / timer.stop()</span><br><span class="line"></span><br><span class="line">num_epochs, lr = 500, 1</span><br><span class="line">train_ch8(net, train_iter, vocab, lr, num_epochs, d2l.try_gpu())</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ul>
<li>5.7 总结<br>我们可以训练一个基于循环神经网络的字符级语言模型，根据用户提供的文本的前缀生成后续文本。</li>
</ul>
<p>一个简单的循环神经网络语言模型包括输入编码、循环神经网络模型和输出生成。</p>
<p>循环神经网络模型在训练以前需要初始化状态，不过随机抽样和顺序划分使用初始化方法不同。</p>
<p>当使用顺序划分时，我们需要分离梯度以减少计算量。</p>
<p>在进行任何预测之前，模型通过预热期进行自我更新（例如，获得比初始值更好的隐状态）。</p>
<p>梯度裁剪可以防止梯度爆炸，但不能应对梯度消失。</p>
<ol start="6">
<li><p>6.循环神经网络的简洁实现 （2024&#x2F;11&#x2F;18更）</p>
</li>
<li><p>7.时间反向传播<br>到目前为止，我们已经反复提到像梯度爆炸或梯度消失， 以及需要对循环神经网络分离梯度。 例如，在 8.5节中， 我们在序列上调用了detach函数。 为了能够快速构建模型并了解其工作原理， 上面所说的这些概念都没有得到充分的解释。<br>当我们首次实现循环神经网络（ 8.5节）时， 遇到了梯度爆炸的问题。 如果做了练习题，就会发现梯度截断对于确保模型收敛至关重要。 为了更好地理解此问题，本节将回顾序列模型梯度的计算方式， 它的工作原理没有什么新概念，毕竟我们使用的仍然是链式法则来计算梯度<br>循环神经网络中的前向传播相对简单。 通过时间反向传播（backpropagation through time，BPTT） (Werbos, 1990)实际上是循环神经网络中反向传播技术的一个特定应用。 它要求我们将循环神经网络的计算图一次展开一个时间步， 以获得模型变量和参数之间的依赖关系。 然后，基于链式法则，应用反向传播来计算和存储梯度。 由于序列可能相当长，因此依赖关系也可能相当长。 例如，某个1000个字符的序列， 其第一个词元可能会对最后位置的词元产生重大影响。 这在计算上是不可行的（它需要的时间和内存都太多了）， 并且还需要超过1000个矩阵的乘积才能得到非常难以捉摸的梯度。 这个过程充满了计算与统计的不确定性。 在下文中，我们将阐明会发生什么以及如何在实践中解决它们。</p>
</li>
</ol>
<ul>
<li>7.1 循环神经网络梯度分析<br>前向过程：<br><img src="/../../../../gallery/RNNforward.jpg" alt="RNNforward"></li>
</ul>
<p>反向过程：<br><img src="/../../../../gallery/RNNbackward.jpg" alt="RNNbackward"></p>
<p>（1）完全计算<br>显然，我们可以仅仅计算 (8.7.7)中的全部总和， 然而，这样的计算非常缓慢，并且可能会发生梯度爆炸， 因为初始条件的微小变化就可能会对结果产生巨大的影响。 也就是说，我们可以观察到类似于蝴蝶效应的现象， 即初始条件的很小变化就会导致结果发生不成比例的变化。 这对于我们想要估计的模型而言是非常不可取的。 毕竟，我们正在寻找的是能够很好地泛化高稳定性模型的估计器。 因此，在实践中，这种方法几乎从未使用过。<br>(思考：这部分梯度累积可以来自多个方面，例如顺序分区的序列输入——在不太batch中累计隐藏状态梯度，或者长序列步长——在单个样本中累计梯度，本质原因是隐藏状态的计算涉及到前一个时间步的隐状态)</p>
<p>（2）截断时间步<br>或者，我们可以在ɽ步后截断 (8.7.7)中的求和计算。 这是我们到目前为止一直在讨论的内容， 例如在 8.5节中分离梯度时。 这会带来真实梯度的近似， 只需将求和终止为∂h(t-ɽ)&#x2F;∂wh。 在实践中，这种方式工作得很好。 它通常被称为截断的通过时间反向传播 (Jaeger, 2002)。 这样做导致该模型主要侧重于短期影响，而不是长期影响。 这在现实中是可取的，因为它会将估计值偏向更简单和更稳定的模型。</p>
<p>（3）随机截断<br><img src="/../../../../gallery/RNNrandomclip.jpg" alt="RNNrandomclip"></p>
<p>（4）比较策略<br><img src="/../../../../gallery/RNNclipcom.jpg" alt="RNNclipcom"></p>
<p>图8.7.1说明了 当基于循环神经网络使用通过时间反向传播 分析《时间机器》书中前几个字符的三种策略：</p>
<p>第一行采用随机截断，方法是将文本划分为不同长度的片断；</p>
<p>第二行采用常规截断，方法是将文本分解为相同长度的子序列。 这也是我们在循环神经网络实验中一直在做的；</p>
<p>第三行采用通过时间的完全反向传播，结果是产生了在计算上不可行的表达式。</p>
<p>遗憾的是，虽然随机截断在理论上具有吸引力， 但很可能是由于多种因素在实践中并不比常规截断更好。 首先，在对过去若干个时间步经过反向传播后， 观测结果足以捕获实际的依赖关系。 其次，增加的方差抵消了时间步数越多梯度越精确的事实。 第三，我们真正想要的是只有短范围交互的模型。 因此，模型需要的正是截断的通过时间反向传播方法所具备的轻度正则化效果。</p>
<ul>
<li><p>7.2 通过实践反向传播的细节<br><img src="/../../../../gallery/RNNgraph.jpg" alt="RNNgraph"></p>
</li>
<li><p>7.3 总结<br>“通过时间反向传播”仅仅适用于反向传播在具有隐状态的序列模型。</p>
</li>
</ul>
<p>截断是计算方便性和数值稳定性的需要。截断包括：规则截断和随机截断。</p>
<p>矩阵的高次幂可能导致神经网络特征值的发散或消失，将以梯度爆炸或梯度消失的形式表现。</p>
<p>为了计算的效率，“通过时间反向传播”在计算期间会缓存中间值。</p>
<h1 id="九、-现代循环神经网络-（2024-11-25更）"><a href="#九、-现代循环神经网络-（2024-11-25更）" class="headerlink" title="九、 现代循环神经网络 （2024&#x2F;11&#x2F;25更）"></a>九、 现代循环神经网络 （2024&#x2F;11&#x2F;25更）</h1><ol>
<li>1.门控循环单元（GRU）<br><strong>面我们简单思考一下这种梯度异常在实践中的意义：</strong></li>
</ol>
<p><strong>我们可能会遇到这样的情况：早期观测值对预测所有未来观测值具有非常重要的意义。 考虑一个极端情况，其中第一个观测值包含一个校验和， 目标是在序列的末尾辨别校验和是否正确。 在这种情况下，第一个词元的影响至关重要。 我们希望有某些机制能够在一个记忆元里存储重要的早期信息。 如果没有这样的机制，我们将不得不给这个观测值指定一个非常大的梯度， 因为它会影响所有后续的观测值。</strong></p>
<p><strong>我们可能会遇到这样的情况：一些词元没有相关的观测值。 例如，在对网页内容进行情感分析时， 可能有一些辅助HTML代码与网页传达的情绪无关。 我们希望有一些机制来跳过隐状态表示中的此类词元。</strong></p>
<p><strong>我们可能会遇到这样的情况：序列的各个部分之间存在逻辑中断。 例如，书的章节之间可能会有过渡存在， 或者证券的熊市和牛市之间可能会有过渡存在。 在这种情况下，最好有一种方法来重置我们的内部状态表示。</strong></p>
<ul>
<li><p>1.1 门控隐状态<br>门控循环单元与普通的循环神经网络之间的关键区别在于： 前者支持隐状态的门控。 这意味着模型有专门的机制来确定应该何时更新隐状态， 以及应该何时重置隐状态。 这些机制是可学习的，并且能够解决了上面列出的问题。 例如，如果第一个词元非常重要， 模型将学会在第一次观测之后不更新隐状态。 同样，模型也可以学会跳过不相关的临时观测。 最后，模型还将学会在需要的时候重置隐状态。 下面我们将详细讨论各类门控。</p>
</li>
<li><p>1.2 重置门和更新门<br><img src="/../../../../gallery/resetgate.jpg" alt="resetgate"></p>
</li>
<li><p>1.3 候选隐状态——重置门<br><img src="/../../../../gallery/chs.jpg" alt="chs"></p>
</li>
<li><p>1.4 隐状态<br><img src="/../../../../gallery/hs.jpg" alt="hs"></p>
</li>
</ul>
<p>总之，门控循环单元具有以下两个显著特征：</p>
<p>重置门有助于捕获序列中的短期依赖关系；（r&#x3D;0，重置旧状态）</p>
<p>更新门有助于捕获序列中的长期依赖关系。（z&#x3D;1，保留旧状态）</p>
<ul>
<li><p>1.5 从零实现</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">def gru(inputs, state, params):</span><br><span class="line">    W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q = params</span><br><span class="line">    H, = state</span><br><span class="line">    outputs = []</span><br><span class="line">    for X in inputs:</span><br><span class="line">        Z = torch.sigmoid((X @ W_xz) + (H @ W_hz) + b_z)</span><br><span class="line">        R = torch.sigmoid((X @ W_xr) + (H @ W_hr) + b_r)</span><br><span class="line">        H_tilda = torch.tanh((X @ W_xh) + ((R * H) @ W_hh) + b_h)</span><br><span class="line">        H = Z * H + (1 - Z) * H_tilda</span><br><span class="line">        Y = H @ W_hq + b_q</span><br><span class="line">        outputs.append(Y)</span><br><span class="line">    return torch.cat(outputs, dim=0), (H,)</span><br></pre></td></tr></table></figure>
<p>当然也可以用API简洁实现</p>
</li>
<li><p>1.6 总结<br>门控循环神经网络可以更好地捕获时间步距离很长的序列上的依赖关系。</p>
</li>
</ul>
<p>重置门有助于捕获序列中的短期依赖关系。</p>
<p>更新门有助于捕获序列中的长期依赖关系。</p>
<p>重置门打开时，门控循环单元包含基本循环神经网络；更新门打开时，门控循环单元可以跳过子序列。</p>
<ol start="2">
<li>2.长短期记忆网络（LSTM）<br>长期以来，隐变量模型存在着长期信息保存和短期输入缺失的问题。 解决这一问题的最早方法之一是长短期存储器（long short-term memory，LSTM） (Hochreiter and Schmidhuber, 1997)。 它有许多与门控循环单元（ 9.1节）一样的属性。 有趣的是，长短期记忆网络的设计比门控循环单元稍微复杂一些， 却比门控循环单元早诞生了近20年。</li>
</ol>
<ul>
<li><p>2.1 门控记忆单元<br>可以说，长短期记忆网络的设计灵感来自于计算机的逻辑门。 长短期记忆网络引入了记忆元（memory cell），或简称为单元（cell）。 有些文献认为记忆元是隐状态的一种特殊类型， 它们与隐状态具有相同的形状，其设计目的是用于记录附加的信息。 为了控制记忆元，我们需要许多门。 其中一个门用来从单元中输出条目，我们将其称为输出门（output gate）。 另外一个门用来决定何时将数据读入单元，我们将其称为输入门（input gate）。 我们还需要一种机制来重置单元的内容，由遗忘门（forget gate）来管理， 这种设计的动机与门控循环单元相同， 能够通过专用机制决定什么时候记忆或忽略隐状态中的输入。 让我们看看这在实践中是如何运作的。</p>
</li>
<li><p>2.1.1 输入门、忘记门和输出门<br><img src="/../../../../gallery/LSTM1.jpg" alt="LSTM1"></p>
</li>
<li><p>2.1.2 候选记忆元<br><img src="/../../../../gallery/LSTM2.jpg" alt="LSTM2"></p>
</li>
<li><p>2.1.3 记忆元<br><img src="/../../../../gallery/LSTM3.jpg" alt="LSTM3"></p>
</li>
<li><p>2.1.4 隐状态<br><img src="/../../../../gallery/LSTM4.jpg" alt="LSTM4"></p>
</li>
<li><p>2.2 从零实现</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">def lstm(inputs, state, params):</span><br><span class="line">    [W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc, b_c,</span><br><span class="line">     W_hq, b_q] = params</span><br><span class="line">    (H, C) = state</span><br><span class="line">    outputs = []</span><br><span class="line">    for X in inputs:</span><br><span class="line">        I = torch.sigmoid((X @ W_xi) + (H @ W_hi) + b_i)</span><br><span class="line">        F = torch.sigmoid((X @ W_xf) + (H @ W_hf) + b_f)</span><br><span class="line">        O = torch.sigmoid((X @ W_xo) + (H @ W_ho) + b_o)</span><br><span class="line">        C_tilda = torch.tanh((X @ W_xc) + (H @ W_hc) + b_c)</span><br><span class="line">        C = F * C + I * C_tilda</span><br><span class="line">        H = O * torch.tanh(C)</span><br><span class="line">        Y = (H @ W_hq) + b_q</span><br><span class="line">        outputs.append(Y)</span><br><span class="line">    return torch.cat(outputs, dim=0), (H, C)</span><br></pre></td></tr></table></figure>
<p>当然也可以用API简洁实现</p>
</li>
<li><p>2.3 总结<br>长短期记忆网络有三种类型的门：输入门、遗忘门和输出门。</p>
</li>
</ul>
<p>长短期记忆网络的隐藏层输出包括“隐状态”和“记忆元”。只有隐状态会传递到输出层，而记忆元完全属于内部信息。</p>
<p>长短期记忆网络可以缓解梯度消失和梯度爆炸。</p>
<ol start="3">
<li>3.深度循环神经网络<br>到目前为止，我们只讨论了具有一个单向隐藏层的循环神经网络。 其中，隐变量和观测值与具体的函数形式的交互方式是相当随意的。 只要交互类型建模具有足够的灵活性，这就不是一个大问题。 然而，对一个单层来说，这可能具有相当的挑战性。 之前在线性模型中，我们通过添加更多的层来解决这个问题。 而在循环神经网络中，我们首先需要确定如何添加更多的层， 以及在哪里添加额外的非线性，因此这个问题有点棘手。</li>
</ol>
<p>事实上，我们可以将多层循环神经网络堆叠在一起， 通过对几个简单层的组合，产生了一个灵活的机制。 特别是，数据可能与不同层的堆叠有关。 例如，我们可能希望保持有关金融市场状况 （熊市或牛市）的宏观数据可用， 而微观数据只记录较短期的时间动态。</p>
<p>图9.3.1描述了一个具有L个隐藏层的深度循环神经网络， 每个隐状态都连续地传递到当前层的下一个时间步和下一层的当前时间步。<br><img src="/../../../../gallery/DRNN.jpg" alt="DRNN"></p>
<ul>
<li><p>3.1 函数依赖关系<br><img src="/../../../../gallery/DRNN1.jpg" alt="DRNN1"></p>
</li>
<li><p>3.2 简洁实现</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch import nn</span><br><span class="line">from d2l import torch as d2l</span><br><span class="line"></span><br><span class="line">batch_size, num_steps = 32, 35</span><br><span class="line">train_iter, vocab = d2l.load_data_time_machine</span><br><span class="line"></span><br><span class="line">vocab_size, num_hiddens, num_layers = len(vocab), 256, 2</span><br><span class="line">num_inputs = vocab_size</span><br><span class="line">device = d2l.try_gpu()</span><br><span class="line">lstm_layer = nn.LSTM(num_inputs, num_hiddens, num_layers)</span><br><span class="line">model = d2l.RNNModel(lstm_layer, len(vocab))</span><br><span class="line">model = model.to(device)</span><br><span class="line"></span><br><span class="line">num_epochs, lr = 500, 2</span><br><span class="line">d2l.train_ch8(model, train_iter, vocab, lr*1.0, num_epochs, device)</span><br></pre></td></tr></table></figure></li>
</ul>
<p>3.3 小结<br>在深度循环神经网络中，隐状态的信息被传递到当前层的下一时间步和下一层的当前时间步。</p>
<p>有许多不同风格的深度循环神经网络， 如长短期记忆网络、门控循环单元、或经典循环神经网络。 这些模型在深度学习框架的高级API中都有涵盖。</p>
<p>总体而言，深度循环神经网络需要大量的调参（如学习率和修剪） 来确保合适的收敛，模型的初始化也需要谨慎。</p>
<ol start="4">
<li>4.双向循环神经网络<br>根据可获得的信息量，我们可以用不同的词填空， 如“很高兴”（“happy”）、“不”（“not”）和“非常”（“very”）。 很明显，每个短语的“下文”传达了重要信息（如果有的话）， 而这些信息关乎到选择哪个词来填空， 所以无法利用这一点的序列模型将在相关任务上表现不佳。 例如，如果要做好命名实体识别 （例如，识别“Green”指的是“格林先生”还是绿色）， 不同长度的上下文范围重要性是相同的。 为了获得一些解决问题的灵感，让我们先迂回到概率图模型。</li>
</ol>
<ul>
<li><p>4.1 隐马尔可夫模型中的动态规划<br><img src="/../../../../gallery/hmarkov.jpg" alt="hmarkov"></p>
</li>
<li><p>4.2 双向模型<br>如果我们希望在循环神经网络中拥有一种机制， 使之能够提供与隐马尔可夫模型类似的前瞻能力， 我们就需要修改循环神经网络的设计。 幸运的是，这在概念上很容易， 只需要增加一个“从最后一个词元开始从后向前运行”的循环神经网络， 而不是只有一个在前向模式下“从第一个词元开始运行”的循环神经网络。 双向循环神经网络（bidirectional RNNs） 添加了反向传递信息的隐藏层，以便更灵活地处理此类信息。 图9.4.2描述了具有单个隐藏层的双向循环神经网络的架构。<br><img src="/../../../../gallery/bdRNN.jpg" alt="bdRNN"><br>事实上，这与隐马尔可夫模型中的动态规划的前向和后向递归没有太大区别。 其主要区别是，在隐马尔可夫模型中的方程具有特定的统计意义。 双向循环神经网络没有这样容易理解的解释， 我们只能把它们当作通用的、可学习的函数。 这种转变集中体现了现代深度网络的设计原则： 首先使用经典统计模型的函数依赖类型，然后将其参数化为通用形式。</p>
</li>
<li><p>4.2.1 定义<br><img src="/../../../../gallery/bdRNN2.jpg" alt="bdRNN2"></p>
</li>
<li><p>4.2.2 模型的计算代价及其应用<br>双向循环神经网络的一个关键特性是：使用来自序列两端的信息来估计输出。 也就是说，我们使用来自过去和未来的观测信息来预测当前的观测。 但是在对下一个词元进行预测的情况中，这样的模型并不是我们所需的。 因为在预测下一个词元时，我们终究无法知道下一个词元的下文是什么， 所以将不会得到很好的精度。 具体地说，在训练期间，我们能够利用过去和未来的数据来估计现在空缺的词； 而在测试期间，我们只有过去的数据，因此精度将会很差。 下面的实验将说明这一点。</p>
</li>
</ul>
<p>另一个严重问题是，双向循环神经网络的计算速度非常慢。 其主要原因是网络的前向传播需要在双向层中进行前向和后向递归， 并且网络的反向传播还依赖于前向传播的结果。 因此，梯度求解将有一个非常长的链。</p>
<p>双向层的使用在实践中非常少，并且仅仅应用于部分场合。 例如，填充缺失的单词、词元注释（例如，用于命名实体识别） 以及作为序列处理流水线中的一个步骤对序列进行编码（例如，用于机器翻译）。 在 14.8节和 15.2节中， 我们将介绍如何使用双向循环神经网络编码文本序列。<br>（过时的你还教我？？）</p>
<ul>
<li>4.3 双向循环神经网络的错误应用</li>
</ul>
<p>由于双向循环神经网络使用了过去的和未来的数据， 所以我们不能盲目地将这一语言模型应用于任何预测任务。 尽管模型产出的困惑度是合理的， 该模型预测未来词元的能力却可能存在严重缺陷。 我们用下面的示例代码引以为戒，以防在错误的环境中使用它们</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch import nn</span><br><span class="line">from d2l import torch as d2l</span><br><span class="line"></span><br><span class="line"># 加载数据</span><br><span class="line">batch_size, num_steps, device = 32, 35, d2l.try_gpu()</span><br><span class="line">train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)</span><br><span class="line"># 通过设置“bidirective=True”来定义双向LSTM模型</span><br><span class="line">vocab_size, num_hiddens, num_layers = len(vocab), 256, 2</span><br><span class="line">num_inputs = vocab_size</span><br><span class="line">lstm_layer = nn.LSTM(num_inputs, num_hiddens, num_layers, bidirectional=True)</span><br><span class="line">model = d2l.RNNModel(lstm_layer, len(vocab))</span><br><span class="line">model = model.to(device)</span><br><span class="line"># 训练模型</span><br><span class="line">num_epochs, lr = 500, 1</span><br><span class="line">d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)</span><br><span class="line"></span><br><span class="line">#perplexity 1.1, 131129.2 tokens/sec on cuda:0</span><br><span class="line">#time #travellerererererererererererererererererererererererererer</span><br><span class="line">#travellerererererererererererererererererererererererererer</span><br></pre></td></tr></table></figure>
<p>上述结果显然令人瞠目结舌。 关于如何更有效地使用双向循环神经网络的讨论， 请参阅 15.2节中的情感分类应用。</p>
<ul>
<li>4.4 总结<br>在双向循环神经网络中，每个时间步的隐状态由当前时间步的前后数据同时决定。</li>
</ul>
<p>双向循环神经网络与概率图模型中的“前向-后向”算法具有相似性。</p>
<p>双向循环神经网络主要用于序列编码和给定双向上下文的观测估计。</p>
<p>由于梯度链更长，因此双向循环神经网络的训练代价非常高。</p>
<ol start="5">
<li>5.机器翻译与数据集<br>语言模型是自然语言处理的关键， 而机器翻译是语言模型最成功的基准测试。 因为机器翻译正是将输入序列转换成输出序列的 序列转换模型（sequence transduction）的核心问题。 序列转换模型在各类现代人工智能应用中发挥着至关重要的作用，</li>
</ol>
<p>机器翻译（machine translation）指的是 将序列从一种语言自动翻译成另一种语言。 事实上，这个研究领域可以追溯到数字计算机发明后不久的20世纪40年代， 特别是在第二次世界大战中使用计算机破解语言编码。 几十年来，在使用神经网络进行端到端学习的兴起之前， 统计学方法在这一领域一直占据主导地位 (Brown et al., 1990, Brown et al., 1988)。 因为统计机器翻译（statistical machine translation）涉及了 翻译模型和语言模型等组成部分的统计分析， 因此基于神经网络的方法通常被称为 神经机器翻译（neural machine translation）， 用于将两种翻译模型区分开来。</p>
<p>本书的关注点是神经网络机器翻译方法，强调的是端到端的学习。 与 8.3节中的语料库 是单一语言的语言模型问题存在不同， 机器翻译的数据集是由源语言和目标语言的文本序列对组成的。 因此，我们需要一种完全不同的方法来预处理机器翻译数据集， 而不是复用语言模型的预处理程序。 下面，我们看一下如何将预处理后的数据加载到小批量中用于训练。</p>
<ul>
<li><p>5.1 下载和预处理数据集</p>
</li>
<li><p>5.2 词元化<br>与 8.3节中的字符级词元化不同， 在机器翻译中，我们更喜欢单词级词元化 （最先进的模型可能使用更高级的词元化技术）。 下面的tokenize_nmt函数对前num_examples个文本序列对进行词元， 其中每个词元要么是一个词，要么是一个标点符号。 此函数返回两个词元列表：source和target： source[i]是源语言（这里是英语）第i个文本序列的词元列表， target[i]是目标语言（这里是法语）第i个文本序列的词元列表。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">#@save</span><br><span class="line">def tokenize_nmt(text, num_examples=None):</span><br><span class="line">    &quot;&quot;&quot;词元化“英语－法语”数据数据集&quot;&quot;&quot;</span><br><span class="line">    source, target = [], []</span><br><span class="line">    for i, line in enumerate(text.split(&#x27;\n&#x27;)):</span><br><span class="line">        if num_examples and i &gt; num_examples:</span><br><span class="line">            break</span><br><span class="line">        parts = line.split(&#x27;\t&#x27;)</span><br><span class="line">        if len(parts) == 2:</span><br><span class="line">            source.append(parts[0].split(&#x27; &#x27;))</span><br><span class="line">            target.append(parts[1].split(&#x27; &#x27;))</span><br><span class="line">    return source, target</span><br><span class="line"></span><br><span class="line">source, target = tokenize_nmt(text)</span><br><span class="line">source[:6], target[:6]</span><br></pre></td></tr></table></figure>
</li>
<li><p>5.3 词表<br>由于机器翻译数据集由语言对组成， 因此我们可以分别为源语言和目标语言构建两个词表。 使用单词级词元化时，词表大小将明显大于使用字符级词元化时的词表大小。 为了缓解这一问题，这里我们将出现次数少于2次的低频率词元 视为相同的未知（“<unk>”）词元。 除此之外，我们还指定了额外的特定词元， 例如在小批量时用于将序列填充到相同长度的填充词元（“<pad>”）， 以及序列的开始词元（“<bos>”）和结束词元（“<eos>”）。 这些特殊词元在自然语言处理任务中比较常用。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">src_vocab = d2l.Vocab(source, min_freq=2,</span><br><span class="line">                      reserved_tokens=[&#x27;&lt;pad&gt;&#x27;, &#x27;&lt;bos&gt;&#x27;, &#x27;&lt;eos&gt;&#x27;])</span><br><span class="line">len(src_vocab)</span><br></pre></td></tr></table></figure>
</li>
<li><p>5.4 加载数据集<br>回想一下，语言模型中的序列样本都有一个固定的长度， 无论这个样本是一个句子的一部分还是跨越了多个句子的一个片断。 这个固定长度是由 8.3节中的 num_steps（时间步数或词元数量）参数指定的。 在机器翻译中，每个样本都是由源和目标组成的文本序列对， 其中的每个文本序列可能具有不同的长度。</p>
</li>
</ul>
<p>为了提高计算效率，我们仍然可以通过截断（truncation）和 填充（padding）方式实现一次只处理一个小批量的文本序列。 假设同一个小批量中的每个序列都应该具有相同的长度num_steps， 那么如果文本序列的词元数目少于num_steps时， 我们将继续在其末尾添加特定的“<pad>”词元， 直到其长度达到num_steps； 反之，我们将截断文本序列时，只取其前num_steps 个词元， 并且丢弃剩余的词元。这样，每个文本序列将具有相同的长度， 以便以相同形状的小批量进行加载。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">def truncate_pad(line, num_steps, padding_token):</span><br><span class="line">    &quot;&quot;&quot;截断或填充文本序列&quot;&quot;&quot;</span><br><span class="line">    if len(line) &gt; num_steps:</span><br><span class="line">        return line[:num_steps]  # 截断</span><br><span class="line">    return line + [padding_token] * (num_steps - len(line))  # 填充</span><br><span class="line"></span><br><span class="line">truncate_pad(src_vocab[source[0]], 10, src_vocab[&#x27;&lt;pad&gt;&#x27;])</span><br><span class="line"></span><br><span class="line"># [47, 4, 1, 1, 1, 1, 1, 1, 1, 1]</span><br></pre></td></tr></table></figure>

<p>现在我们定义一个函数，可以将文本序列 转换成小批量数据集用于训练。 我们将特定的“<eos>”词元添加到所有序列的末尾， 用于表示序列的结束。 当模型通过一个词元接一个词元地生成序列进行预测时， 生成的“<eos>”词元说明完成了序列输出工作。 此外，我们还记录了每个文本序列的长度， 统计长度时排除了填充词元， 在稍后将要介绍的一些模型会需要这个长度信息。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">def build_array_nmt(lines, vocab, num_steps):</span><br><span class="line">    &quot;&quot;&quot;将机器翻译的文本序列转换成小批量&quot;&quot;&quot;</span><br><span class="line">    lines = [vocab[l] for l in lines]</span><br><span class="line">    lines = [l + [vocab[&#x27;&lt;eos&gt;&#x27;]] for l in lines]</span><br><span class="line">    array = torch.tensor([truncate_pad(</span><br><span class="line">        l, num_steps, vocab[&#x27;&lt;pad&gt;&#x27;]) for l in lines])</span><br><span class="line">    valid_len = (array != vocab[&#x27;&lt;pad&gt;&#x27;]).type(torch.int32).sum(1)</span><br><span class="line">    return array, valid_len</span><br></pre></td></tr></table></figure>

<ul>
<li><p>5.5 批量数据<br>最后，我们定义load_data_nmt函数来返回数据迭代器， 以及源语言和目标语言的两种词表。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">def load_data_nmt(batch_size, num_steps, num_examples=600):</span><br><span class="line">    &quot;&quot;&quot;返回翻译数据集的迭代器和词表&quot;&quot;&quot;</span><br><span class="line">    text = preprocess_nmt(read_data_nmt())</span><br><span class="line">    source, target = tokenize_nmt(text, num_examples)</span><br><span class="line">    src_vocab = d2l.Vocab(source, min_freq=2,</span><br><span class="line">                          reserved_tokens=[&#x27;&lt;pad&gt;&#x27;, &#x27;&lt;bos&gt;&#x27;, &#x27;&lt;eos&gt;&#x27;])</span><br><span class="line">    tgt_vocab = d2l.Vocab(target, min_freq=2,</span><br><span class="line">                          reserved_tokens=[&#x27;&lt;pad&gt;&#x27;, &#x27;&lt;bos&gt;&#x27;, &#x27;&lt;eos&gt;&#x27;])</span><br><span class="line">    src_array, src_valid_len = build_array_nmt(source, src_vocab, num_steps)</span><br><span class="line">    tgt_array, tgt_valid_len = build_array_nmt(target, tgt_vocab, num_steps)</span><br><span class="line">    data_arrays = (src_array, src_valid_len, tgt_array, tgt_valid_len)</span><br><span class="line">    data_iter = d2l.load_array(data_arrays, batch_size)</span><br><span class="line">    return data_iter, src_vocab, tgt_vocab</span><br><span class="line"></span><br><span class="line">train_iter, src_vocab, tgt_vocab = load_data_nmt(batch_size=2, num_steps=8)</span><br><span class="line">for X, X_valid_len, Y, Y_valid_len in train_iter:</span><br><span class="line">    print(&#x27;X:&#x27;, X.type(torch.int32))</span><br><span class="line">    print(&#x27;X的有效长度:&#x27;, X_valid_len)</span><br><span class="line">    print(&#x27;Y:&#x27;, Y.type(torch.int32))</span><br><span class="line">    print(&#x27;Y的有效长度:&#x27;, Y_valid_len)</span><br><span class="line">    break</span><br><span class="line"></span><br><span class="line">#X: tensor([[ 7, 43,  4,  3,  1,  1,  1,  1],</span><br><span class="line">#        [44, 23,  4,  3,  1,  1,  1,  1]], dtype=torch.int32)</span><br><span class="line">#X的有效长度: tensor([4, 4])</span><br><span class="line">#Y: tensor([[ 6,  7, 40,  4,  3,  1,  1,  1],</span><br><span class="line">#        [ 0,  5,  3,  1,  1,  1,  1,  1]], dtype=torch.int32)</span><br><span class="line">#Y的有效长度: tensor([5, 3])</span><br></pre></td></tr></table></figure>
</li>
<li><p>5.6 总结<br>机器翻译指的是将文本序列从一种语言自动翻译成另一种语言。</p>
</li>
</ul>
<p>使用单词级词元化时的词表大小，将明显大于使用字符级词元化时的词表大小。为了缓解这一问题，我们可以将低频词元视为相同的未知词元。</p>
<p>通过截断和填充文本序列，可以保证所有的文本序列都具有相同的长度，以便以小批量的方式加载。</p>
<ol start="6">
<li>6.编码器-解码器架构<br>正如我们在 9.5节中所讨论的， 机器翻译是序列转换模型的一个核心问题， 其输入和输出都是长度可变的序列。 为了处理这种类型的输入和输出， 我们可以设计一个包含两个主要组件的架构： 第一个组件是一个编码器（encoder）： 它接受一个长度可变的序列作为输入， 并将其转换为具有固定形状的编码状态。 第二个组件是解码器（decoder）： 它将固定形状的编码状态映射到长度可变的序列。 这被称为编码器-解码器（encoder-decoder）架构， 如 图9.6.1 所示。<br><img src="/../../../../gallery/En-Decoder.jpg" alt="En-Decoder"><br>我们以英语到法语的机器翻译为例： 给定一个英文的输入序列：“They”“are”“watching”“.”。 首先，这种“编码器－解码器”架构将长度可变的输入序列编码成一个“状态”， 然后对该状态进行解码， 一个词元接着一个词元地生成翻译后的序列作为输出： “Ils”“regordent”“.”。</li>
</ol>
<ul>
<li><p>6.1 编码器<br>在编码器接口中，我们只指定长度可变的序列作为编码器的输入X。 任何继承这个Encoder基类的模型将完成代码实现。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">from torch import nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#@save</span><br><span class="line">class Encoder(nn.Module):</span><br><span class="line">    &quot;&quot;&quot;编码器-解码器架构的基本编码器接口&quot;&quot;&quot;</span><br><span class="line">    def __init__(self, **kwargs):</span><br><span class="line">        super(Encoder, self).__init__(**kwargs)</span><br><span class="line"></span><br><span class="line">    def forward(self, X, *args):</span><br><span class="line">        raise NotImplementedError</span><br></pre></td></tr></table></figure>
</li>
<li><p>6.2 解码器<br>在下面的解码器接口中，我们新增一个init_state函数， 用于将编码器的输出（enc_outputs）转换为编码后的状态。 注意，此步骤可能需要额外的输入，例如：输入序列的有效长度， 这在 9.5.4节中进行了解释。 为了逐个地生成长度可变的词元序列， 解码器在每个时间步都会将输入 （例如：在前一时间步生成的词元）和编码后的状态 映射成当前时间步的输出词元。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">#@save</span><br><span class="line">class Decoder(nn.Module):</span><br><span class="line">    &quot;&quot;&quot;编码器-解码器架构的基本解码器接口&quot;&quot;&quot;</span><br><span class="line">    def __init__(self, **kwargs):</span><br><span class="line">        super(Decoder, self).__init__(**kwargs)</span><br><span class="line"></span><br><span class="line">    def init_state(self, enc_outputs, *args):</span><br><span class="line">        raise NotImplementedError</span><br><span class="line"></span><br><span class="line">    def forward(self, X, state):</span><br><span class="line">        raise NotImplementedError</span><br></pre></td></tr></table></figure>
</li>
<li><p>6.3 合并编码器和解码器<br>总而言之，“编码器-解码器”架构包含了一个编码器和一个解码器， 并且还拥有可选的额外的参数。 在前向传播中，编码器的输出用于生成编码状态， 这个状态又被解码器作为其输入的一部分。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">#@save</span><br><span class="line">class EncoderDecoder(nn.Module):</span><br><span class="line">    &quot;&quot;&quot;编码器-解码器架构的基类&quot;&quot;&quot;</span><br><span class="line">    def __init__(self, encoder, decoder, **kwargs):</span><br><span class="line">        super(EncoderDecoder, self).__init__(**kwargs)</span><br><span class="line">        self.encoder = encoder</span><br><span class="line">        self.decoder = decoder</span><br><span class="line"></span><br><span class="line">    def forward(self, enc_X, dec_X, *args):</span><br><span class="line">        enc_outputs = self.encoder(enc_X, *args)</span><br><span class="line">        dec_state = self.decoder.init_state(enc_outputs, *args)</span><br><span class="line">        return self.decoder(dec_X, dec_state)</span><br></pre></td></tr></table></figure>
<p>“编码器－解码器”体系架构中的术语状态 会启发人们使用具有状态的神经网络来实现该架构。 在下一节中，我们将学习如何应用循环神经网络， 来设计基于“编码器－解码器”架构的序列转换模型。</p>
</li>
<li><p>6.4 总结<br>编码器－解码器”架构可以将长度可变的序列作为输入和输出，因此适用于机器翻译等序列转换问题。</p>
</li>
</ul>
<p>编码器将长度可变的序列作为输入，并将其转换为具有固定形状的编码状态。</p>
<p>解码器将具有固定形状的编码状态映射为长度可变的序列。</p>
<ol start="7">
<li>7.序列到序列学习（Seq2seq）<br>我们将使用两个循环神经网络的编码器和解码器， 并将其应用于序列到序列（sequence to sequence，seq2seq）类的学习任务 (Cho et al., 2014, Sutskever et al., 2014)。</li>
</ol>
<p>遵循编码器－解码器架构的设计原则， 循环神经网络编码器使用长度可变的序列作为输入， 将其转换为固定形状的隐状态。 换言之，输入序列的信息被编码到循环神经网络编码器的隐状态中。 为了连续生成输出序列的词元， 独立的循环神经网络解码器是基于输入序列的编码信息 和输出序列已经看见的或者生成的词元来预测下一个词元。 图9.7.1演示了 如何在机器翻译中使用两个循环神经网络进行序列到序列学习。<br><img src="/../../../../gallery/seq2seq.jpg" alt="seq2seq"><br>在 图9.7.1中， 特定的“<eos>”表示序列结束词元。 一旦输出序列生成此词元，模型就会停止预测。 在循环神经网络解码器的初始化时间步，有两个特定的设计决定： 首先，特定的“<bos>”表示序列开始词元，它是解码器的输入序列的第一个词元。 其次，使用循环神经网络编码器最终的隐状态来初始化解码器的隐状态。 例如，在 (Sutskever et al., 2014)的设计中， 正是基于这种设计将输入序列的编码信息送入到解码器中来生成输出序列的。 在其他一些设计中 (Cho et al., 2014)， 如 图9.7.1所示， 编码器最终的隐状态在每一个时间步都作为解码器的输入序列的一部分。 类似于 8.3节中语言模型的训练， 可以允许标签成为原始的输出序列， 从源序列词元“<bos>”“Ils”“regardent”“.” 到新序列词元 “Ils”“regardent”“.”“<eos>”来移动预测的位置。</p>
<ul>
<li>7.1 编码器<br>从技术上讲，编码器将长度可变的输入序列转换成 形状固定的上下文变量c， 并且将输入序列的信息在该上下文变量中进行编码。 如 图9.7.1所示，可以使用循环神经网络来设计编码器。</li>
</ul>
<p>考虑由一个序列组成的样本（批量大小是1）。 假设输入序列是x1,…,xT,， 其中xt是输入文本序列中的第t个词元。 在时间步t，循环神经网络将词元xt的输入特征向量 xt和ht-1（即上一时间步的隐状态） 转换为ht（即当前步的隐状态）。 使用一个函数f来描述循环神经网络的循环层所做的变换：<br>ht&#x3D;f(xt,ht-1)<br>总之，编码器通过选定的函数q， 将所有时间步的隐状态转换为上下文变量：<br>c&#x3D;q(h1,…,hT)</p>
<p>现在，让我们实现循环神经网络编码器。 注意，我们使用了嵌入层（embedding layer） 来获得输入序列中每个词元的特征向量。 嵌入层的权重是一个矩阵， 其行数等于输入词表的大小（vocab_size）， 其列数等于特征向量的维度（embed_size）。 对于任意输入词元的索引i， 嵌入层获取权重矩阵的第i行（从0开始）以返回其特征向量。 另外，本文选择了一个多层门控循环单元来实现编码器。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">#@save</span><br><span class="line">class Seq2SeqEncoder(d2l.Encoder):</span><br><span class="line">    &quot;&quot;&quot;用于序列到序列学习的循环神经网络编码器&quot;&quot;&quot;</span><br><span class="line">    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,</span><br><span class="line">                 dropout=0, **kwargs):</span><br><span class="line">        super(Seq2SeqEncoder, self).__init__(**kwargs)</span><br><span class="line">        # 嵌入层</span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, embed_size)</span><br><span class="line">        self.rnn = nn.GRU(embed_size, num_hiddens, num_layers,</span><br><span class="line">                          dropout=dropout)</span><br><span class="line"></span><br><span class="line">    def forward(self, X, *args):</span><br><span class="line">        # 输出&#x27;X&#x27;的形状：(batch_size,num_steps,embed_size)</span><br><span class="line">        X = self.embedding(X)</span><br><span class="line">        # 在循环神经网络模型中，第一个轴对应于时间步</span><br><span class="line">        X = X.permute(1, 0, 2)</span><br><span class="line">        # 如果未提及状态，则默认为0</span><br><span class="line">        output, state = self.rnn(X)</span><br><span class="line">        # output的形状:(num_steps,batch_size,num_hiddens)</span><br><span class="line">        # state的形状:(num_layers,batch_size,num_hiddens)</span><br><span class="line">        return output, state</span><br></pre></td></tr></table></figure>

<p>循环层返回变量的说明可以参考 8.6节。</p>
<p>下面，我们实例化上述编码器的实现： 我们使用一个两层门控循环单元编码器，其隐藏单元数为16。 给定一小批量的输入序列X（批量大小为x，时间步为）。 在完成所有时间步后， 最后一层的隐状态的输出是一个张量（output由编码器的循环层返回）， 其形状为（时间步数，批量大小，隐藏单元数）。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">encoder = Seq2SeqEncoder(vocab_size=10, embed_size=8, num_hiddens=16,</span><br><span class="line">                         num_layers=2)</span><br><span class="line">encoder.eval()</span><br><span class="line">X = torch.zeros((4, 7), dtype=torch.long)</span><br><span class="line">output, state = encoder(X)</span><br><span class="line">output.shape</span><br><span class="line"># torch.Size([7, 4, 16])</span><br></pre></td></tr></table></figure>
<p>由于这里使用的是门控循环单元， 所以在最后一个时间步的多层隐状态的形状是 （隐藏层的数量，批量大小，隐藏单元的数量）。 如果使用长短期记忆网络，state中还将包含记忆单元信息。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">state.shape</span><br><span class="line"># torch.Size([2, 4, 16])</span><br></pre></td></tr></table></figure>

<ul>
<li><p>7.2 解码器<br><img src="/../../../../gallery/rnndecoder.jpg" alt="rnndecoder"><br>（这里所谓上下文变量就是指编码输出，区别于循环神经网络的隐状态，类似于神经网络的隐变量；到目前为止，循环神经网络的输出总是取决于历史和当下的输入，其中历史输入由历史状态表示）<br>根据 图9.7.1，当实现解码器时， 我们直接使用编码器最后一个时间步的隐状态来初始化解码器的隐状态。 这就要求使用循环神经网络实现的编码器和解码器具有相同数量的层和隐藏单元。 为了进一步包含经过编码的输入序列的信息， 上下文变量在所有的时间步与解码器的输入进行拼接（concatenate）。 为了预测输出词元的概率分布， 在循环神经网络解码器的最后一层使用全连接层来变换隐状态。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">class Seq2SeqDecoder(d2l.Decoder):</span><br><span class="line">    &quot;&quot;&quot;用于序列到序列学习的循环神经网络解码器&quot;&quot;&quot;</span><br><span class="line">    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,</span><br><span class="line">                 dropout=0, **kwargs):</span><br><span class="line">        super(Seq2SeqDecoder, self).__init__(**kwargs)</span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, embed_size)</span><br><span class="line">        self.rnn = nn.GRU(embed_size + num_hiddens, num_hiddens, num_layers,</span><br><span class="line">                          dropout=dropout)</span><br><span class="line">        self.dense = nn.Linear(num_hiddens, vocab_size)</span><br><span class="line"></span><br><span class="line">    def init_state(self, enc_outputs, *args):</span><br><span class="line">        return enc_outputs[1]</span><br><span class="line"></span><br><span class="line">    def forward(self, X, state):</span><br><span class="line">        # 输出&#x27;X&#x27;的形状：(batch_size,num_steps,embed_size)</span><br><span class="line">        X = self.embedding(X).permute(1, 0, 2)</span><br><span class="line">        # 广播context，使其具有与X相同的num_steps</span><br><span class="line">        context = state[-1].repeat(X.shape[0], 1, 1)</span><br><span class="line">        X_and_context = torch.cat((X, context), 2)</span><br><span class="line">        output, state = self.rnn(X_and_context, state)</span><br><span class="line">        output = self.dense(output).permute(1, 0, 2)</span><br><span class="line">        # output的形状:(batch_size,num_steps,vocab_size)</span><br><span class="line">        # state的形状:(num_layers,batch_size,num_hiddens)</span><br><span class="line">        return output, state</span><br></pre></td></tr></table></figure>
<p>下面，我们用与前面提到的编码器中相同的超参数来实例化解码器。 如我们所见，解码器的输出形状变为（批量大小，时间步数，词表大小）， 其中张量的最后一个维度存储预测的词元分布。\</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">decoder = Seq2SeqDecoder(vocab_size=10, embed_size=8, num_hiddens=16,</span><br><span class="line">                         num_layers=2)</span><br><span class="line">decoder.eval()</span><br><span class="line">state = decoder.init_state(encoder(X))</span><br><span class="line">output, state = decoder(X, state)</span><br><span class="line">output.shape, state.shape</span><br></pre></td></tr></table></figure>
<p><img src="/../../../../gallery/layers.jpg" alt="layers"></p>
</li>
<li><p>7.3 损失函数<br>在每个时间步，解码器预测了输出词元的概率分布。 类似于语言模型，可以使用softmax来获得分布， 并通过计算交叉熵损失函数来进行优化。 回想一下 9.5节中， 特定的填充词元被添加到序列的末尾， 因此不同长度的序列可以以相同形状的小批量加载。 但是，我们应该将填充词元的预测排除在损失函数的计算之外。</p>
</li>
</ul>
<p>为此，我们可以使用下面的sequence_mask函数 通过零值化屏蔽不相关的项， 以便后面任何不相关预测的计算都是与零的乘积，结果都等于零。 例如，如果两个序列的有效长度（不包括填充词元）分别为1和2， 则第一个序列的第一项和第二个序列的前两项之后的剩余项将被清除为零。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">#@save</span><br><span class="line">def sequence_mask(X, valid_len, value=0):</span><br><span class="line">    &quot;&quot;&quot;在序列中屏蔽不相关的项&quot;&quot;&quot;</span><br><span class="line">    maxlen = X.size(1)</span><br><span class="line">    mask = torch.arange((maxlen), dtype=torch.float32,</span><br><span class="line">                        device=X.device)[None, :] &lt; valid_len[:, None]</span><br><span class="line">    X[~mask] = value</span><br><span class="line">    return X</span><br><span class="line"></span><br><span class="line">X = torch.tensor([[1, 2, 3], [4, 5, 6]])</span><br><span class="line">sequence_mask(X, torch.tensor([1, 2]))</span><br><span class="line"># tensor([[1, 0, 0],</span><br><span class="line">#        [4, 5, 0]])</span><br></pre></td></tr></table></figure>
<p>现在，我们可以通过扩展softmax交叉熵损失函数来遮蔽不相关的预测。 最初，所有预测词元的掩码都设置为1。 一旦给定了有效长度，与填充词元对应的掩码将被设置为0。 最后，将所有词元的损失乘以掩码，以过滤掉损失中填充词元产生的不相关预测。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">#@save</span><br><span class="line">class MaskedSoftmaxCELoss(nn.CrossEntropyLoss):</span><br><span class="line">    &quot;&quot;&quot;带遮蔽的softmax交叉熵损失函数&quot;&quot;&quot;</span><br><span class="line">    # pred的形状：(batch_size,num_steps,vocab_size)</span><br><span class="line">    # label的形状：(batch_size,num_steps)</span><br><span class="line">    # valid_len的形状：(batch_size,)</span><br><span class="line">    def forward(self, pred, label, valid_len):</span><br><span class="line">        weights = torch.ones_like(label)</span><br><span class="line">        weights = sequence_mask(weights, valid_len)</span><br><span class="line">        self.reduction=&#x27;none&#x27;</span><br><span class="line">        unweighted_loss = super(MaskedSoftmaxCELoss, self).forward(</span><br><span class="line">            pred.permute(0, 2, 1), label)</span><br><span class="line">        weighted_loss = (unweighted_loss * weights).mean(dim=1)</span><br><span class="line">        return weighted_loss</span><br></pre></td></tr></table></figure>

<ul>
<li><p>7.4 训练<br>在下面的循环训练过程中，如 图9.7.1所示， 特定的序列开始词元（“<bos>”）和 原始的输出序列（不包括序列结束词元“<eos>”） 拼接在一起作为解码器的输入。 这被称为强制教学（teacher forcing）， 因为原始的输出序列（词元的标签）被送入解码器。 或者，将来自上一个时间步的预测得到的词元作为解码器的当前输入。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">#@save</span><br><span class="line">def train_seq2seq(net, data_iter, lr, num_epochs, tgt_vocab, device):</span><br><span class="line">    &quot;&quot;&quot;训练序列到序列模型&quot;&quot;&quot;</span><br><span class="line">    def xavier_init_weights(m):</span><br><span class="line">        if type(m) == nn.Linear:</span><br><span class="line">            nn.init.xavier_uniform_(m.weight)</span><br><span class="line">        if type(m) == nn.GRU:</span><br><span class="line">            for param in m._flat_weights_names:</span><br><span class="line">                if &quot;weight&quot; in param:</span><br><span class="line">                    nn.init.xavier_uniform_(m._parameters[param])</span><br><span class="line"></span><br><span class="line">    net.apply(xavier_init_weights)</span><br><span class="line">    net.to(device)</span><br><span class="line">    optimizer = torch.optim.Adam(net.parameters(), lr=lr)</span><br><span class="line">    loss = MaskedSoftmaxCELoss()</span><br><span class="line">    net.train()</span><br><span class="line">    animator = d2l.Animator(xlabel=&#x27;epoch&#x27;, ylabel=&#x27;loss&#x27;,</span><br><span class="line">                     xlim=[10, num_epochs])</span><br><span class="line">    for epoch in range(num_epochs):</span><br><span class="line">        timer = d2l.Timer()</span><br><span class="line">        metric = d2l.Accumulator(2)  # 训练损失总和，词元数量</span><br><span class="line">        for batch in data_iter:</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            X, X_valid_len, Y, Y_valid_len = [x.to(device) for x in batch]</span><br><span class="line">            bos = torch.tensor([tgt_vocab[&#x27;&lt;bos&gt;&#x27;]] * Y.shape[0],</span><br><span class="line">                          device=device).reshape(-1, 1)</span><br><span class="line">            dec_input = torch.cat([bos, Y[:, :-1]], 1)  # 强制教学</span><br><span class="line">            Y_hat, _ = net(X, dec_input, X_valid_len)</span><br><span class="line">            l = loss(Y_hat, Y, Y_valid_len)</span><br><span class="line">            l.sum().backward()      # 损失函数的标量进行“反向传播”</span><br><span class="line">            d2l.grad_clipping(net, 1)</span><br><span class="line">            num_tokens = Y_valid_len.sum()</span><br><span class="line">            optimizer.step()</span><br><span class="line">            with torch.no_grad():</span><br><span class="line">                metric.add(l.sum(), num_tokens)</span><br><span class="line">        if (epoch + 1) % 10 == 0:</span><br><span class="line">            animator.add(epoch + 1, (metric[0] / metric[1],))</span><br><span class="line">    print(f&#x27;loss &#123;metric[0] / metric[1]:.3f&#125;, &#123;metric[1] / timer.stop():.1f&#125; &#x27;</span><br><span class="line">        f&#x27;tokens/sec on &#123;str(device)&#125;&#x27;)</span><br></pre></td></tr></table></figure>
<p>现在，在机器翻译数据集上，我们可以 创建和训练一个循环神经网络“编码器－解码器”模型用于序列到序列的学习。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">embed_size, num_hiddens, num_layers, dropout = 32, 32, 2, 0.1</span><br><span class="line">batch_size, num_steps = 64, 10</span><br><span class="line">lr, num_epochs, device = 0.005, 300, d2l.try_gpu()</span><br><span class="line"></span><br><span class="line">train_iter, src_vocab, tgt_vocab = d2l.load_data_nmt(batch_size, num_steps)</span><br><span class="line">encoder = Seq2SeqEncoder(len(src_vocab), embed_size, num_hiddens, num_layers,</span><br><span class="line">                        dropout)</span><br><span class="line">decoder = Seq2SeqDecoder(len(tgt_vocab), embed_size, num_hiddens, num_layers,</span><br><span class="line">                        dropout)</span><br><span class="line">net = d2l.EncoderDecoder(encoder, decoder)</span><br><span class="line">train_seq2seq(net, train_iter, lr, num_epochs, tgt_vocab, device)</span><br></pre></td></tr></table></figure>
</li>
<li><p>7.5 预测<br>为了采用一个接着一个词元的方式预测输出序列， 每个解码器当前时间步的输入都将来自于前一时间步的预测词元。 与训练类似，序列开始词元（“<bos>”） 在初始时间步被输入到解码器中。 该预测过程如 图9.7.3所示， 当输出序列的预测遇到序列结束词元（“<eos>”）时，预测就结束了。<br><img src="/../../../../gallery/seq2seqpre.jpg" alt="seq2seqpre"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">#@save</span><br><span class="line">def predict_seq2seq(net, src_sentence, src_vocab, tgt_vocab, num_steps,</span><br><span class="line">                    device, save_attention_weights=False):</span><br><span class="line">    &quot;&quot;&quot;序列到序列模型的预测&quot;&quot;&quot;</span><br><span class="line">    # 在预测时将net设置为评估模式</span><br><span class="line">    net.eval()</span><br><span class="line">    src_tokens = src_vocab[src_sentence.lower().split(&#x27; &#x27;)] + [</span><br><span class="line">        src_vocab[&#x27;&lt;eos&gt;&#x27;]]</span><br><span class="line">    enc_valid_len = torch.tensor([len(src_tokens)], device=device)</span><br><span class="line">    src_tokens = d2l.truncate_pad(src_tokens, num_steps, src_vocab[&#x27;&lt;pad&gt;&#x27;])</span><br><span class="line">    # 添加批量轴</span><br><span class="line">    enc_X = torch.unsqueeze(</span><br><span class="line">        torch.tensor(src_tokens, dtype=torch.long, device=device), dim=0)</span><br><span class="line">    enc_outputs = net.encoder(enc_X, enc_valid_len)</span><br><span class="line">    dec_state = net.decoder.init_state(enc_outputs, enc_valid_len)</span><br><span class="line">    # 添加批量轴</span><br><span class="line">    dec_X = torch.unsqueeze(torch.tensor(</span><br><span class="line">        [tgt_vocab[&#x27;&lt;bos&gt;&#x27;]], dtype=torch.long, device=device), dim=0)</span><br><span class="line">    output_seq, attention_weight_seq = [], []</span><br><span class="line">    for _ in range(num_steps):</span><br><span class="line">        Y, dec_state = net.decoder(dec_X, dec_state)</span><br><span class="line">        # 我们使用具有预测最高可能性的词元，作为解码器在下一时间步的输入</span><br><span class="line">        dec_X = Y.argmax(dim=2)</span><br><span class="line">        pred = dec_X.squeeze(dim=0).type(torch.int32).item()</span><br><span class="line">        # 保存注意力权重（稍后讨论）</span><br><span class="line">        if save_attention_weights:</span><br><span class="line">            attention_weight_seq.append(net.decoder.attention_weights)</span><br><span class="line">        # 一旦序列结束词元被预测，输出序列的生成就完成了</span><br><span class="line">        if pred == tgt_vocab[&#x27;&lt;eos&gt;&#x27;]:</span><br><span class="line">            break</span><br><span class="line">        output_seq.append(pred)</span><br><span class="line">    return &#x27; &#x27;.join(tgt_vocab.to_tokens(output_seq)), attention_weight_seq</span><br></pre></td></tr></table></figure>
</li>
<li><p>7.6 评估<br>BLEU: 原则上说，对于预测序列中的任意n元语法（n-grams）， BLEU的评估都是这个n元语法是否出现在标签序列中。<br><img src="/../../../../gallery/BLEU.jpg" alt="BLEU"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">def bleu(pred_seq, label_seq, k):  #@save</span><br><span class="line">    &quot;&quot;&quot;计算BLEU&quot;&quot;&quot;</span><br><span class="line">    pred_tokens, label_tokens = pred_seq.split(&#x27; &#x27;), label_seq.split(&#x27; &#x27;)</span><br><span class="line">    len_pred, len_label = len(pred_tokens), len(label_tokens)</span><br><span class="line">    score = math.exp(min(0, 1 - len_label / len_pred))</span><br><span class="line">    for n in range(1, k + 1):</span><br><span class="line">        num_matches, label_subs = 0, collections.defaultdict(int)</span><br><span class="line">        for i in range(len_label - n + 1):</span><br><span class="line">            label_subs[&#x27; &#x27;.join(label_tokens[i: i + n])] += 1</span><br><span class="line">        for i in range(len_pred - n + 1):</span><br><span class="line">            if label_subs[&#x27; &#x27;.join(pred_tokens[i: i + n])] &gt; 0:</span><br><span class="line">                num_matches += 1</span><br><span class="line">                label_subs[&#x27; &#x27;.join(pred_tokens[i: i + n])] -= 1</span><br><span class="line">        score *= math.pow(num_matches / (len_pred - n + 1), math.pow(0.5, n))</span><br><span class="line">    return score</span><br><span class="line"></span><br><span class="line">engs = [&#x27;go .&#x27;, &quot;i lost .&quot;, &#x27;he\&#x27;s calm .&#x27;, &#x27;i\&#x27;m home .&#x27;]</span><br><span class="line">fras = [&#x27;va !&#x27;, &#x27;j\&#x27;ai perdu .&#x27;, &#x27;il est calme .&#x27;, &#x27;je suis chez moi .&#x27;]</span><br><span class="line">for eng, fra in zip(engs, fras):</span><br><span class="line">    translation, attention_weight_seq = predict_seq2seq(</span><br><span class="line">        net, eng, src_vocab, tgt_vocab, num_steps, device)</span><br><span class="line">    print(f&#x27;&#123;eng&#125; =&gt; &#123;translation&#125;, bleu &#123;bleu(translation, fra, k=2):.3f&#125;&#x27;)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
</li>
<li><p>7.7 总结<br>根据“编码器-解码器”架构的设计， 我们可以使用两个循环神经网络来设计一个序列到序列学习的模型。</p>
</li>
</ul>
<p>在实现编码器和解码器时，我们可以使用多层循环神经网络。</p>
<p>我们可以使用遮蔽来过滤不相关的计算，例如在计算损失时。</p>
<p>在“编码器－解码器”训练中，强制教学方法将原始输出序列（而非预测结果）输入解码器。</p>
<p>BLEU是一种常用的评估方法，它通过测量预测序列和标签序列之间的<br>元语法的匹配度来评估预测。</p>
<ol start="8">
<li>8.束搜索 （2024&#x2F;12&#x2F;2更）<br>我们逐个预测输出序列， 直到预测序列中出现特定的序列结束词元“<eos>”。 本节将首先介绍贪心搜索（greedy search）策略， 并探讨其存在的问题，然后对比其他替代策略： 穷举搜索（exhaustive search）和束搜索（beam search）。</li>
</ol>
<ul>
<li><p>8.1 贪心搜索<br><img src="/../../../../gallery/greedy.jpg" alt="greedy"></p>
</li>
<li><p>8.2 穷举搜索<br>如果目标是获得最优序列， 我们可以考虑使用穷举搜索（exhaustive search）： 穷举地列举所有可能的输出序列及其条件概率， 然后计算输出条件概率最高的一个。</p>
</li>
</ul>
<p>虽然我们可以使用穷举搜索来获得最优序列， 但其计算量O(|y|**T)可能高的惊人。贪心搜索则为O(|y|T)其中|y|词表大小，T是输出最大词元数(时间步)。</p>
<ul>
<li>8.3 束搜索<br>那么该选取哪种序列搜索策略呢？ 如果精度最重要，则显然是穷举搜索。 如果计算成本最重要，则显然是贪心搜索。 而束搜索的实际应用则介于这两个极端之间。</li>
</ul>
<p>束搜索（beam search）是贪心搜索的一个改进版本。 它有一个超参数，名为束宽（beam size）k。 在时间步1，我们选择具有最高条件概率的k个词元。 这k个词元将分别是k个候选输出序列的第一个词元。 在随后的每个时间步，基于上一时间步的k个候选输出序列， 我们将继续从k|y|个可能的选择中 挑出具有最高条件概率的k个候选输出序列。<br><img src="/../../../../gallery/beamsearch.jpg" alt="beamsearch"></p>
<p>束搜索的计算量为O(k|y|T)， 这个结果介于贪心搜索和穷举搜索之间。 实际上，贪心搜索可以看作一种束宽为1的特殊类型的束搜索。 通过灵活地选择束宽，束搜索可以在正确率和计算代价之间进行权衡。</p>
<ul>
<li>8.4 小结<br>序列搜索策略包括贪心搜索、穷举搜索和束搜索。</li>
</ul>
<p>贪心搜索所选取序列的计算量最小，但精度相对较低。</p>
<p>穷举搜索所选取序列的精度最高，但计算量最大。</p>
<p>束搜索通过灵活选择束宽，在正确率和计算代价之间进行权衡。</p>
<h1 id="十、-注意力机制"><a href="#十、-注意力机制" class="headerlink" title="十、 注意力机制"></a>十、 注意力机制</h1><p><strong>灵长类动物的视觉系统接受了大量的感官输入， 这些感官输入远远超过了大脑能够完全处理的程度。 然而，并非所有刺激的影响都是相等的。 意识的聚集和专注使灵长类动物能够在复杂的视觉环境中将注意力引向感兴趣的物体，例如猎物和天敌。 只关注一小部分信息的能力对进化更加有意义，使人类得以生存和成功。</strong></p>
<p><strong>自19世纪以来，科学家们一直致力于研究认知神经科学领域的注意力。 本章的很多章节将涉及到一些研究。</strong></p>
<p><strong>首先回顾一个经典注意力框架，解释如何在视觉场景中展开注意力。 受此框架中的注意力提示（attention cues）的启发， 我们将设计能够利用这些注意力提示的模型。 1964年的Nadaraya-Waston核回归（kernel regression）正是具有 注意力机制（attention mechanism）的机器学习的简单演示。</strong></p>
<p><strong>然后继续介绍的是注意力函数，它们在深度学习的注意力模型设计中被广泛使用。 具体来说，我们将展示如何使用这些函数来设计Bahdanau注意力。 Bahdanau注意力是深度学习中的具有突破性价值的注意力模型，它双向对齐并且可以微分。</strong></p>
<p><strong>最后将描述仅仅基于注意力机制的Transformer架构， 该架构中使用了多头注意力（multi-head attention） 和自注意力（self-attention）。 自2017年横空出世，Transformer一直都普遍存在于现代的深度学习应用中， 例如语言、视觉、语音和强化学习领域。</strong></p>
<ol>
<li>1.注意力提示</li>
</ol>
<p>自经济学研究稀缺资源分配以来，人们正处在“注意力经济”时代， 即人类的注意力被视为可以交换的、有限的、有价值的且稀缺的商品。 许多商业模式也被开发出来去利用这一点： 在音乐或视频流媒体服务上，人们要么消耗注意力在广告上，要么付钱来隐藏广告； 为了在网络游戏世界的成长，人们要么消耗注意力在游戏战斗中， 从而帮助吸引新的玩家，要么付钱立即变得强大。 总之，注意力不是免费的。</p>
<p>注意力是稀缺的，而环境中的干扰注意力的信息却并不少。 比如人类的视觉神经系统大约每秒收到(10^8)位的信息， 这远远超过了大脑能够完全处理的水平。 幸运的是，人类的祖先已经从经验（也称为数据）中认识到 “并非感官的所有输入都是一样的”。 在整个人类历史中，这种只将注意力引向感兴趣的一小部分信息的能力， 使人类的大脑能够更明智地分配资源来生存、成长和社交， 例如发现天敌、找寻食物和伴侣。</p>
<ul>
<li>1.1 生物中的注意力提示<br>注意力是如何应用于视觉世界中的呢？ 这要从当今十分普及的双组件（two-component）的框架开始讲起： 这个框架的出现可以追溯到19世纪90年代的威廉·詹姆斯， 他被认为是“美国心理学之父” (James, 2007)。 在这个框架中，受试者基于非自主性提示和自主性提示 有选择地引导注意力的焦点。</li>
</ul>
<p>非自主性提示是基于环境中物体的突出性和易见性。 想象一下，假如我们面前有五个物品： 一份报纸、一篇研究论文、一杯咖啡、一本笔记本和一本书， 就像 图10.1.1。 所有纸制品都是黑白印刷的，但咖啡杯是红色的。 换句话说，这个咖啡杯在这种视觉环境中是突出和显眼的， 不由自主地引起人们的注意。 所以我们会把视力最敏锐的地方放到咖啡上， 如 图10.1.1所示。<br><img src="/../../../../gallery/attention1.jpg" alt="attention1"></p>
<p>喝咖啡后，我们会变得兴奋并想读书， 所以转过头，重新聚焦眼睛，然后看看书， 就像 图10.1.2中描述那样。 与 图10.1.1中由于突出性导致的选择不同， 此时选择书是受到了认知和意识的控制， 因此注意力在基于自主性提示去辅助选择时将更为谨慎。 受试者的主观意愿推动，选择的力量也就更强大。<br><img src="/../../../../gallery/attention2.jpg" alt="attention2"></p>
<ul>
<li>1.2 查询、键和值 （QKV）<br>自主性的与非自主性的注意力提示解释了人类的注意力的方式， 下面来看看如何通过这两种注意力提示， 用神经网络来设计注意力机制的框架，</li>
</ul>
<p>首先，考虑一个相对简单的状况， 即只使用非自主性提示。 要想将选择偏向于感官输入， 则可以简单地使用参数化的全连接层， 甚至是非参数化的最大汇聚层或平均汇聚层。</p>
<p>因此，“是否包含自主性提示”将注意力机制与全连接层或汇聚层区别开来。 在注意力机制的背景下，自主性提示被称为查询（query）。 给定任何查询，注意力机制通过注意力汇聚（attention pooling） 将选择引导至感官输入（sensory inputs，例如中间特征表示）。 在注意力机制中，这些感官输入被称为值（value）。 更通俗的解释，每个值都与一个键（key）配对， 这可以想象为感官输入的非自主提示。 如 图10.1.3所示，可以通过设计注意力汇聚的方式， 便于给定的查询（自主性提示）与键（非自主性提示）进行匹配， 这将引导得出最匹配的值（感官输入）。<br><img src="/../../../../gallery/attention3.jpg" alt="attention3"></p>
<ul>
<li><p>1.3 注意力的可视化<br>平均汇聚层可以被视为输入的加权平均值， 其中各输入的权重是一样的。 实际上，注意力汇聚得到的是加权平均的总和值， 其中权重是在给定的查询和不同的键之间计算得出的。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">#@save</span><br><span class="line">def show_heatmaps(matrices, xlabel, ylabel, titles=None, figsize=(2.5, 2.5),</span><br><span class="line">                  cmap=&#x27;Reds&#x27;):</span><br><span class="line">    &quot;&quot;&quot;显示矩阵热图&quot;&quot;&quot;</span><br><span class="line">    d2l.use_svg_display()</span><br><span class="line">    num_rows, num_cols = matrices.shape[0], matrices.shape[1]</span><br><span class="line">    fig, axes = d2l.plt.subplots(num_rows, num_cols, figsize=figsize,</span><br><span class="line">                                 sharex=True, sharey=True, squeeze=False)</span><br><span class="line">    for i, (row_axes, row_matrices) in enumerate(zip(axes, matrices)):</span><br><span class="line">        for j, (ax, matrix) in enumerate(zip(row_axes, row_matrices)):</span><br><span class="line">            pcm = ax.imshow(matrix.detach().numpy(), cmap=cmap)</span><br><span class="line">            if i == num_rows - 1:</span><br><span class="line">                ax.set_xlabel(xlabel)</span><br><span class="line">            if j == 0:</span><br><span class="line">                ax.set_ylabel(ylabel)</span><br><span class="line">            if titles:</span><br><span class="line">                ax.set_title(titles[j])</span><br><span class="line">    fig.colorbar(pcm, ax=axes, shrink=0.6);</span><br></pre></td></tr></table></figure>
</li>
<li><p>1.4 小结<br>人类的注意力是有限的、有价值和稀缺的资源。</p>
</li>
</ul>
<p>受试者使用非自主性和自主性提示有选择性地引导注意力。前者基于突出性，后者则依赖于意识。</p>
<p>注意力机制与全连接层或者汇聚层的区别源于增加的自主提示。</p>
<p>由于包含了自主性提示，注意力机制与全连接的层或汇聚层不同。</p>
<p>注意力机制通过注意力汇聚使选择偏向于值（感官输入），其中包含查询（自主性提示）和键（非自主性提示）。键和值是成对的。</p>
<p>可视化查询和键之间的注意力权重是可行的。</p>
<ol start="2">
<li>2.注意力汇聚：Nadaraya-Watson 核回归<br>上节介绍了框架下的注意力机制的主要成分 图10.1.3： 查询（自主提示）和键（非自主提示）之间的交互形成了注意力汇聚； 注意力汇聚有选择地聚合了值（感官输入）以生成最终的输出。 本节将介绍注意力汇聚的更多细节， 以便从宏观上了解注意力机制在实践中的运作方式。 具体来说，1964年提出的Nadaraya-Watson核回归模型 是一个简单但完整的例子，可以用于演示具有注意力机制的机器学习。</li>
</ol>
<ul>
<li><p>2.1 生成数据集</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">n_train = 50  # 训练样本数</span><br><span class="line">x_train, _ = torch.sort(torch.rand(n_train) * 5)   # 排序后的训练样本</span><br><span class="line"></span><br><span class="line">def f(x):</span><br><span class="line">    return 2 * torch.sin(x) + x**0.8</span><br><span class="line"></span><br><span class="line">y_train = f(x_train) + torch.normal(0.0, 0.5, (n_train,))  # 训练样本的输出</span><br><span class="line">x_test = torch.arange(0, 5, 0.1)  # 测试样本</span><br><span class="line">y_truth = f(x_test)  # 测试样本的真实输出</span><br><span class="line">n_test = len(x_test)  # 测试样本数</span><br><span class="line">n_test</span><br></pre></td></tr></table></figure>
</li>
<li><p>2.2 平均汇聚<br>先使用最简单的估计器来解决回归问题。 基于平均汇聚来计算所有训练样本输出值的平均值：<br>f(x)&#x3D;1&#x2F;nΣyi<br><img src="/../../../../gallery/ap.jpg" alt="ap"></p>
</li>
<li><p>2.3 非参数注意力汇聚<br><img src="/../../../../gallery/NW.jpg" alt="NW"></p>
</li>
</ul>
<p>值得注意的是，Nadaraya-Watson核回归是一个非参数模型。 因此， (10.2.6)是 非参数的注意力汇聚（nonparametric attention pooling）模型。 接下来，我们将基于这个非参数的注意力汇聚模型来绘制预测结果。 从绘制的结果会发现新的模型预测线是平滑的，并且比平均汇聚的预测更接近真实。<br><img src="/../../../../gallery/ap2.jpg" alt="ap2"><br>现在来观察注意力的权重。 这里测试数据的输入相当于查询，而训练数据的输入相当于键。 因为两个输入都是经过排序的，因此由观察可知“查询-键”对越接近， 注意力汇聚的注意力权重就越高。</p>
<ul>
<li><p>2.4 带参数注意力汇聚<br><img src="/../../../../gallery/NW2.jpg" alt="NW2"><br>基于 (10.2.7)中的 带参数的注意力汇聚，使用小批量矩阵乘法， 定义Nadaraya-Watson核回归的带参数版本为：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">class NWKernelRegression(nn.Module):</span><br><span class="line">    def __init__(self, **kwargs):</span><br><span class="line">        super().__init__(**kwargs)</span><br><span class="line">        self.w = nn.Parameter(torch.rand((1,), requires_grad=True))</span><br><span class="line"></span><br><span class="line">    def forward(self, queries, keys, values):</span><br><span class="line">        # queries和attention_weights的形状为(查询个数，“键－值”对个数)</span><br><span class="line">        queries = queries.repeat_interleave(keys.shape[1]).reshape((-1, keys.shape[1]))</span><br><span class="line">        self.attention_weights = nn.functional.softmax(</span><br><span class="line">            -((queries - keys) * self.w)**2 / 2, dim=1)</span><br><span class="line">        # values的形状为(查询个数，“键－值”对个数)</span><br><span class="line">        return torch.bmm(self.attention_weights.unsqueeze(1),</span><br><span class="line">                         values.unsqueeze(-1)).reshape(-1)</span><br></pre></td></tr></table></figure>

</li>
<li><p>2.5 总结<br>Nadaraya-Watson核回归是具有注意力机制的机器学习范例。</p>
</li>
</ul>
<p>Nadaraya-Watson核回归的注意力汇聚是对训练数据中输出的加权平均。从注意力的角度来看，分配给每个值的注意力权重取决于将值所对应的键和查询作为输入的函数。</p>
<p>注意力汇聚可以分为非参数型和带参数型。</p>
<ol start="3">
<li>3.注意力评分函数<br>10.2节使用了高斯核来对查询和键之间的关系建模。 (10.2.6)中的 高斯核指数部分可以视为注意力评分函数（attention scoring function）， 简称评分函数（scoring function）， 然后把这个函数的输出结果输入到softmax函数中进行运算。 通过上述步骤，将得到与键对应的值的概率分布（即注意力权重）。 最后，注意力汇聚的输出就是基于这些注意力权重的值的加权和。<br>从宏观来看，上述算法可以用来实现 图10.1.3中的注意力机制框架。 图10.3.1说明了 如何将注意力汇聚的输出计算成为值的加权和， 其中α表示注意力评分函数。 由于注意力权重是概率分布， 因此加权和其本质上是加权平均值。<br><img src="/../../../../gallery/asf.jpg" alt="asf"></li>
</ol>
<ul>
<li><p>3.1 掩蔽softmax操作<br>正如上面提到的，softmax操作用于输出一个概率分布作为注意力权重。 在某些情况下，并非所有的值都应该被纳入到注意力汇聚中。 例如，为了在 9.5节中高效处理小批量数据集， 某些文本序列被填充了没有意义的特殊词元。 为了仅将有意义的词元作为值来获取注意力汇聚， 可以指定一个有效序列长度（即词元的个数）， 以便在计算softmax时过滤掉超出指定范围的位置。 下面的masked_softmax函数 实现了这样的掩蔽softmax操作（masked softmax operation）， 其中任何超出有效长度的位置都被掩蔽并置为0。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">#@save</span><br><span class="line">def masked_softmax(X, valid_lens):</span><br><span class="line">    &quot;&quot;&quot;通过在最后一个轴上掩蔽元素来执行softmax操作&quot;&quot;&quot;</span><br><span class="line">    # X:3D张量，valid_lens:1D或2D张量</span><br><span class="line">    if valid_lens is None:</span><br><span class="line">        return nn.functional.softmax(X, dim=-1)</span><br><span class="line">    else:</span><br><span class="line">        shape = X.shape</span><br><span class="line">        if valid_lens.dim() == 1:</span><br><span class="line">            valid_lens = torch.repeat_interleave(valid_lens, shape[1])</span><br><span class="line">        else:</span><br><span class="line">            valid_lens = valid_lens.reshape(-1)</span><br><span class="line">        # 最后一轴上被掩蔽的元素使用一个非常大的负值替换，从而其softmax输出为0</span><br><span class="line">        X = d2l.sequence_mask(X.reshape(-1, shape[-1]), valid_lens,</span><br><span class="line">                              value=-1e6)</span><br><span class="line">        return nn.functional.softmax(X.reshape(shape), dim=-1)</span><br></pre></td></tr></table></figure>
</li>
<li><p>3.2 加性注意力<br><img src="/../../../../gallery/aa.jpg" alt="aa"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">class AdditiveAttention(nn.Module):</span><br><span class="line">    &quot;&quot;&quot;加性注意力&quot;&quot;&quot;</span><br><span class="line">    def __init__(self, key_size, query_size, num_hiddens, dropout, **kwargs):</span><br><span class="line">        super(AdditiveAttention, self).__init__(**kwargs)</span><br><span class="line">        self.W_k = nn.Linear(key_size, num_hiddens, bias=False)</span><br><span class="line">        self.W_q = nn.Linear(query_size, num_hiddens, bias=False)</span><br><span class="line">        self.w_v = nn.Linear(num_hiddens, 1, bias=False)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    def forward(self, queries, keys, values, valid_lens):</span><br><span class="line">        queries, keys = self.W_q(queries), self.W_k(keys)</span><br><span class="line">        # 在维度扩展后，</span><br><span class="line">        # queries的形状：(batch_size，查询的个数，1，num_hidden)</span><br><span class="line">        # key的形状：(batch_size，1，“键－值”对的个数，num_hiddens)</span><br><span class="line">        # 使用广播方式进行求和</span><br><span class="line">        features = queries.unsqueeze(2) + keys.unsqueeze(1)</span><br><span class="line">        features = torch.tanh(features)</span><br><span class="line">        # self.w_v仅有一个输出，因此从形状中移除最后那个维度。</span><br><span class="line">        # scores的形状：(batch_size，查询的个数，“键-值”对的个数)</span><br><span class="line">        scores = self.w_v(features).squeeze(-1)</span><br><span class="line">        self.attention_weights = masked_softmax(scores, valid_lens)</span><br><span class="line">        # values的形状：(batch_size，“键－值”对的个数，值的维度)</span><br><span class="line">        return torch.bmm(self.dropout(self.attention_weights), values)</span><br></pre></td></tr></table></figure>
</li>
<li><p>3.3 缩放点积注意力<br><img src="/../../../../gallery/sda.jpg" alt="sda"><br>下面的缩放点积注意力的实现使用了暂退法进行模型正则化。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">#@save</span><br><span class="line">class DotProductAttention(nn.Module):</span><br><span class="line">    &quot;&quot;&quot;缩放点积注意力&quot;&quot;&quot;</span><br><span class="line">    def __init__(self, dropout, **kwargs):</span><br><span class="line">        super(DotProductAttention, self).__init__(**kwargs)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    # queries的形状：(batch_size，查询的个数，d)</span><br><span class="line">    # keys的形状：(batch_size，“键－值”对的个数，d)</span><br><span class="line">    # values的形状：(batch_size，“键－值”对的个数，值的维度)</span><br><span class="line">    # valid_lens的形状:(batch_size，)或者(batch_size，查询的个数)</span><br><span class="line">    def forward(self, queries, keys, values, valid_lens=None):</span><br><span class="line">        d = queries.shape[-1]</span><br><span class="line">        # 设置transpose_b=True为了交换keys的最后两个维度</span><br><span class="line">        scores = torch.bmm(queries, keys.transpose(1,2)) / math.sqrt(d)</span><br><span class="line">        self.attention_weights = masked_softmax(scores, valid_lens)</span><br><span class="line">        return torch.bmm(self.dropout(self.attention_weights), values)</span><br></pre></td></tr></table></figure>
</li>
<li><p>3.4 小结<br>将注意力汇聚的输出计算可以作为值的加权平均，选择不同的注意力评分函数会带来不同的注意力汇聚操作。</p>
</li>
</ul>
<p>当查询和键是不同长度的矢量时，可以使用可加性注意力评分函数。当它们的长度相同时，使用缩放的“点－积”注意力评分函数的计算效率更高。</p>
<ol start="4">
<li>4.Bahdanau 注意力<br>过去我们探讨了机器翻译问题： 通过设计一个基于两个循环神经网络的编码器-解码器架构， 用于序列到序列学习。 具体来说，循环神经网络编码器将长度可变的序列转换为固定形状的上下文变量， 然后循环神经网络解码器根据生成的词元和上下文变量 按词元生成输出（目标）序列词元。 然而，即使并非所有输入（源）词元都对解码某个词元都有用， 在每个解码步骤中仍使用编码相同的上下文变量。 有什么方法能改变上下文变量呢？</li>
</ol>
<p>我们试着从 (Graves, 2013)中找到灵感： 在为给定文本序列生成手写的挑战中， Graves设计了一种可微注意力模型， 将文本字符与更长的笔迹对齐， 其中对齐方式仅向一个方向移动。 受学习对齐想法的启发，Bahdanau等人提出了一个没有严格单向对齐限制的 可微注意力模型 (Bahdanau et al., 2014)。 在预测词元时，如果不是所有输入词元都相关，模型将仅对齐（或参与）输入序列中与当前预测相关的部分。这是通过将上下文变量视为注意力集中的输出来实现的。</p>
<ul>
<li><p>4.1 模型<br><img src="/../../../../gallery/am.jpg" alt="am"></p>
</li>
<li><p>4.2 定义注意力解码器<br>下面看看如何定义Bahdanau注意力，实现循环神经网络编码器-解码器。 其实，我们只需重新定义解码器即可。 为了更方便地显示学习的注意力权重， 以下AttentionDecoder类定义了带有注意力机制解码器的基本接口。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">#@save</span><br><span class="line">class AttentionDecoder(d2l.Decoder):</span><br><span class="line">    &quot;&quot;&quot;带有注意力机制解码器的基本接口&quot;&quot;&quot;</span><br><span class="line">    def __init__(self, **kwargs):</span><br><span class="line">        super(AttentionDecoder, self).__init__(**kwargs)</span><br><span class="line"></span><br><span class="line">    @property</span><br><span class="line">    def attention_weights(self):</span><br><span class="line">        raise NotImplementedError</span><br></pre></td></tr></table></figure>
<p>接下来，让我们在接下来的Seq2SeqAttentionDecoder类中 实现带有Bahdanau注意力的循环神经网络解码器。 首先，初始化解码器的状态，需要下面的输入：</p>
</li>
</ul>
<p>编码器在所有时间步的最终层隐状态，将作为注意力的键和值；</p>
<p>最终时间步的编码器全层隐状态，将作为初始化解码器的隐状态；</p>
<p>编码器有效长度（排除在注意力池中填充词元）。</p>
<p>在每个解码时间步骤中，解码器上一个时间步的最终层隐状态将用作查询。 因此，注意力输出和输入嵌入都连结为循环神经网络解码器的输入。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">class Seq2SeqAttentionDecoder(AttentionDecoder):</span><br><span class="line">    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,</span><br><span class="line">                 dropout=0, **kwargs):</span><br><span class="line">        super(Seq2SeqAttentionDecoder, self).__init__(**kwargs)</span><br><span class="line">        self.attention = d2l.AdditiveAttention(</span><br><span class="line">            num_hiddens, num_hiddens, num_hiddens, dropout)</span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, embed_size)</span><br><span class="line">        self.rnn = nn.GRU(</span><br><span class="line">            embed_size + num_hiddens, num_hiddens, num_layers,</span><br><span class="line">            dropout=dropout)</span><br><span class="line">        self.dense = nn.Linear(num_hiddens, vocab_size)</span><br><span class="line"></span><br><span class="line">    def init_state(self, enc_outputs, enc_valid_lens, *args):</span><br><span class="line">        # outputs的形状为(batch_size，num_steps，num_hiddens).</span><br><span class="line">        # hidden_state的形状为(num_layers，batch_size，num_hiddens)</span><br><span class="line">        outputs, hidden_state = enc_outputs</span><br><span class="line">        return (outputs.permute(1, 0, 2), hidden_state, enc_valid_lens)</span><br><span class="line"></span><br><span class="line">    def forward(self, X, state):</span><br><span class="line">        # enc_outputs的形状为(batch_size,num_steps,num_hiddens).</span><br><span class="line">        # hidden_state的形状为(num_layers,batch_size,</span><br><span class="line">        # num_hiddens)</span><br><span class="line">        enc_outputs, hidden_state, enc_valid_lens = state</span><br><span class="line">        # 输出X的形状为(num_steps,batch_size,embed_size)</span><br><span class="line">        X = self.embedding(X).permute(1, 0, 2)</span><br><span class="line">        outputs, self._attention_weights = [], []</span><br><span class="line">        for x in X:</span><br><span class="line">            # query的形状为(batch_size,1,num_hiddens)</span><br><span class="line">            query = torch.unsqueeze(hidden_state[-1], dim=1)</span><br><span class="line">            # context的形状为(batch_size,1,num_hiddens)</span><br><span class="line">            context = self.attention(</span><br><span class="line">                query, enc_outputs, enc_outputs, enc_valid_lens)</span><br><span class="line">            # 在特征维度上连结</span><br><span class="line">            x = torch.cat((context, torch.unsqueeze(x, dim=1)), dim=-1)</span><br><span class="line">            # 将x变形为(1,batch_size,embed_size+num_hiddens)</span><br><span class="line">            out, hidden_state = self.rnn(x.permute(1, 0, 2), hidden_state)</span><br><span class="line">            outputs.append(out)</span><br><span class="line">            self._attention_weights.append(self.attention.attention_weights)</span><br><span class="line">        # 全连接层变换后，outputs的形状为</span><br><span class="line">        # (num_steps,batch_size,vocab_size)</span><br><span class="line">        outputs = self.dense(torch.cat(outputs, dim=0))</span><br><span class="line">        return outputs.permute(1, 0, 2), [enc_outputs, hidden_state,</span><br><span class="line">                                          enc_valid_lens]</span><br><span class="line"></span><br><span class="line">    @property</span><br><span class="line">    def attention_weights(self):</span><br><span class="line">        return self._attention_weights</span><br></pre></td></tr></table></figure>
<p>接下来，使用包含7个时间步的4个序列输入的小批量测试Bahdanau注意力解码器。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">encoder = d2l.Seq2SeqEncoder(vocab_size=10, embed_size=8, num_hiddens=16,</span><br><span class="line">                             num_layers=2)</span><br><span class="line">encoder.eval()</span><br><span class="line">decoder = Seq2SeqAttentionDecoder(vocab_size=10, embed_size=8, num_hiddens=16,</span><br><span class="line">                                  num_layers=2)</span><br><span class="line">decoder.eval()</span><br><span class="line">X = torch.zeros((4, 7), dtype=torch.long)  # (batch_size,num_steps)</span><br><span class="line">state = decoder.init_state(encoder(X), None)</span><br><span class="line">output, state = decoder(X, state)</span><br><span class="line">output.shape, len(state), state[0].shape, len(state[1]), state[1][0].shape</span><br></pre></td></tr></table></figure>

<ul>
<li><p>4.3 训练<br>与 9.7.4节类似， 我们在这里指定超参数，实例化一个带有Bahdanau注意力的编码器和解码器， 并对这个模型进行机器翻译训练。 由于新增的注意力机制，训练要比没有注意力机制的 9.7.4节慢得多。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">embed_size, num_hiddens, num_layers, dropout = 32, 32, 2, 0.1</span><br><span class="line">batch_size, num_steps = 64, 10</span><br><span class="line">lr, num_epochs, device = 0.005, 250, d2l.try_gpu()</span><br><span class="line"></span><br><span class="line">train_iter, src_vocab, tgt_vocab = d2l.load_data_nmt(batch_size, num_steps)</span><br><span class="line">encoder = d2l.Seq2SeqEncoder(</span><br><span class="line">    len(src_vocab), embed_size, num_hiddens, num_layers, dropout)</span><br><span class="line">decoder = Seq2SeqAttentionDecoder(</span><br><span class="line">    len(tgt_vocab), embed_size, num_hiddens, num_layers, dropout)</span><br><span class="line">net = d2l.EncoderDecoder(encoder, decoder)</span><br><span class="line">d2l.train_seq2seq(net, train_iter, lr, num_epochs, tgt_vocab, device)</span><br></pre></td></tr></table></figure>
<p>模型训练后，我们用它将几个英语句子翻译成法语并计算它们的BLEU分数。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">engs = [&#x27;go .&#x27;, &quot;i lost .&quot;, &#x27;he\&#x27;s calm .&#x27;, &#x27;i\&#x27;m home .&#x27;]</span><br><span class="line">fras = [&#x27;va !&#x27;, &#x27;j\&#x27;ai perdu .&#x27;, &#x27;il est calme .&#x27;, &#x27;je suis chez moi .&#x27;]</span><br><span class="line">for eng, fra in zip(engs, fras):</span><br><span class="line">    translation, dec_attention_weight_seq = d2l.predict_seq2seq(</span><br><span class="line">        net, eng, src_vocab, tgt_vocab, num_steps, device, True)</span><br><span class="line">    print(f&#x27;&#123;eng&#125; =&gt; &#123;translation&#125;, &#x27;,</span><br><span class="line">          f&#x27;bleu &#123;d2l.bleu(translation, fra, k=2):.3f&#125;&#x27;)</span><br></pre></td></tr></table></figure>
<p>训练结束后，下面通过可视化注意力权重 会发现，每个查询都会在键值对上分配不同的权重，这说明 在每个解码步中，输入序列的不同部分被选择性地聚集在注意力池中。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 加上一个包含序列结束词元</span><br><span class="line">d2l.show_heatmaps(</span><br><span class="line">    attention_weights[:, :, :, :len(engs[-1].split()) + 1].cpu(),</span><br><span class="line">    xlabel=&#x27;Key positions&#x27;, ylabel=&#x27;Query positions&#x27;)</span><br></pre></td></tr></table></figure>
<p><img src="/../../../../gallery/map.jpg" alt="map"></p>
</li>
<li><p>4.4 小结<br>在预测词元时，如果不是所有输入词元都是相关的，那么具有Bahdanau注意力的循环神经网络编码器-解码器会有选择地统计输入序列的不同部分。这是通过将上下文变量视为加性注意力池化的输出来实现的。</p>
</li>
</ul>
<p>在循环神经网络编码器-解码器中，Bahdanau注意力将上一时间步的解码器隐状态视为查询，在所有时间步的编码器隐状态同时视为键和值。</p>
<ol start="5">
<li>5.多头注意力<br>在实践中，当给定相同的查询、键和值的集合时， 我们希望模型可以基于相同的注意力机制学习到不同的行为， 然后将不同的行为作为知识组合起来， 捕获序列内各种范围的依赖关系 （例如，短距离依赖和长距离依赖关系）。 因此，允许注意力机制组合使用查询、键和值的不同 子空间表示（representation subspaces）可能是有益的。</li>
</ol>
<p>为此，与其只使用单独一个注意力汇聚， 我们可以用独立学习得到的h组不同的 线性投影（linear projections）来变换查询、键和值。 然后，这h组变换后的查询、键和值将并行地送到注意力汇聚中。 最后，将这<br>h个注意力汇聚的输出拼接在一起， 并且通过另一个可以学习的线性投影进行变换， 以产生最终输出。 这种设计被称为多头注意力（multihead attention） (Vaswani et al., 2017)。 对于h个注意力汇聚输出，每一个注意力汇聚都被称作一个头（head）。 图10.5.1 展示了使用全连接层来实现可学习的线性变换的多头注意力。<br><img src="/../../../../gallery/multihead.jpg" alt="multihead"></p>
<p>*5.1 模型<br><img src="/../../../../gallery/mh.jpg" alt="mh"></p>
<p>*5.2 实现<br>在实现过程中通常选择缩放点积注意力作为每一个注意力头。 为了避免计算代价和参数代价的大幅增长， 我们设定<br>pq&#x3D;pk&#x3D;pv&#x3D;po&#x2F;h。 值得注意的是，如果将查询、键和值的线性变换的输出数量设置为<br>pqh&#x3D;pkh&#x3D;pvh&#x3D;po， 则可以并行计算h个头。 在下面的实现中，po是通过参数num_hiddens指定的。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line">#@save</span><br><span class="line">class MultiHeadAttention(nn.Module):</span><br><span class="line">    &quot;&quot;&quot;多头注意力&quot;&quot;&quot;</span><br><span class="line">    def __init__(self, key_size, query_size, value_size, num_hiddens,</span><br><span class="line">                 num_heads, dropout, bias=False, **kwargs):</span><br><span class="line">        super(MultiHeadAttention, self).__init__(**kwargs)</span><br><span class="line">        self.num_heads = num_heads</span><br><span class="line">        self.attention = d2l.DotProductAttention(dropout)</span><br><span class="line">        self.W_q = nn.Linear(query_size, num_hiddens, bias=bias)</span><br><span class="line">        self.W_k = nn.Linear(key_size, num_hiddens, bias=bias)</span><br><span class="line">        self.W_v = nn.Linear(value_size, num_hiddens, bias=bias)</span><br><span class="line">        self.W_o = nn.Linear(num_hiddens, num_hiddens, bias=bias)</span><br><span class="line"></span><br><span class="line">    def forward(self, queries, keys, values, valid_lens):</span><br><span class="line">        # queries，keys，values的形状:</span><br><span class="line">        # (batch_size，查询或者“键－值”对的个数，num_hiddens)</span><br><span class="line">        # valid_lens　的形状:</span><br><span class="line">        # (batch_size，)或(batch_size，查询的个数)</span><br><span class="line">        # 经过变换后，输出的queries，keys，values　的形状:</span><br><span class="line">        # (batch_size*num_heads，查询或者“键－值”对的个数，</span><br><span class="line">        # num_hiddens/num_heads)</span><br><span class="line">        queries = transpose_qkv(self.W_q(queries), self.num_heads)</span><br><span class="line">        keys = transpose_qkv(self.W_k(keys), self.num_heads)</span><br><span class="line">        values = transpose_qkv(self.W_v(values), self.num_heads)</span><br><span class="line"></span><br><span class="line">        if valid_lens is not None:</span><br><span class="line">            # 在轴0，将第一项（标量或者矢量）复制num_heads次，</span><br><span class="line">            # 然后如此复制第二项，然后诸如此类。</span><br><span class="line">            valid_lens = torch.repeat_interleave(</span><br><span class="line">                valid_lens, repeats=self.num_heads, dim=0)</span><br><span class="line"></span><br><span class="line">        # output的形状:(batch_size*num_heads，查询的个数，</span><br><span class="line">        # num_hiddens/num_heads)</span><br><span class="line">        output = self.attention(queries, keys, values, valid_lens)</span><br><span class="line"></span><br><span class="line">        # output_concat的形状:(batch_size，查询的个数，num_hiddens)</span><br><span class="line">        output_concat = transpose_output(output, self.num_heads)</span><br><span class="line">        return self.W_o(output_concat)</span><br><span class="line"></span><br><span class="line">#@save</span><br><span class="line">def transpose_qkv(X, num_heads):</span><br><span class="line">    &quot;&quot;&quot;为了多注意力头的并行计算而变换形状&quot;&quot;&quot;</span><br><span class="line">    # 输入X的形状:(batch_size，查询或者“键－值”对的个数，num_hiddens)</span><br><span class="line">    # 输出X的形状:(batch_size，查询或者“键－值”对的个数，num_heads，</span><br><span class="line">    # num_hiddens/num_heads)</span><br><span class="line">    X = X.reshape(X.shape[0], X.shape[1], num_heads, -1)</span><br><span class="line"></span><br><span class="line">    # 输出X的形状:(batch_size，num_heads，查询或者“键－值”对的个数,</span><br><span class="line">    # num_hiddens/num_heads)</span><br><span class="line">    X = X.permute(0, 2, 1, 3)</span><br><span class="line"></span><br><span class="line">    # 最终输出的形状:(batch_size*num_heads,查询或者“键－值”对的个数,</span><br><span class="line">    # num_hiddens/num_heads)</span><br><span class="line">    return X.reshape(-1, X.shape[2], X.shape[3])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#@save</span><br><span class="line">def transpose_output(X, num_heads):</span><br><span class="line">    &quot;&quot;&quot;逆转transpose_qkv函数的操作&quot;&quot;&quot;</span><br><span class="line">    X = X.reshape(-1, num_heads, X.shape[1], X.shape[2])</span><br><span class="line">    X = X.permute(0, 2, 1, 3)</span><br><span class="line">    return X.reshape(X.shape[0], X.shape[1], -1)</span><br></pre></td></tr></table></figure>

<p>*5.3 总结<br>多头注意力融合了来自于多个注意力汇聚的不同知识，这些知识的不同来源于相同的查询、键和值的不同的子空间表示。</p>
<p>基于适当的张量操作，可以实现多头注意力的并行计算。</p>
<ol start="6">
<li>6.自注意力和位置编码<br>在深度学习中，经常使用卷积神经网络（CNN）或循环神经网络（RNN）对序列进行编码。 想象一下，有了注意力机制之后，我们将词元序列输入注意力池化中， 以便同一组词元同时充当查询、键和值。 具体来说，每个查询都会关注所有的键－值对并生成一个注意力输出。 由于查询、键和值来自同一组输入，因此被称为 自注意力（self-attention） (Lin et al., 2017, Vaswani et al., 2017)， 也被称为内部注意力（intra-attention） (Cheng et al., 2016, Parikh et al., 2016, Paulus et al., 2017)。</li>
</ol>
<ul>
<li><p>6.1 自注意力<br><img src="/../../../../gallery/selfa.jpg" alt="selfa"></p>
</li>
<li><p>6.2 比较卷积神经网络、循环神经网络和自注意力<br><img src="/../../../../gallery/compare.jpg" alt="compare"></p>
</li>
<li><p>6.3 位置编码<br>在处理词元序列时，循环神经网络是逐个的重复地处理词元的， 而自注意力则因为并行计算而放弃了顺序操作。 为了使用序列的顺序信息，通过在输入表示中添加 位置编码（positional encoding）来注入绝对的或相对的位置信息。 位置编码可以通过学习得到也可以直接固定得到。 接下来描述的是基于正弦函数和余弦函数的固定位置编码 (Vaswani et al., 2017)。</p>
</li>
<li><p>6.3.1 绝对位置信息</p>
</li>
<li><p>6.3.2 相对位置信息</p>
</li>
<li><p>6.4 总结<br>在自注意力中，查询、键和值都来自同一组输入。</p>
</li>
</ul>
<p>卷积神经网络和自注意力都拥有并行计算的优势，而且自注意力的最大路径长度最短。但是因为其计算复杂度是关于序列长度的二次方，所以在很长的序列中计算会非常慢。</p>
<p>为了使用序列的顺序信息，可以通过在输入表示中添加位置编码，来注入绝对的或相对的位置信息。</p>
<ol start="7">
<li>7.Transformer （2024&#x2F;12&#x2F;5 更）<br>10.6.2节中比较了卷积神经网络（CNN）、循环神经网络（RNN）和自注意力（self-attention）。值得注意的是，自注意力同时具有并行计算和最短的最大路径长度这两个优势。因此，使用自注意力来设计深度架构是很有吸引力的。对比之前仍然依赖循环神经网络实现输入表示的自注意力模型 (Cheng et al., 2016, Lin et al., 2017, Paulus et al., 2017)，Transformer模型完全基于注意力机制，没有任何卷积层或循环神经网络层 (Vaswani et al., 2017)。尽管Transformer最初是应用于在文本数据上的序列到序列学习，但现在已经推广到各种现代的深度学习中，例如语言、视觉、语音和强化学习领域。</li>
</ol>
<ul>
<li>7.1 模型<br>Transformer作为编码器－解码器架构的一个实例，其整体架构图在 图10.7.1中展示。正如所见到的，Transformer是由编码器和解码器组成的。与 图10.4.1中基于Bahdanau注意力实现的序列到序列的学习相比，Transformer的编码器和解码器是基于自注意力的模块叠加而成的，源（输入）序列和目标（输出）序列的嵌入（embedding）表示将加上位置编码（positional encoding），再分别输入到编码器和解码器中。<br><img src="/../../../../gallery/transformer.jpg" alt="transformer"></li>
</ul>
<p>图 图10.7.1中概述了Transformer的架构。从宏观角度来看，Transformer的编码器是由多个相同的层叠加而成的，每个层都有两个子层（子层表示为sublayer）。第一个子层是多头自注意力（multi-head self-attention）汇聚；第二个子层是基于位置的前馈网络（positionwise feed-forward network）。具体来说，在计算编码器的自注意力时，查询、键和值都来自前一个编码器层的输出。受 7.6节中残差网络的启发，每个子层都采用了残差连接（residual connection）。在Transformer中，对于序列中任何位置的任何输入<br>x∈Rd，都要求满足sublayer(x)∈Rd，以便残差连接满足x+sublayer(x)∈Rd。在残差连接的加法计算之后，紧接着应用层规范化（layer normalization） (Ba et al., 2016)。因此，输入序列对应的每个位置，Transformer编码器都将输出一个d维表示向量。</p>
<p>Transformer解码器也是由多个相同的层叠加而成的，并且层中使用了残差连接和层规范化。除了编码器中描述的两个子层之外，解码器还在这两个子层之间插入了第三个子层，称为编码器－解码器注意力（encoder-decoder attention）层。在编码器－解码器注意力中，查询来自前一个解码器层的输出，而键和值来自整个编码器的输出。在解码器自注意力中，查询、键和值都来自上一个解码器层的输出。但是，解码器中的每个位置只能考虑该位置之前的所有位置。这种掩蔽（masked）注意力保留了自回归（auto-regressive）属性，确保预测仅依赖于已生成的输出词元。</p>
<p>在此之前已经描述并实现了基于缩放点积多头注意力 10.5节和位置编码 10.6.3节。接下来将实现Transformer模型的剩余部分。</p>
<ul>
<li><p>7.2 基于位置的前馈网络<br>基于位置的前馈网络对序列中的所有位置的表示进行变换时使用的是同一个多层感知机（MLP），这就是称前馈网络是基于位置的（positionwise）的原因。在下面的实现中，输入X的形状（批量大小，时间步数或序列长度，隐单元数或特征维度）将被一个两层的感知机转换成形状为（批量大小，时间步数，ffn_num_outputs）的输出张量。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">#@save</span><br><span class="line">class PositionWiseFFN(nn.Module):</span><br><span class="line">    &quot;&quot;&quot;基于位置的前馈网络&quot;&quot;&quot;</span><br><span class="line">    def __init__(self, ffn_num_input, ffn_num_hiddens, ffn_num_outputs,</span><br><span class="line">                 **kwargs):</span><br><span class="line">        super(PositionWiseFFN, self).__init__(**kwargs)</span><br><span class="line">        self.dense1 = nn.Linear(ffn_num_input, ffn_num_hiddens)</span><br><span class="line">        self.relu = nn.ReLU()</span><br><span class="line">        self.dense2 = nn.Linear(ffn_num_hiddens, ffn_num_outputs)</span><br><span class="line"></span><br><span class="line">    def forward(self, X):</span><br><span class="line">        return self.dense2(self.relu(self.dense1(X)))</span><br></pre></td></tr></table></figure>
<p>下面的例子显示，改变张量的最里层维度的尺寸，会改变成基于位置的前馈网络的输出尺寸。因为用同一个多层感知机对所有位置上的输入进行变换，所以当所有这些位置的输入相同时，它们的输出也是相同的。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">ffn = PositionWiseFFN(4, 4, 8)</span><br><span class="line">ffn.eval()</span><br><span class="line">ffn(torch.ones((2, 3, 4)))[0]</span><br><span class="line"></span><br><span class="line">#tensor([[-0.8290,  1.0067,  0.3619,  0.3594, -0.5328,  0.2712,  0.7394,  0.0747],</span><br><span class="line">#        [-0.8290,  1.0067,  0.3619,  0.3594, -0.5328,  0.2712,  0.7394,  0.0747],</span><br><span class="line">#        [-0.8290,  1.0067,  0.3619,  0.3594, -0.5328,  0.2712,  0.7394,  0.0747]],</span><br><span class="line">#       grad_fn=&lt;SelectBackward0&gt;)</span><br></pre></td></tr></table></figure>
</li>
<li><p>7.3 残差连接和层规范化<br>现在让我们关注 图10.7.1中的加法和规范化（add&amp;norm）组件。正如在本节开头所述，这是由残差连接和紧随其后的层规范化组成的。两者都是构建有效的深度架构的关键。</p>
</li>
</ul>
<p>7.5节中解释了在一个小批量的样本内基于批量规范化对数据进行重新中心化和重新缩放的调整。层规范化和批量规范化的目标相同，但层规范化是基于特征维度进行规范化。尽管批量规范化在计算机视觉中被广泛应用，但在自然语言处理任务中（输入通常是变长序列）批量规范化通常不如层规范化的效果好。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">#@save</span><br><span class="line">class AddNorm(nn.Module):</span><br><span class="line">    &quot;&quot;&quot;残差连接后进行层规范化&quot;&quot;&quot;</span><br><span class="line">    def __init__(self, normalized_shape, dropout, **kwargs):</span><br><span class="line">        super(AddNorm, self).__init__(**kwargs)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        self.ln = nn.LayerNorm(normalized_shape)</span><br><span class="line"></span><br><span class="line">    def forward(self, X, Y):</span><br><span class="line">        return self.ln(self.dropout(Y) + X)</span><br></pre></td></tr></table></figure>
<p>残差连接要求两个输入的形状相同，以便加法操作后输出张量的形状相同。</p>
<ul>
<li>7.4 编码器<br>有了组成Transformer编码器的基础组件，现在可以先实现编码器中的一个层。下面的EncoderBlock类包含两个子层：多头自注意力和基于位置的前馈网络，这两个子层都使用了残差连接和紧随的层规范化。<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">#@save</span><br><span class="line">class EncoderBlock(nn.Module):</span><br><span class="line">    &quot;&quot;&quot;Transformer编码器块&quot;&quot;&quot;</span><br><span class="line">    def __init__(self, key_size, query_size, value_size, num_hiddens,</span><br><span class="line">                 norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,</span><br><span class="line">                 dropout, use_bias=False, **kwargs):</span><br><span class="line">        super(EncoderBlock, self).__init__(**kwargs)</span><br><span class="line">        self.attention = d2l.MultiHeadAttention(</span><br><span class="line">            key_size, query_size, value_size, num_hiddens, num_heads, dropout,</span><br><span class="line">            use_bias)</span><br><span class="line">        self.addnorm1 = AddNorm(norm_shape, dropout)</span><br><span class="line">        self.ffn = PositionWiseFFN(</span><br><span class="line">            ffn_num_input, ffn_num_hiddens, num_hiddens)</span><br><span class="line">        self.addnorm2 = AddNorm(norm_shape, dropout)</span><br><span class="line"></span><br><span class="line">    def forward(self, X, valid_lens):</span><br><span class="line">        Y = self.addnorm1(X, self.attention(X, X, X, valid_lens))</span><br><span class="line">        return self.addnorm2(Y, self.ffn(Y))</span><br></pre></td></tr></table></figure>
下面实现的Transformer编码器的代码中，堆叠了num_layers个EncoderBlock类的实例。由于这里使用的是值范围在-1和1之间的固定位置编码，因此通过学习得到的输入的嵌入表示的值需要先乘以嵌入维度的平方根进行重新缩放，然后再与位置编码相加。</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">#@save</span><br><span class="line">class TransformerEncoder(d2l.Encoder):</span><br><span class="line">    &quot;&quot;&quot;Transformer编码器&quot;&quot;&quot;</span><br><span class="line">    def __init__(self, vocab_size, key_size, query_size, value_size,</span><br><span class="line">                 num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens,</span><br><span class="line">                 num_heads, num_layers, dropout, use_bias=False, **kwargs):</span><br><span class="line">        super(TransformerEncoder, self).__init__(**kwargs)</span><br><span class="line">        self.num_hiddens = num_hiddens</span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, num_hiddens)</span><br><span class="line">        self.pos_encoding = d2l.PositionalEncoding(num_hiddens, dropout)</span><br><span class="line">        self.blks = nn.Sequential()</span><br><span class="line">        for i in range(num_layers):</span><br><span class="line">            self.blks.add_module(&quot;block&quot;+str(i),</span><br><span class="line">                EncoderBlock(key_size, query_size, value_size, num_hiddens,</span><br><span class="line">                             norm_shape, ffn_num_input, ffn_num_hiddens,</span><br><span class="line">                             num_heads, dropout, use_bias))</span><br><span class="line"></span><br><span class="line">    def forward(self, X, valid_lens, *args):</span><br><span class="line">        # 因为位置编码值在-1和1之间，</span><br><span class="line">        # 因此嵌入值乘以嵌入维度的平方根进行缩放，</span><br><span class="line">        # 然后再与位置编码相加。</span><br><span class="line">        X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens))</span><br><span class="line">        self.attention_weights = [None] * len(self.blks)</span><br><span class="line">        for i, blk in enumerate(self.blks):</span><br><span class="line">            X = blk(X, valid_lens)</span><br><span class="line">            self.attention_weights[</span><br><span class="line">                i] = blk.attention.attention.attention_weights</span><br><span class="line">        return X</span><br></pre></td></tr></table></figure>
<p>下面我们指定了超参数来创建一个两层的Transformer编码器。 Transformer编码器输出的形状是（批量大小，时间步数目，num_hiddens）。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">encoder = TransformerEncoder(</span><br><span class="line">    200, 24, 24, 24, 24, [100, 24], 24, 48, 8, 2, 0.5)</span><br><span class="line">encoder.eval()</span><br><span class="line">encoder(torch.ones((2, 100), dtype=torch.long), valid_lens).shape</span><br></pre></td></tr></table></figure>

<ul>
<li>7.5 解码器<br>如 图10.7.1所示，Transformer解码器也是由多个相同的层组成。在DecoderBlock类中实现的每个层包含了三个子层：解码器自注意力、“编码器-解码器”注意力和基于位置的前馈网络。这些子层也都被残差连接和紧随的层规范化围绕。</li>
</ul>
<p>正如在本节前面所述，在掩蔽多头解码器自注意力层（第一个子层）中，查询、键和值都来自上一个解码器层的输出。关于序列到序列模型（sequence-to-sequence model），在训练阶段，其输出序列的所有位置（时间步）的词元都是已知的；然而，在预测阶段，其输出序列的词元是逐个生成的。因此，在任何解码器时间步中，只有生成的词元才能用于解码器的自注意力计算中。为了在解码器中保留自回归的属性，其掩蔽自注意力设定了参数dec_valid_lens，以便任何查询都只会与解码器中所有已经生成词元的位置（即直到该查询位置为止）进行注意力计算。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">class DecoderBlock(nn.Module):</span><br><span class="line">    &quot;&quot;&quot;解码器中第i个块&quot;&quot;&quot;</span><br><span class="line">    def __init__(self, key_size, query_size, value_size, num_hiddens,</span><br><span class="line">                 norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,</span><br><span class="line">                 dropout, i, **kwargs):</span><br><span class="line">        super(DecoderBlock, self).__init__(**kwargs)</span><br><span class="line">        self.i = i</span><br><span class="line">        self.attention1 = d2l.MultiHeadAttention(</span><br><span class="line">            key_size, query_size, value_size, num_hiddens, num_heads, dropout)</span><br><span class="line">        self.addnorm1 = AddNorm(norm_shape, dropout)</span><br><span class="line">        self.attention2 = d2l.MultiHeadAttention(</span><br><span class="line">            key_size, query_size, value_size, num_hiddens, num_heads, dropout)</span><br><span class="line">        self.addnorm2 = AddNorm(norm_shape, dropout)</span><br><span class="line">        self.ffn = PositionWiseFFN(ffn_num_input, ffn_num_hiddens,</span><br><span class="line">                                   num_hiddens)</span><br><span class="line">        self.addnorm3 = AddNorm(norm_shape, dropout)</span><br><span class="line"></span><br><span class="line">    def forward(self, X, state):</span><br><span class="line">        enc_outputs, enc_valid_lens = state[0], state[1]</span><br><span class="line">        # 训练阶段，输出序列的所有词元都在同一时间处理，</span><br><span class="line">        # 因此state[2][self.i]初始化为None。</span><br><span class="line">        # 预测阶段，输出序列是通过词元一个接着一个解码的，</span><br><span class="line">        # 因此state[2][self.i]包含着直到当前时间步第i个块解码的输出表示</span><br><span class="line">        if state[2][self.i] is None:</span><br><span class="line">            key_values = X</span><br><span class="line">        else:</span><br><span class="line">            key_values = torch.cat((state[2][self.i], X), axis=1)</span><br><span class="line">        state[2][self.i] = key_values</span><br><span class="line">        if self.training:</span><br><span class="line">            batch_size, num_steps, _ = X.shape</span><br><span class="line">            # dec_valid_lens的开头:(batch_size,num_steps),</span><br><span class="line">            # 其中每一行是[1,2,...,num_steps]</span><br><span class="line">            dec_valid_lens = torch.arange(</span><br><span class="line">                1, num_steps + 1, device=X.device).repeat(batch_size, 1)</span><br><span class="line">        else:</span><br><span class="line">            dec_valid_lens = None</span><br><span class="line"></span><br><span class="line">        # 自注意力</span><br><span class="line">        X2 = self.attention1(X, key_values, key_values, dec_valid_lens)</span><br><span class="line">        Y = self.addnorm1(X, X2)</span><br><span class="line">        # 编码器－解码器注意力。</span><br><span class="line">        # enc_outputs的开头:(batch_size,num_steps,num_hiddens)</span><br><span class="line">        Y2 = self.attention2(Y, enc_outputs, enc_outputs, enc_valid_lens)</span><br><span class="line">        Z = self.addnorm2(Y, Y2)</span><br><span class="line">        return self.addnorm3(Z, self.ffn(Z)), state</span><br></pre></td></tr></table></figure>
<p>为了便于在“编码器－解码器”注意力中进行缩放点积计算和残差连接中进行加法计算，编码器和解码器的特征维度都是num_hiddens。</p>
<p>现在我们构建了由num_layers个DecoderBlock实例组成的完整的Transformer解码器。最后，通过一个全连接层计算所有vocab_size个可能的输出词元的预测值。解码器的自注意力权重和编码器解码器注意力权重都被存储下来，方便日后可视化的需要。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">class TransformerDecoder(d2l.AttentionDecoder):</span><br><span class="line">    def __init__(self, vocab_size, key_size, query_size, value_size,</span><br><span class="line">                 num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens,</span><br><span class="line">                 num_heads, num_layers, dropout, **kwargs):</span><br><span class="line">        super(TransformerDecoder, self).__init__(**kwargs)</span><br><span class="line">        self.num_hiddens = num_hiddens</span><br><span class="line">        self.num_layers = num_layers</span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, num_hiddens)</span><br><span class="line">        self.pos_encoding = d2l.PositionalEncoding(num_hiddens, dropout)</span><br><span class="line">        self.blks = nn.Sequential()</span><br><span class="line">        for i in range(num_layers):</span><br><span class="line">            self.blks.add_module(&quot;block&quot;+str(i),</span><br><span class="line">                DecoderBlock(key_size, query_size, value_size, num_hiddens,</span><br><span class="line">                             norm_shape, ffn_num_input, ffn_num_hiddens,</span><br><span class="line">                             num_heads, dropout, i))</span><br><span class="line">        self.dense = nn.Linear(num_hiddens, vocab_size)</span><br><span class="line"></span><br><span class="line">    def init_state(self, enc_outputs, enc_valid_lens, *args):</span><br><span class="line">        return [enc_outputs, enc_valid_lens, [None] * self.num_layers]</span><br><span class="line"></span><br><span class="line">    def forward(self, X, state):</span><br><span class="line">        X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens))</span><br><span class="line">        self._attention_weights = [[None] * len(self.blks) for _ in range (2)]</span><br><span class="line">        for i, blk in enumerate(self.blks):</span><br><span class="line">            X, state = blk(X, state)</span><br><span class="line">            # 解码器自注意力权重</span><br><span class="line">            self._attention_weights[0][</span><br><span class="line">                i] = blk.attention1.attention.attention_weights</span><br><span class="line">            # “编码器－解码器”自注意力权重</span><br><span class="line">            self._attention_weights[1][</span><br><span class="line">                i] = blk.attention2.attention.attention_weights</span><br><span class="line">        return self.dense(X), state</span><br><span class="line"></span><br><span class="line">    @property</span><br><span class="line">    def attention_weights(self):</span><br><span class="line">        return self._attention_weights</span><br></pre></td></tr></table></figure>

<ul>
<li>7.6 训练<br>依照Transformer架构来实例化编码器－解码器模型。在这里，指定Transformer的编码器和解码器都是2层，都使用4头注意力。与 9.7.4节类似，为了进行序列到序列的学习，下面在“英语－法语”机器翻译数据集上训练Transformer模型。<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">num_hiddens, num_layers, dropout, batch_size, num_steps = 32, 2, 0.1, 64, 10</span><br><span class="line">lr, num_epochs, device = 0.005, 200, d2l.try_gpu()</span><br><span class="line">ffn_num_input, ffn_num_hiddens, num_heads = 32, 64, 4</span><br><span class="line">key_size, query_size, value_size = 32, 32, 32</span><br><span class="line">norm_shape = [32]</span><br><span class="line"></span><br><span class="line">train_iter, src_vocab, tgt_vocab = d2l.load_data_nmt(batch_size, num_steps)</span><br><span class="line"></span><br><span class="line">encoder = TransformerEncoder(</span><br><span class="line">    len(src_vocab), key_size, query_size, value_size, num_hiddens,</span><br><span class="line">    norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,</span><br><span class="line">    num_layers, dropout)</span><br><span class="line">decoder = TransformerDecoder(</span><br><span class="line">    len(tgt_vocab), key_size, query_size, value_size, num_hiddens,</span><br><span class="line">    norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,</span><br><span class="line">    num_layers, dropout)</span><br><span class="line">net = d2l.EncoderDecoder(encoder, decoder)</span><br><span class="line">d2l.train_seq2seq(net, train_iter, lr, num_epochs, tgt_vocab, device)</span><br></pre></td></tr></table></figure>
训练结束后，使用Transformer模型将一些英语句子翻译成法语，并且计算它们的BLEU分数。<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">engs = [&#x27;go .&#x27;, &quot;i lost .&quot;, &#x27;he\&#x27;s calm .&#x27;, &#x27;i\&#x27;m home .&#x27;]</span><br><span class="line">fras = [&#x27;va !&#x27;, &#x27;j\&#x27;ai perdu .&#x27;, &#x27;il est calme .&#x27;, &#x27;je suis chez moi .&#x27;]</span><br><span class="line">for eng, fra in zip(engs, fras):</span><br><span class="line">    translation, dec_attention_weight_seq = d2l.predict_seq2seq(</span><br><span class="line">        net, eng, src_vocab, tgt_vocab, num_steps, device, True)</span><br><span class="line">    print(f&#x27;&#123;eng&#125; =&gt; &#123;translation&#125;, &#x27;,</span><br><span class="line">          f&#x27;bleu &#123;d2l.bleu(translation, fra, k=2):.3f&#125;&#x27;)</span><br></pre></td></tr></table></figure>
当进行最后一个英语到法语的句子翻译工作时，让我们可视化Transformer的注意力权重。编码器自注意力权重的形状为（编码器层数，注意力头数，num_steps或查询的数目，num_steps或“键－值”对的数目）。</li>
</ul>
<p>在<strong>编码器的自注意力</strong>中，查询和键都来自相同的输入序列。因为填充词元是不携带信息的，因此通过指定输入序列的有效长度可以避免查询与使用填充词元的位置计算注意力。接下来，将逐行呈现两层多头注意力的权重。每个注意力头都根据查询、键和值的不同的表示子空间来表示不同的注意力。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">enc_attention_weights = torch.cat(net.encoder.attention_weights, 0).reshape((num_layers, num_heads,</span><br><span class="line">    -1, num_steps))</span><br><span class="line">enc_attention_weights.shape</span><br><span class="line"></span><br><span class="line">d2l.show_heatmaps(</span><br><span class="line">    enc_attention_weights.cpu(), xlabel=&#x27;Key positions&#x27;,</span><br><span class="line">    ylabel=&#x27;Query positions&#x27;, titles=[&#x27;Head %d&#x27; % i for i in range(1, 5)],</span><br><span class="line">    figsize=(7, 3.5))</span><br></pre></td></tr></table></figure>
<p><img src="/../../../../gallery/transmap1.jpg" alt="transmap1"></p>
<p>为了可视化<strong>解码器的自注意力权重和“编码器－解码器”的注意力权重</strong>，我们需要完成更多的数据操作工作。例如用零填充被掩蔽住的注意力权重。值得注意的是，解码器的自注意力权重和“编码器－解码器”的注意力权重都有相同的查询：即以序列开始词元（beginning-of-sequence,BOS）打头，再与后续输出的词元共同组成序列。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">dec_attention_weights_2d = [head[0].tolist()</span><br><span class="line">                            for step in dec_attention_weight_seq</span><br><span class="line">                            for attn in step for blk in attn for head in blk]</span><br><span class="line">dec_attention_weights_filled = torch.tensor(</span><br><span class="line">    pd.DataFrame(dec_attention_weights_2d).fillna(0.0).values)</span><br><span class="line">dec_attention_weights = dec_attention_weights_filled.reshape((-1, 2, num_layers, num_heads, num_steps))</span><br><span class="line">dec_self_attention_weights, dec_inter_attention_weights = \</span><br><span class="line">    dec_attention_weights.permute(1, 2, 3, 0, 4)</span><br><span class="line">dec_self_attention_weights.shape, dec_inter_attention_weights.shape</span><br><span class="line"></span><br><span class="line"># Plusonetoincludethebeginning-of-sequencetoken</span><br><span class="line">d2l.show_heatmaps(</span><br><span class="line">    dec_self_attention_weights[:, :, :, :len(translation.split()) + 1],</span><br><span class="line">    xlabel=&#x27;Key positions&#x27;, ylabel=&#x27;Query positions&#x27;,</span><br><span class="line">    titles=[&#x27;Head %d&#x27; % i for i in range(1, 5)], figsize=(7, 3.5))</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><img src="/../../../../gallery/transmap2.jpg" alt="transmap2"></p>
<p>与编码器的自注意力的情况类似，通过指定输入序列的有效长度，输出序列的查询不会与输入序列中填充位置的词元进行注意力计算。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">d2l.show_heatmaps(</span><br><span class="line">    dec_inter_attention_weights, xlabel=&#x27;Key positions&#x27;,</span><br><span class="line">    ylabel=&#x27;Query positions&#x27;, titles=[&#x27;Head %d&#x27; % i for i in range(1, 5)],</span><br><span class="line">    figsize=(7, 3.5))</span><br></pre></td></tr></table></figure>
<p><img src="/../../../../gallery/transmap3.jpg" alt="transmap3"></p>
<p>尽管Transformer架构是为了序列到序列的学习而提出的，但正如本书后面将提及的那样，Transformer编码器或Transformer解码器通常被单独用于不同的深度学习任务中。</p>
<ul>
<li>7.7 总结<br>Transformer是编码器－解码器架构的一个实践，尽管在实际情况中编码器或解码器可以单独使用。</li>
</ul>
<p>在Transformer中，多头自注意力用于表示输入序列和输出序列，不过解码器必须通过掩蔽机制来保留自回归属性。</p>
<p>Transformer中的残差连接和层规范化是训练非常深度模型的重要工具。</p>
<p>Transformer模型中基于位置的前馈网络使用同一个多层感知机，作用是对所有序列位置的表示进行转换。</p>
<h1 id="十一、-优化算法"><a href="#十一、-优化算法" class="headerlink" title="十一、 优化算法"></a>十一、 优化算法</h1><h1 id="十二、-计算性能"><a href="#十二、-计算性能" class="headerlink" title="十二、 计算性能"></a>十二、 计算性能</h1><h1 id="十三、-计算机视觉"><a href="#十三、-计算机视觉" class="headerlink" title="十三、 计算机视觉"></a>十三、 计算机视觉</h1><h1 id="十四、-NLP：预处理"><a href="#十四、-NLP：预处理" class="headerlink" title="十四、 NLP：预处理"></a>十四、 NLP：预处理</h1><h1 id="十五、-NLP：-应用"><a href="#十五、-NLP：-应用" class="headerlink" title="十五、 NLP： 应用"></a>十五、 NLP： 应用</h1><h1 id="十六、-深度学习工具"><a href="#十六、-深度学习工具" class="headerlink" title="十六、 深度学习工具"></a>十六、 深度学习工具</h1>
            </div>

            <!-- Post Comments -->
            
    <!-- 使用 DISQUS_CLICK -->
<div id="disqus-comment">
    <div id="disqus_thread"></div>

<!-- add animation -->
<style>
	.disqus_click_btn {
            line-height: 30px;
            margin: 0;
            min-width: 50px;
            padding: 0 14px;
            display: inline-block;
            font-family: "Roboto", "Helvetica", "Arial", sans-serif;
            font-size: 14px;
            font-weight: 400;
            text-transform: uppercase;
            letter-spacing: 0;
            overflow: hidden;
            will-change: box-shadow;
            transition: box-shadow .2s cubic-bezier(.4, 0, 1, 1), background-color .2s cubic-bezier(.4, 0, .2, 1), color .2s cubic-bezier(.4, 0, .2, 1);
            outline: 0;
            cursor: pointer;
            text-decoration: none;
            text-align: center;
            vertical-align: middle;
            border: 0;
            background: rgba(158, 158, 158, .2);
            box-shadow: 0 2px 2px 0 rgba(0, 0, 0, .14), 0 3px 1px -2px rgba(0, 0, 0, .2), 0 1px 5px 0 rgba(0, 0, 0, .12);
            color: #fff;
            background-color: #7EC0EE;
            text-shadow: 0
        }
</style>
	
<div class="btn_click_load" id="disqus_bt"> 
    <button class="disqus_click_btn">点击查看评论</button>
</div>

<!--
<script type="text/javascript">
$('.btn_click_load').click(function() {
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'http-eggman-blog'; // required: replace example with your forum shortname

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();

    document.getElementById('disqus_bt').style.display = "none";
});
</script>
-->
<script type="text/javascript">
    var disqus_config = function () {
        this.page.url = 'http://cuiyi126.github.io/2024/07/14/Review_%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/';  // Replace PAGE_URL with your page's canonical URL variable
        this.page.identifier = 'http://cuiyi126.github.io/2024/07/14/Review_%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/'; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
</script>

<script type="text/javascript">
    $('.btn_click_load').click(function() {  //click to load comments
        (function() { // DON'T EDIT BELOW THIS LINE
            var d = document;
            var s = d.createElement('script');
            s.src = '//http-eggman-blog.disqus.com/embed.js';
            s.setAttribute('data-timestamp', + new Date());
            (d.head || d.body).appendChild(s);
        })();
        $('.btn_click_load').css('display','none');
    });
</script>
</div>
<style>
    #disqus-comment{
        background-color: #eee;
        padding: 2pc;
    }
</style>


        </div>
        <!-- Copyright 版权 start -->
                <div id="copyright">
            <ul>
                <li>&copy;Powered By <a target="_blank" rel="noopener" href="https://hexo.io/zh-cn/" style="border-bottom: none;">hexo</a></li>
                <li>Design: <a target="_blank" rel="noopener" href="http://miccall.tech " style="border-bottom: none;">Thanks: miccall</a></li>
            </ul>
            <ul>
                <li xmlns:cc="http://creativecommons.org/ns#" > Licensed under <a href="http://creativecommons.org/licenses/by-nc-sa/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;">CC BY-NC-SA 4.0<img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/nc.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/sa.svg?ref=chooser-v1"></a></li>
            </ul>
            
                <span id="busuanzi_container_site_pv">Views: <span id="busuanzi_value_site_pv"></span> times</span>
			
        </div>
    </div>
</body>



 	
</html>
