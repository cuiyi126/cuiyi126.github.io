<!DOCTYPE HTML>
<html>

<head>
	<link rel="bookmark"  type="image/x-icon"  href="/img/logo.jpg"/>
	<link rel="shortcut icon" href="/img/logo.jpg">
	
			    <title>
    Eggman
    </title>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
    <link rel="stylesheet" href="/css/mic_main.css" />
    <link rel="stylesheet" href="/css/dropdownMenu.css" />
    <meta name="keywords" content="cuiyi eggman" />
    
    	<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
	 
    <noscript>
        <link rel="stylesheet" href="/css/noscript.css" />
    </noscript>
    <style type="text/css">
        body:before {
          content: ' ';
          position: fixed;
          top: 0;
          background: url('/img/bg.jpg') center 0 no-repeat;
          right: 0;
          bottom: 0;
          left: 0;
          background-size: cover; 
        }
    </style>

			    
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script async type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


    <script src="/js/jquery.min.js"></script>
    <script src="/js/jquery.scrollex.min.js"></script>
    <script src="/js/jquery.scrolly.min.js"></script>
    <script src="/js/skel.min.js"></script>
    <script src="/js/util.js"></script>
    <script src="/js/main.js"></script>
	
<meta name="generator" content="Hexo 6.3.0"></head>
    
		
<!-- Layouts -->



<!--  代码渲染  -->
<link rel="stylesheet" href="/css/prism_coy.css" />
<link rel="stylesheet" href="/css/typo.css" />
<!-- 文章页 -->
<body class="is-loading">
    <!-- Wrapper 外包 s-->
    <div id="wrapper" class="fade-in">
        <!-- Intro 头部显示 s -->
        <!-- Intro 头部显示 e -->
        <!-- Header 头部logo start -->
        <header id="header">
    <a href="/" class="logo">Innovation</a>
</header>
        <!-- Nav 导航条 start -->
        <nav id="nav" class="special" >
            <ul class="menu links" >
			<!-- Homepage  主页  --> 
			<li >
	            <a href="/" rel="nofollow">Home</a>
	        </li>
			<!-- categories_name  分类   --> 
	        
	        <!-- archives  归档   --> 
	        
	        <li class="active">
	            <a href="#s1">Archive</a>
	                    <ul class="submenu">
	                        <li>
	                        <a class="archive-link" href="/archives/2024/09/">September 2024</a></li><li><a class="archive-link" href="/archives/2024/07/">July 2024</a></li><li><a class="archive-link" href="/archives/2024/04/">April 2024</a></li><li><a class="archive-link" href="/archives/2023/09/">September 2023</a>
	                    </ul>
	        </li>
	        
	        
		        <!-- Pages 自定义   -->
		        
		        <li>
		            <a href="/about/" title="About me">
		                About me
		            </a>
		        </li>
		        
		        <li>
		            <a href="/group/" title="Team">
		                Team
		            </a>
		        </li>
		        
		        <li>
		            <a href="/gallery/" title="Gallery">
		                Gallery
		            </a>
		        </li>
		        


            </ul>
            <!-- icons 图标   -->
			<ul class="icons">
                    
                    <li>
                        <a title="github" href="https://github.com/cuiyi126" target="_blank" rel="noopener">
                            <i class="icon fa fa-github"></i>
                        </a>
                    </li>
                    
                    <li>
                        <a title="qq" href="https://qm.qq.com/q/9CbJ0aB9u0&personal_qrcode_source=4" target="_blank" rel="noopener">
                            <i class="icon fa fa-qq"></i>
                        </a>
                    </li>
                    
			</ul>
</nav>

        <div id="main" >
            <div class ="post_page_title_img" style="height: 25rem;background-image: url(../../../../gallery/capybara2.jpg);background-position: center; background-repeat:no-repeat; background-size:cover;-moz-background-size:cover;overflow:hidden;" >
                <a href="#" style="padding: 4rem 4rem 2rem 4rem ;"><h2 >Deep-learning知识简要回顾</h2></a>
            </div>
            <!-- Post -->
            <div class="typo" style="padding: 3rem;">
                <h1 id="参考“李沐-动手学习深度学习”重新系统的回顾必要知识，顺手记录感悟"><a href="#参考“李沐-动手学习深度学习”重新系统的回顾必要知识，顺手记录感悟" class="headerlink" title="参考“李沐-动手学习深度学习”重新系统的回顾必要知识，顺手记录感悟"></a>参考<a target="_blank" rel="noopener" href="https://zh-v2.d2l.ai/chapter_introduction/index.html">“李沐-动手学习深度学习”</a>重新系统的回顾必要知识，顺手记录感悟</h1><h1 id="一、机器学习与神经网络前言-2024-7-14更"><a href="#一、机器学习与神经网络前言-2024-7-14更" class="headerlink" title="一、机器学习与神经网络前言 (2024&#x2F;7&#x2F;14更)"></a>一、机器学习与神经网络前言 (2024&#x2F;7&#x2F;14更)</h1><p><strong>机器学习的核心组件：数据，模型，目标函数，算法</strong></p>
<ul>
<li>*没有高质量的数据，深度学习将黯然失色（所以说数据就像对人的教育）</li>
<li>*深度学习倾向于end-to-end模型，模型就像做事的方法逻辑</li>
<li>*目标函数用于反馈和迭代，不然如何进步？做事要有一定的方向</li>
<li>*优化算法是实际执行的步骤代码，是开始行动的标志；高效的算法代表高效的行动，作用于数据-模型-函数之间。</li>
</ul>
<p><strong>看了一会发现这节看过了，简单再记一些平时做的少的部分。</strong></p>
<ul>
<li>*监督学习：…，标记（多标签），搜索，推荐，序列…</li>
<li>*无监督学习：…，因果，概率图…</li>
<li>*强化学习：…</li>
</ul>
<h1 id="二、预备知识"><a href="#二、预备知识" class="headerlink" title="二、预备知识"></a>二、预备知识</h1><p><strong>这部分涉及数据处理和一些数学（向量，矩阵，微分…）回顾浏览一下并简要记录一些可能不熟悉的部分。当然也可以看看一些教程里的一些没用过的库和表达。</strong></p>
<ol>
<li><p>1.数据操作：这部分介绍了基础的张量操作，新手可以注意节省内存部分、广播部分、对象转换部分</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#以下操作导致id重新分配</span><br><span class="line">before = id(Y)</span><br><span class="line">Y = Y + X</span><br><span class="line">id(Y) == before</span><br><span class="line">#通过切片指向指定id</span><br><span class="line">before = id(X)</span><br><span class="line">X += Y</span><br><span class="line">id(X) == before</span><br></pre></td></tr></table></figure>
</li>
<li><p>2.数据预处理：pandas（我还挺少用来着），对于处理大批量，高质量的源头数据要用到，可以用于处理缺失数据</p>
</li>
<li><p>3.线性代数： 矩阵&#x2F;张量操作；非降维求和（keepdims）；矩阵乘法与Hadamard积；范数…</p>
</li>
<li><p>4.微积分： （仿佛回到了高中），这部分要清楚导数，梯度，链式法则，有一说一太久不用我都不一定写的出来链式法则0.0</p>
</li>
<li><p>5.自动微分：这应该是基于梯度的优化最重要的部分了（但是对使用者并不需要严格掌握，大部分时间几乎不需要知道，但深入底层其实往往会自己autograd函数），大部分深度学习库的底层都是一个个计算图；对于高阶和高维的y和x，求导的结果可以是一个高阶张量；控制流的梯度计算允许梯度在各种结构中被计算</p>
</li>
<li><p>6.概率：真神降临！——概率是更古典的数学美学。概率将不确定的事情进行了确定的描述，概率和人性有一种关联，概率可以通往哲学。随机变量，最基础的概念，但实际上很容易搞错；贝叶斯定理，统计学中最有用的方程之一；边际概率；期望、（协）方差（也非常重要的概念）</p>
</li>
</ol>
<h1 id="三、线性神经网络-2024-7-16更"><a href="#三、线性神经网络-2024-7-16更" class="headerlink" title="三、线性神经网络 (2024&#x2F;7&#x2F;16更)"></a>三、线性神经网络 (2024&#x2F;7&#x2F;16更)</h1><p><strong>在介绍深度神经网络之前，我们需要了解神经网络训练的基础知识。 本章我们将介绍神经网络的整个训练过程， 包括：定义简单的神经网络架构、数据处理、指定损失函数和如何训练模型。 为了更容易学习，我们将从经典算法————线性神经网络开始，介绍神经网络的基础知识。 经典统计学习技术中的线性回归和softmax回归可以视为线性神经网络， 这些知识将为其他部分中更复杂的技术奠定基础。</strong></p>
<ol>
<li>1.线性回归：线性模型y&#x3D;Xw+b;损失函数；解析解（简单的模型可以有解析解了，复杂的深度学习模型很难有解析解）；随机梯度下降；预测&#x2F;推断；矢量化运算；正态分布与平方损失——这部分建议深入思考学习（在高斯噪声的假设下，最小化均方误差等价于对线性模型的极大似然估计）（插一嘴极大似然，哲学的理解就是冥冥之中自有定数，既然发生了，那一定是有因果可循，一定是最有可能如此才会被观察到）；从线性回归到深度网络。</li>
</ol>
<ul>
<li>小结：机器学习模型中的关键要素是训练数据、损失函数、优化算法，还有模型本身。矢量化使数学表达上更简洁，同时运行的更快。最小化目标函数和执行极大似然估计等价。线性回归模型也是一个简单的神经网络。</li>
</ul>
<ol start="2">
<li><p>2.从0开始实现线性回归（回顾入门代码）<br>Examples：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"># 我们定义一个data_iter函数， 该函数接收批量大小、特征矩阵和标</span><br><span class="line"># 签向量作为输入，生成大小为batch_size的小批量。 </span><br><span class="line"># 每个小批量包含一组特征和标签。</span><br><span class="line">def data_iter(batch_size, features, labels):</span><br><span class="line">    num_examples = len(features)</span><br><span class="line">    indices = list(range(num_examples))</span><br><span class="line">    # 这些样本是随机读取的，没有特定的顺序</span><br><span class="line">    random.shuffle(indices)</span><br><span class="line">    for i in range(0, num_examples, batch_size):</span><br><span class="line">        batch_indices = np.array(</span><br><span class="line">            indices[i: min(i + batch_size, num_examples)])</span><br><span class="line">        yield features[batch_indices], labels[batch_indices]</span><br><span class="line"></span><br><span class="line">batch_size = 10</span><br><span class="line"></span><br><span class="line">for X, y in data_iter(batch_size, features, labels):</span><br><span class="line">    print(X, &#x27;\n&#x27;, y)</span><br><span class="line">    break</span><br></pre></td></tr></table></figure>
</li>
<li><p>3.线性回归的深度学习框架实现</p>
</li>
<li><p>4.Softmax回归——分类：全连接层的参数开销（具体来说，对于任何具有d个输入和q个输出的全连接层， 参数开销为O(dq)，这个数字在实践中可能高得令人望而却步。幸运的是，将d个输入转换为q个输出的成本可以减少到O(dq&#x2F;n)， 其中超参数n可以由我们灵活指定，以在实际应用中平衡参数节约和模型有效性）；交叉熵与信息论（这部分同样很重要）</p>
</li>
<li><p>5.图像数据集——MNIST&#x2F;F-MNIST</p>
</li>
<li><p>6.Softmax回归从0实现</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">def softmax(X):</span><br><span class="line">    X_exp = torch.exp(X)</span><br><span class="line">    partition = X_exp.sum(1, keepdim=True)</span><br><span class="line">    return X_exp / partition  # 这里应用了广播机制</span><br></pre></td></tr></table></figure>
</li>
<li><p>7.Softmax回归简洁实现</p>
</li>
</ol>
<h1 id="四、多层感知机"><a href="#四、多层感知机" class="headerlink" title="四、多层感知机"></a>四、多层感知机</h1><p><strong>深度学习的基本模型</strong></p>
<ol>
<li><p>1.MLP：从单层到多层；从线性到非线性；通用近似；激活函数；</p>
</li>
<li><p>2.从0开始MLP （我觉得教程这个也不算从0开始，依然用库简化了很多步骤，真男人要从梯度开始手撕）</p>
</li>
<li><p>3.简洁实现MLP（。。。）</p>
</li>
<li><p>4.模型选择，欠拟合与过拟合</p>
</li>
</ol>
<ul>
<li>4.1.将模型在训练数据上拟合的比在潜在分布中更接近的现象称为过拟合（overfitting）， 用于对抗过拟合的技术称为正则化（regularization）。训练误差与泛化误差</li>
<li>4.2.i.i.d. 与 VC理论</li>
<li>4.3.模型选择，验证集（k-fold），欠拟合or过拟合？</li>
<li>4.4.模型复杂性-损失 关系图</li>
<li>4.5.数据集</li>
</ul>
<ol start="5">
<li><p>5.权重衰减(weight decay): </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># L(w,b)+λ/2*L2</span><br><span class="line">def l2_penalty(w):</span><br><span class="line">    return (w**2).sum() / 2</span><br></pre></td></tr></table></figure>
</li>
<li><p>6.Dropout: 扰动的稳健性</p>
</li>
<li><p>7.前向传播，反向传播与计算图 （这部分的推导可以看看，但不够详细）；在训练期间需要额外存储计算图中元素的梯度，导致额外的内存开销</p>
</li>
<li><p>8.数值稳定性与模型初始化<br><strong>初始化方案的选择在神经网络学习中起着举足轻重的作用， 它对保持数值稳定性至关重要。 此外，这些初始化方案的选择可以与非线性激活函数的选择有趣的结合在一起。 我们选择哪个函数以及如何初始化参数可以决定优化算法收敛的速度有多快。 糟糕选择可能会导致我们在训练时遇到梯度爆炸或梯度消失。</strong></p>
</li>
</ol>
<ul>
<li>*梯度消失和梯度爆炸：不稳定梯度带来的风险不止在于数值表示； 不稳定梯度也威胁到我们优化算法的稳定性。 我们可能面临一些问题。 要么是梯度爆炸（gradient exploding）问题： 参数更新过大，破坏了模型的稳定收敛； 要么是梯度消失（gradient vanishing）问题： 参数更新过小，在每次更新时几乎不会移动，导致模型无法学习。</li>
<li>*Sigmoid梯度消失</li>
<li>*打破对称性：对称的参数更新时也依然对称；随机初始化</li>
<li>*Xavier初始化 1&#x2F;2(nin+nout)σ2&#x3D;1</li>
<li>*…其他巧妙初始化方法也被广泛研究</li>
</ul>
<ol start="9">
<li>9.环境和分布偏移 （非常容易被忽略，非常致命）<br><strong>许多失败的机器学习部署（即实际应用）都可以追究到这种方式——我们从来没有想过数据最初从哪里来？以及我们计划最终如何处理模型的输出？ 通常情况下，开发人员会拥有一些数据且急于开发模型，而不关注这些基本问题。 有时，根据测试集的精度衡量，模型表现得非常出色。 但是当数据分布突然改变时，模型在部署中会出现灾难性的失败。 更隐蔽的是，有时模型的部署本身就是扰乱数据分布的催化剂。</strong></li>
</ol>
<ul>
<li>*分布偏移的类型：协变量（特征）偏移；标签偏移；概念偏移；</li>
<li>*分布偏移实例</li>
<li>*纠正分布偏移：协变量偏移纠正（加权）；标签偏移纠正（线性补偿）；概念偏移纠正（类似迁移学习）</li>
<li>*批量学习；在线学习；老虎机；强化学习——上述不同情况之间的一个关键区别是：在静止环境中可能一直有效的相同策略， 在环境能够改变的情况下可能不会始终有效。</li>
<li>*最后，重要的是，当我们部署机器学习系统时， 不仅仅是在优化一个预测模型， 而通常是在提供一个会被用来（部分或完全）进行自动化决策的工具。 这些技术系统可能会通过其进行的决定而影响到每个人的生活。</li>
</ul>
<ol start="10">
<li>10.<a target="_blank" rel="noopener" href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques">Kaggle实战</a></li>
</ol>
<ul>
<li>小结：真实数据通常混合了不同的数据类型，需要进行预处理。常用的预处理方法：将实值数据重新缩放为零均值和单位方法；用均值替换缺失值。将类别特征转化为指标特征，可以使我们把这个特征当作一个独热向量来对待。我们可以使用k折交叉验证来选择模型并调整超参数。对数对于相对误差很有用。</li>
</ul>
<h1 id="五、深度学习-（2024-9-22更）"><a href="#五、深度学习-（2024-9-22更）" class="headerlink" title="五、深度学习 （2024&#x2F;9&#x2F;22更）"></a>五、深度学习 （2024&#x2F;9&#x2F;22更）</h1><p><strong>在本章中，我们将深入探索深度学习计算的关键组件， 即模型构建、参数访问与初始化、设计自定义层和块、将模型读写到磁盘， 以及利用GPU实现显著的加速</strong></p>
<ol>
<li>1.层和块</li>
</ol>
<ul>
<li>*事实证明，研究讨论“比单个层大”但“比整个模型小”的组件更有价值。 例如，在计算机视觉中广泛流行的ResNet-152架构就有数百层， 这些层是由层组（groups of layers）的重复模式组成。</li>
<li>*自定义块<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">class MLP(nn.Module):</span><br><span class="line">    # 用模型参数声明层。这里，我们声明两个全连接的层</span><br><span class="line">    def __init__(self):</span><br><span class="line">        # 调用MLP的父类Module的构造函数来执行必要的初始化。</span><br><span class="line">        # 这样，在类实例化时也可以指定其他函数参数，例如模型参数params（稍后将介绍）</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.hidden = nn.Linear(20, 256)  # 隐藏层</span><br><span class="line">        self.out = nn.Linear(256, 10)  # 输出层</span><br><span class="line"></span><br><span class="line">    # 定义模型的前向传播，即如何根据输入X返回所需的模型输出</span><br><span class="line">    def forward(self, X):</span><br><span class="line">        # 注意，这里我们使用ReLU的函数版本，其在nn.functional模块中定义。</span><br><span class="line">        return self.out(F.relu(self.hidden(X)))</span><br></pre></td></tr></table></figure></li>
<li>顺序块、嵌套模块<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">class NestMLP(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.net = nn.Sequential(nn.Linear(20, 64), nn.ReLU(),</span><br><span class="line">                                 nn.Linear(64, 32), nn.ReLU())</span><br><span class="line">        self.linear = nn.Linear(32, 16)</span><br><span class="line"></span><br><span class="line">    def forward(self, X):</span><br><span class="line">        return self.linear(self.net(X))</span><br><span class="line"></span><br><span class="line">chimera = nn.Sequential(NestMLP(), nn.Linear(16, 20), FixedHiddenMLP())</span><br><span class="line">chimera(X)</span><br></pre></td></tr></table></figure></li>
</ul>
<ol start="2">
<li>2.参数管理</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">print(net[1].params) # 层参数概要</span><br><span class="line">print(type(net[1].bias)) </span><br><span class="line">print(net[1].bias)</span><br><span class="line">print(net[1].bias.data())</span><br><span class="line">net[1].weight.grad() #访问参数梯度</span><br><span class="line"></span><br><span class="line">print(net[0].collect_params()) #访问所有参数</span><br><span class="line">print(net.collect_params())</span><br></pre></td></tr></table></figure>
<ul>
<li>*嵌套块的参数访问</li>
<li>*参数初始化：<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"># 内置初始化</span><br><span class="line">def init_normal(m):</span><br><span class="line">    if type(m) == nn.Linear:</span><br><span class="line">        nn.init.normal_(m.weight, mean=0, std=0.01)</span><br><span class="line">        nn.init.zeros_(m.bias)</span><br><span class="line">net.apply(init_normal)</span><br><span class="line">net[0].weight.data[0], net[0].bias.data[0]</span><br><span class="line"></span><br><span class="line"># 常数/自定义初始化</span><br><span class="line">def init_constant(m):</span><br><span class="line">    if type(m) == nn.Linear:</span><br><span class="line">        nn.init.constant_(m.weight, 1)</span><br><span class="line">        nn.init.zeros_(m.bias)</span><br><span class="line">net.apply(init_constant)</span><br><span class="line">net[0].weight.data[0], net[0].bias.data[0]</span><br><span class="line"></span><br><span class="line">def my_init(m):</span><br><span class="line">    if type(m) == nn.Linear:</span><br><span class="line">        print(&quot;Init&quot;, *[(name, param.shape)</span><br><span class="line">                        for name, param in m.named_parameters()][0])</span><br><span class="line">        nn.init.uniform_(m.weight, -10, 10)</span><br><span class="line">        m.weight.data *= m.weight.data.abs() &gt;= 5</span><br><span class="line">net.apply(my_init)</span><br><span class="line">net[0].weight[:2]</span><br><span class="line"></span><br><span class="line"># 分层初始化</span><br><span class="line">def init_xavier(m):</span><br><span class="line">    if type(m) == nn.Linear:</span><br><span class="line">        nn.init.xavier_uniform_(m.weight)</span><br><span class="line">def init_42(m):</span><br><span class="line">    if type(m) == nn.Linear:</span><br><span class="line">        nn.init.constant_(m.weight, 42)</span><br><span class="line">net[0].apply(init_xavier)</span><br><span class="line">net[2].apply(init_42)</span><br></pre></td></tr></table></figure></li>
<li>*参数绑定&#x2F;共享：有时我们希望在多个层间共享参数： 我们可以定义一个稠密层，然后使用它的参数来设置另一个层的参数。</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># 我们需要给共享层一个名称，以便可以引用它的参数</span><br><span class="line">shared = nn.Linear(8, 8)</span><br><span class="line">net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(),</span><br><span class="line">                    shared, nn.ReLU(),</span><br><span class="line">                    shared, nn.ReLU(),</span><br><span class="line">                    nn.Linear(8, 1))</span><br><span class="line">net(X)</span><br><span class="line"># 检查参数是否相同</span><br><span class="line">print(net[2].weight.data[0] == net[4].weight.data[0])</span><br><span class="line">net[2].weight.data[0, 0] = 100</span><br><span class="line"># 确保它们实际上是同一个对象，而不只是有相同的值</span><br><span class="line">print(net[2].weight.data[0] == net[4].weight.data[0])</span><br></pre></td></tr></table></figure>
<p>这个例子表明第三个和第五个神经网络层的参数是绑定的。 它们不仅值相等，而且由相同的张量表示。 因此，如果我们改变其中一个参数，另一个参数也会改变。 这里有一个问题：当参数绑定时，梯度会发生什么情况？ 答案是由于模型参数包含梯度，因此在反向传播期间第二个隐藏层 （即第三个神经网络层）和第三个隐藏层（即第五个神经网络层）的梯度会加在一起。</p>
<ol start="3">
<li>3.延后初始化<br><strong>到目前为止，我们忽略了建立网络时需要做的以下这些事情：我们定义了网络架构，但没有指定输入维度。我们添加层时没有指定前一层的输出维度。我们在初始化参数时，甚至没有足够的信息来确定模型应该包含多少参数。这里的诀窍是框架的延后初始化（defers initialization）， 即直到数据第一次通过模型传递时，框架才会动态地推断出每个层的大小。在以后，当使用卷积神经网络时， 由于输入维度（即图像的分辨率）将影响每个后续层的维数， 有了该技术将更加方便。</strong></li>
</ol>
<ul>
<li>*延后初始化使框架能够自动推断参数形状，使修改模型架构变得容易，避免了一些常见的错误。我们可以通过模型传递数据，使框架最终初始化参数</li>
</ul>
<ol start="4">
<li>4.自定义层<br><strong>深度学习成功背后的一个因素是神经网络的灵活性： 我们可以用创造性的方式组合不同的层，从而设计出适用于各种任务的架构。</strong></li>
</ol>
<ul>
<li><p>*不带任何参数的层</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">class CenteredLayer(nn.Block):</span><br><span class="line">    def __init__(self, **kwargs):</span><br><span class="line">        super().__init__(**kwargs)</span><br><span class="line"></span><br><span class="line">    def forward(self, X):</span><br><span class="line">        return X - X.mean()</span><br></pre></td></tr></table></figure>
</li>
<li><p>*带参数的层</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">class MyLinear(nn.Module):</span><br><span class="line">    def __init__(self, in_units, units):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.weight = nn.Parameter(torch.randn(in_units, units))</span><br><span class="line">        self.bias = nn.Parameter(torch.randn(units,))</span><br><span class="line">    def forward(self, X):</span><br><span class="line">        linear = torch.matmul(X, self.weight.data) + self.bias.data</span><br><span class="line">        return F.relu(linear)</span><br></pre></td></tr></table></figure></li>
</ul>
<ol start="5">
<li>5.读写文件<br><strong>到目前为止，我们讨论了如何处理数据， 以及如何构建、训练和测试深度学习模型。 然而，有时我们希望保存训练的模型， 以备将来在各种环境中使用（比如在部署中进行预测）。 此外，当运行一个耗时较长的训练过程时， 最佳的做法是定期保存中间结果， 以确保在服务器电源被不小心断掉时，我们不会损失几天的计算结果。</strong></li>
</ol>
<ul>
<li>*结果你只给了个 torch.save(net.state_dict(), ‘mlp.params’)</li>
</ul>
<ol start="6">
<li>6.GPU</li>
</ol>
<ul>
<li>*简而言之，自2000年以来，GPU性能每十年增长1000倍。</li>
<li>*如果有多个GPU，我们使用torch.device(f’cuda:{i}’) 来表示第<br>块GPU（从0开始）OS： 贫穷限制了我的想象</li>
<li>*有几种方法可以在GPU上存储张量。 例如，我们可以在创建张量时指定存储设备。接 下来，我们在第一个gpu上创建张量变量X。 在GPU上创建的张量只消耗这个GPU的显存。 我们可以使用nvidia-smi命令查看显存使用情况。 一般来说，我们需要确保不创建超过GPU显存限制的数据。</li>
<li>*复制，X(GPU0) +Y(GPU1)将导致错误，需复制Z&#x3D;X(GPU1),使用Z+Y</li>
</ul>
<h1 id="六、-卷积神经网络"><a href="#六、-卷积神经网络" class="headerlink" title="六、 卷积神经网络"></a>六、 卷积神经网络</h1>
            </div>

            <!-- Post Comments -->
            
    <!-- 使用 DISQUS_CLICK -->
<div id="disqus-comment">
    <div id="disqus_thread"></div>

<!-- add animation -->
<style>
	.disqus_click_btn {
            line-height: 30px;
            margin: 0;
            min-width: 50px;
            padding: 0 14px;
            display: inline-block;
            font-family: "Roboto", "Helvetica", "Arial", sans-serif;
            font-size: 14px;
            font-weight: 400;
            text-transform: uppercase;
            letter-spacing: 0;
            overflow: hidden;
            will-change: box-shadow;
            transition: box-shadow .2s cubic-bezier(.4, 0, 1, 1), background-color .2s cubic-bezier(.4, 0, .2, 1), color .2s cubic-bezier(.4, 0, .2, 1);
            outline: 0;
            cursor: pointer;
            text-decoration: none;
            text-align: center;
            vertical-align: middle;
            border: 0;
            background: rgba(158, 158, 158, .2);
            box-shadow: 0 2px 2px 0 rgba(0, 0, 0, .14), 0 3px 1px -2px rgba(0, 0, 0, .2), 0 1px 5px 0 rgba(0, 0, 0, .12);
            color: #fff;
            background-color: #7EC0EE;
            text-shadow: 0
        }
</style>
	
<div class="btn_click_load" id="disqus_bt"> 
    <button class="disqus_click_btn">点击查看评论</button>
</div>

<!--
<script type="text/javascript">
$('.btn_click_load').click(function() {
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'http-eggman-blog'; // required: replace example with your forum shortname

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();

    document.getElementById('disqus_bt').style.display = "none";
});
</script>
-->
<script type="text/javascript">
    var disqus_config = function () {
        this.page.url = 'http://cuiyi126.github.io/2024/07/14/Review_%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/';  // Replace PAGE_URL with your page's canonical URL variable
        this.page.identifier = 'http://cuiyi126.github.io/2024/07/14/Review_%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/'; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
</script>

<script type="text/javascript">
    $('.btn_click_load').click(function() {  //click to load comments
        (function() { // DON'T EDIT BELOW THIS LINE
            var d = document;
            var s = d.createElement('script');
            s.src = '//http-eggman-blog.disqus.com/embed.js';
            s.setAttribute('data-timestamp', + new Date());
            (d.head || d.body).appendChild(s);
        })();
        $('.btn_click_load').css('display','none');
    });
</script>
</div>
<style>
    #disqus-comment{
        background-color: #eee;
        padding: 2pc;
    }
</style>


        </div>
        <!-- Copyright 版权 start -->
                <div id="copyright">
            <ul>
                <li>&copy;Powered By <a target="_blank" rel="noopener" href="https://hexo.io/zh-cn/" style="border-bottom: none;">hexo</a></li>
                <li>Design: <a target="_blank" rel="noopener" href="http://miccall.tech " style="border-bottom: none;">Thanks: miccall</a></li>
            </ul>
            <ul>
                <li xmlns:cc="http://creativecommons.org/ns#" > Licensed under <a href="http://creativecommons.org/licenses/by-nc-sa/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;">CC BY-NC-SA 4.0<img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/nc.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/sa.svg?ref=chooser-v1"></a></li>
            </ul>
            
                <span id="busuanzi_container_site_pv">Views: <span id="busuanzi_value_site_pv"></span> times</span>
			
        </div>
    </div>
</body>



 	
</html>
